{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep Convolutional GANs\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = (64,64) # We set the size of the generated images (64x64)\n",
    "num_epochs = 301 # number of epochs to train the DCGAN for\n",
    "length_of_noise_vector = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.ImageFolder(root='input pics for GANs', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(10, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the generator\n",
    "\n",
    "class G(nn.Module): # We introduce a class to define the generator.\n",
    "\n",
    "    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.\n",
    "        super(G, self).__init__() # We inherit from the nn.Module tools.\n",
    "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
    "            nn.ConvTranspose2d(length_of_noise_vector, 512, 4, 1, 0, bias = False), # We add an inversed convolution. 512 feature maps. kernel size is 4x4\n",
    "            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.\n",
    "            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution. 256 feature maps. kernel size is 4, stride=2, padding=1\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution. 128 feature maps\n",
    "            nn.BatchNorm2d(128), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution. 64 feature maps\n",
    "            nn.BatchNorm2d(64), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution. color images have 3 channels\n",
    "            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.\n",
    "        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.\n",
    "        return output # We return the output containing the generated images.\n",
    "\n",
    "# Creating the generator\n",
    "netG = G() # We create the generator object.\n",
    "netG.apply(weights_init) # We initialize all the weights of its neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module): # We introduce a class to define the discriminator.\n",
    "\n",
    "    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.\n",
    "        super(D, self).__init__() # We inherit from the nn.Module tools.\n",
    "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution. color images have 3 channels. 64 feature maps. kernel size is 4x4, stride=2, padding=1\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(512), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.\n",
    "            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.\n",
    "        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.\n",
    "        return output.view(-1) # We return the output which will be a value between 0 and 1.\n",
    "\n",
    "# Creating the discriminator\n",
    "netD = D() # We create the discriminator object.\n",
    "netD.apply(weights_init) # We initialize all the weights of its neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/301][0/8] Loss_D: 1.6567 Loss_G: 3.1920\n",
      "[0/301][1/8] Loss_D: 2.8554 Loss_G: 4.2840\n",
      "[0/301][2/8] Loss_D: 1.3190 Loss_G: 6.0122\n",
      "[0/301][3/8] Loss_D: 0.5371 Loss_G: 6.4904\n",
      "[0/301][4/8] Loss_D: 0.2944 Loss_G: 6.1497\n",
      "[0/301][5/8] Loss_D: 0.4829 Loss_G: 6.2067\n",
      "[0/301][6/8] Loss_D: 0.2690 Loss_G: 6.4676\n",
      "[0/301][7/8] Loss_D: 0.3183 Loss_G: 8.5185\n",
      "[1/301][0/8] Loss_D: 0.1244 Loss_G: 6.7655\n",
      "[1/301][1/8] Loss_D: 0.1459 Loss_G: 6.1242\n",
      "[1/301][2/8] Loss_D: 0.2784 Loss_G: 6.9780\n",
      "[1/301][3/8] Loss_D: 0.0928 Loss_G: 6.9305\n",
      "[1/301][4/8] Loss_D: 0.1925 Loss_G: 6.3364\n",
      "[1/301][5/8] Loss_D: 0.3258 Loss_G: 6.8616\n",
      "[1/301][6/8] Loss_D: 0.1609 Loss_G: 7.7586\n",
      "[1/301][7/8] Loss_D: 0.0430 Loss_G: 7.5404\n",
      "[2/301][0/8] Loss_D: 0.0922 Loss_G: 7.3310\n",
      "[2/301][1/8] Loss_D: 0.0993 Loss_G: 7.8713\n",
      "[2/301][2/8] Loss_D: 0.0799 Loss_G: 7.5748\n",
      "[2/301][3/8] Loss_D: 0.1334 Loss_G: 7.4985\n",
      "[2/301][4/8] Loss_D: 0.0832 Loss_G: 7.5133\n",
      "[2/301][5/8] Loss_D: 0.0794 Loss_G: 7.9070\n",
      "[2/301][6/8] Loss_D: 0.0796 Loss_G: 8.0489\n",
      "[2/301][7/8] Loss_D: 0.0351 Loss_G: 7.6001\n",
      "[3/301][0/8] Loss_D: 0.0818 Loss_G: 7.9759\n",
      "[3/301][1/8] Loss_D: 0.0726 Loss_G: 8.3225\n",
      "[3/301][2/8] Loss_D: 0.0581 Loss_G: 7.7979\n",
      "[3/301][3/8] Loss_D: 0.0960 Loss_G: 7.6195\n",
      "[3/301][4/8] Loss_D: 0.0719 Loss_G: 8.5907\n",
      "[3/301][5/8] Loss_D: 0.0345 Loss_G: 8.0648\n",
      "[3/301][6/8] Loss_D: 0.0441 Loss_G: 7.6929\n",
      "[3/301][7/8] Loss_D: 0.0723 Loss_G: 9.0432\n",
      "[4/301][0/8] Loss_D: 0.0296 Loss_G: 8.0397\n",
      "[4/301][1/8] Loss_D: 0.0419 Loss_G: 7.8616\n",
      "[4/301][2/8] Loss_D: 0.0977 Loss_G: 10.4470\n",
      "[4/301][3/8] Loss_D: 0.0484 Loss_G: 9.8186\n",
      "[4/301][4/8] Loss_D: 0.0106 Loss_G: 7.2253\n",
      "[4/301][5/8] Loss_D: 0.0841 Loss_G: 9.4039\n",
      "[4/301][6/8] Loss_D: 0.0176 Loss_G: 8.8291\n",
      "[4/301][7/8] Loss_D: 0.0131 Loss_G: 7.6109\n",
      "[5/301][0/8] Loss_D: 0.0296 Loss_G: 7.5946\n",
      "[5/301][1/8] Loss_D: 0.0966 Loss_G: 9.3793\n",
      "[5/301][2/8] Loss_D: 0.0232 Loss_G: 8.6820\n",
      "[5/301][3/8] Loss_D: 0.0196 Loss_G: 7.3789\n",
      "[5/301][4/8] Loss_D: 0.0523 Loss_G: 8.4919\n",
      "[5/301][5/8] Loss_D: 0.0241 Loss_G: 8.1118\n",
      "[5/301][6/8] Loss_D: 0.0282 Loss_G: 7.6762\n",
      "[5/301][7/8] Loss_D: 0.1148 Loss_G: 9.0154\n",
      "[6/301][0/8] Loss_D: 0.0089 Loss_G: 8.4492\n",
      "[6/301][1/8] Loss_D: 0.0170 Loss_G: 7.8355\n",
      "[6/301][2/8] Loss_D: 0.0438 Loss_G: 9.3298\n",
      "[6/301][3/8] Loss_D: 0.0099 Loss_G: 8.8136\n",
      "[6/301][4/8] Loss_D: 0.0186 Loss_G: 7.8198\n",
      "[6/301][5/8] Loss_D: 0.0366 Loss_G: 8.6285\n",
      "[6/301][6/8] Loss_D: 0.0245 Loss_G: 8.3741\n",
      "[6/301][7/8] Loss_D: 0.0239 Loss_G: 9.2101\n",
      "[7/301][0/8] Loss_D: 0.0236 Loss_G: 7.9001\n",
      "[7/301][1/8] Loss_D: 0.0252 Loss_G: 7.9195\n",
      "[7/301][2/8] Loss_D: 0.0389 Loss_G: 8.5579\n",
      "[7/301][3/8] Loss_D: 0.0166 Loss_G: 8.0541\n",
      "[7/301][4/8] Loss_D: 0.0318 Loss_G: 7.3050\n",
      "[7/301][5/8] Loss_D: 0.0305 Loss_G: 8.3793\n",
      "[7/301][6/8] Loss_D: 0.0215 Loss_G: 8.1534\n",
      "[7/301][7/8] Loss_D: 0.0329 Loss_G: 7.3439\n",
      "[8/301][0/8] Loss_D: 0.0373 Loss_G: 9.3946\n",
      "[8/301][1/8] Loss_D: 0.0148 Loss_G: 8.7716\n",
      "[8/301][2/8] Loss_D: 0.0143 Loss_G: 8.1577\n",
      "[8/301][3/8] Loss_D: 0.0245 Loss_G: 8.3255\n",
      "[8/301][4/8] Loss_D: 0.0186 Loss_G: 8.3697\n",
      "[8/301][5/8] Loss_D: 0.0280 Loss_G: 7.3084\n",
      "[8/301][6/8] Loss_D: 0.0448 Loss_G: 9.4593\n",
      "[8/301][7/8] Loss_D: 0.0089 Loss_G: 9.2580\n",
      "[9/301][0/8] Loss_D: 0.0093 Loss_G: 7.6942\n",
      "[9/301][1/8] Loss_D: 0.0339 Loss_G: 7.9276\n",
      "[9/301][2/8] Loss_D: 0.0463 Loss_G: 10.7579\n",
      "[9/301][3/8] Loss_D: 0.0554 Loss_G: 7.1863\n",
      "[9/301][4/8] Loss_D: 0.0671 Loss_G: 12.8489\n",
      "[9/301][5/8] Loss_D: 0.0024 Loss_G: 13.6555\n",
      "[9/301][6/8] Loss_D: 0.0042 Loss_G: 11.4388\n",
      "[9/301][7/8] Loss_D: 0.0046 Loss_G: 7.8118\n",
      "[10/301][0/8] Loss_D: 0.0387 Loss_G: 10.3954\n",
      "[10/301][1/8] Loss_D: 0.0636 Loss_G: 4.6018\n",
      "[10/301][2/8] Loss_D: 0.6927 Loss_G: 42.2487\n",
      "[10/301][3/8] Loss_D: 25.6152 Loss_G: 32.9685\n",
      "[10/301][4/8] Loss_D: 0.4382 Loss_G: 24.2929\n",
      "[10/301][5/8] Loss_D: 0.0001 Loss_G: 11.3946\n",
      "[10/301][6/8] Loss_D: 1.4161 Loss_G: 32.2348\n",
      "[10/301][7/8] Loss_D: 1.1954 Loss_G: 28.6224\n",
      "[11/301][0/8] Loss_D: 0.0000 Loss_G: 24.6331\n",
      "[11/301][1/8] Loss_D: 0.0000 Loss_G: 15.3676\n",
      "[11/301][2/8] Loss_D: 0.0944 Loss_G: 8.9506\n",
      "[11/301][3/8] Loss_D: 2.7664 Loss_G: 36.7923\n",
      "[11/301][4/8] Loss_D: 9.3071 Loss_G: 30.2518\n",
      "[11/301][5/8] Loss_D: 0.0002 Loss_G: 27.2743\n",
      "[11/301][6/8] Loss_D: 0.0000 Loss_G: 21.3871\n",
      "[11/301][7/8] Loss_D: 0.0001 Loss_G: 10.2688\n",
      "[12/301][0/8] Loss_D: 2.1346 Loss_G: 24.0948\n",
      "[12/301][1/8] Loss_D: 0.1296 Loss_G: 26.6219\n",
      "[12/301][2/8] Loss_D: 1.7553 Loss_G: 15.5364\n",
      "[12/301][3/8] Loss_D: 0.0314 Loss_G: 2.8938\n",
      "[12/301][4/8] Loss_D: 7.6109 Loss_G: 21.2652\n",
      "[12/301][5/8] Loss_D: 0.0000 Loss_G: 24.7471\n",
      "[12/301][6/8] Loss_D: 0.0522 Loss_G: 22.7788\n",
      "[12/301][7/8] Loss_D: 0.3767 Loss_G: 15.0595\n",
      "[13/301][0/8] Loss_D: 0.0121 Loss_G: 6.5555\n",
      "[13/301][1/8] Loss_D: 2.0836 Loss_G: 17.0572\n",
      "[13/301][2/8] Loss_D: 0.0170 Loss_G: 18.3682\n",
      "[13/301][3/8] Loss_D: 0.8292 Loss_G: 10.5158\n",
      "[13/301][4/8] Loss_D: 0.1475 Loss_G: 5.4444\n",
      "[13/301][5/8] Loss_D: 2.7930 Loss_G: 20.7166\n",
      "[13/301][6/8] Loss_D: 1.4555 Loss_G: 17.1033\n",
      "[13/301][7/8] Loss_D: 0.0003 Loss_G: 11.5731\n",
      "[14/301][0/8] Loss_D: 0.0875 Loss_G: 5.9070\n",
      "[14/301][1/8] Loss_D: 2.0945 Loss_G: 18.3286\n",
      "[14/301][2/8] Loss_D: 1.4821 Loss_G: 11.8691\n",
      "[14/301][3/8] Loss_D: 0.0129 Loss_G: 5.9320\n",
      "[14/301][4/8] Loss_D: 2.1709 Loss_G: 16.6835\n",
      "[14/301][5/8] Loss_D: 0.0696 Loss_G: 18.7448\n",
      "[14/301][6/8] Loss_D: 0.5710 Loss_G: 10.7336\n",
      "[14/301][7/8] Loss_D: 0.0372 Loss_G: 3.8748\n",
      "[15/301][0/8] Loss_D: 3.5206 Loss_G: 19.0544\n",
      "[15/301][1/8] Loss_D: 0.9886 Loss_G: 16.5090\n",
      "[15/301][2/8] Loss_D: 0.0163 Loss_G: 11.6441\n",
      "[15/301][3/8] Loss_D: 0.0592 Loss_G: 5.9511\n",
      "[15/301][4/8] Loss_D: 1.1325 Loss_G: 13.8633\n",
      "[15/301][5/8] Loss_D: 1.9460 Loss_G: 5.0132\n",
      "[15/301][6/8] Loss_D: 2.6042 Loss_G: 18.9151\n",
      "[15/301][7/8] Loss_D: 4.3173 Loss_G: 8.7615\n",
      "[16/301][0/8] Loss_D: 0.2492 Loss_G: 6.1806\n",
      "[16/301][1/8] Loss_D: 1.8405 Loss_G: 19.1775\n",
      "[16/301][2/8] Loss_D: 5.6922 Loss_G: 7.3375\n",
      "[16/301][3/8] Loss_D: 0.7680 Loss_G: 9.5443\n",
      "[16/301][4/8] Loss_D: 0.4372 Loss_G: 13.4238\n",
      "[16/301][5/8] Loss_D: 0.9289 Loss_G: 4.4034\n",
      "[16/301][6/8] Loss_D: 3.1560 Loss_G: 17.4628\n",
      "[16/301][7/8] Loss_D: 3.8265 Loss_G: 8.0716\n",
      "[17/301][0/8] Loss_D: 0.4595 Loss_G: 6.7533\n",
      "[17/301][1/8] Loss_D: 1.4027 Loss_G: 16.1427\n",
      "[17/301][2/8] Loss_D: 1.4899 Loss_G: 8.2622\n",
      "[17/301][3/8] Loss_D: 0.3852 Loss_G: 6.9280\n",
      "[17/301][4/8] Loss_D: 0.9229 Loss_G: 14.3105\n",
      "[17/301][5/8] Loss_D: 2.6488 Loss_G: 3.6798\n",
      "[17/301][6/8] Loss_D: 3.7202 Loss_G: 16.6519\n",
      "[17/301][7/8] Loss_D: 4.1141 Loss_G: 4.8725\n",
      "[18/301][0/8] Loss_D: 1.7299 Loss_G: 11.9657\n",
      "[18/301][1/8] Loss_D: 0.1598 Loss_G: 10.9012\n",
      "[18/301][2/8] Loss_D: 0.5852 Loss_G: 2.1055\n",
      "[18/301][3/8] Loss_D: 3.9559 Loss_G: 13.2171\n",
      "[18/301][4/8] Loss_D: 2.9876 Loss_G: 5.6385\n",
      "[18/301][5/8] Loss_D: 0.9567 Loss_G: 8.2072\n",
      "[18/301][6/8] Loss_D: 0.2458 Loss_G: 7.3967\n",
      "[18/301][7/8] Loss_D: 0.3898 Loss_G: 5.0263\n",
      "[19/301][0/8] Loss_D: 1.0908 Loss_G: 11.2478\n",
      "[19/301][1/8] Loss_D: 3.4484 Loss_G: 0.9447\n",
      "[19/301][2/8] Loss_D: 3.3804 Loss_G: 12.5480\n",
      "[19/301][3/8] Loss_D: 1.5224 Loss_G: 6.4917\n",
      "[19/301][4/8] Loss_D: 0.3135 Loss_G: 5.2419\n",
      "[19/301][5/8] Loss_D: 0.9417 Loss_G: 11.5059\n",
      "[19/301][6/8] Loss_D: 1.2765 Loss_G: 3.8360\n",
      "[19/301][7/8] Loss_D: 1.1633 Loss_G: 8.2485\n",
      "[20/301][0/8] Loss_D: 0.2595 Loss_G: 7.6976\n",
      "[20/301][1/8] Loss_D: 0.9162 Loss_G: 1.7917\n",
      "[20/301][2/8] Loss_D: 2.6522 Loss_G: 13.6793\n",
      "[20/301][3/8] Loss_D: 3.9274 Loss_G: 5.5590\n",
      "[20/301][4/8] Loss_D: 0.4424 Loss_G: 4.8783\n",
      "[20/301][5/8] Loss_D: 1.0905 Loss_G: 13.0261\n",
      "[20/301][6/8] Loss_D: 3.2985 Loss_G: 4.3298\n",
      "[20/301][7/8] Loss_D: 1.3076 Loss_G: 11.2702\n",
      "[21/301][0/8] Loss_D: 0.7487 Loss_G: 5.2291\n",
      "[21/301][1/8] Loss_D: 0.7712 Loss_G: 8.2212\n",
      "[21/301][2/8] Loss_D: 0.4132 Loss_G: 5.1426\n",
      "[21/301][3/8] Loss_D: 0.8318 Loss_G: 9.5030\n",
      "[21/301][4/8] Loss_D: 1.8480 Loss_G: 1.1354\n",
      "[21/301][5/8] Loss_D: 3.4983 Loss_G: 12.2968\n",
      "[21/301][6/8] Loss_D: 1.4246 Loss_G: 8.7436\n",
      "[21/301][7/8] Loss_D: 0.0374 Loss_G: 4.5095\n",
      "[22/301][0/8] Loss_D: 0.7468 Loss_G: 8.3901\n",
      "[22/301][1/8] Loss_D: 0.5660 Loss_G: 4.7971\n",
      "[22/301][2/8] Loss_D: 0.6477 Loss_G: 7.0047\n",
      "[22/301][3/8] Loss_D: 0.5498 Loss_G: 3.3684\n",
      "[22/301][4/8] Loss_D: 1.3135 Loss_G: 10.8457\n",
      "[22/301][5/8] Loss_D: 2.3747 Loss_G: 3.0876\n",
      "[22/301][6/8] Loss_D: 1.5950 Loss_G: 11.4731\n",
      "[22/301][7/8] Loss_D: 1.3806 Loss_G: 5.6907\n",
      "[23/301][0/8] Loss_D: 0.3143 Loss_G: 5.4859\n",
      "[23/301][1/8] Loss_D: 0.6458 Loss_G: 10.3238\n",
      "[23/301][2/8] Loss_D: 1.5984 Loss_G: 2.2863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/301][3/8] Loss_D: 2.9614 Loss_G: 12.7356\n",
      "[23/301][4/8] Loss_D: 2.3796 Loss_G: 6.5827\n",
      "[23/301][5/8] Loss_D: 0.1614 Loss_G: 3.5483\n",
      "[23/301][6/8] Loss_D: 1.3261 Loss_G: 11.5704\n",
      "[23/301][7/8] Loss_D: 2.9028 Loss_G: 4.8181\n",
      "[24/301][0/8] Loss_D: 0.6967 Loss_G: 7.0381\n",
      "[24/301][1/8] Loss_D: 0.5383 Loss_G: 4.5982\n",
      "[24/301][2/8] Loss_D: 1.2374 Loss_G: 9.7517\n",
      "[24/301][3/8] Loss_D: 1.6973 Loss_G: 2.8993\n",
      "[24/301][4/8] Loss_D: 2.2829 Loss_G: 12.6615\n",
      "[24/301][5/8] Loss_D: 1.7228 Loss_G: 8.7533\n",
      "[24/301][6/8] Loss_D: 0.0928 Loss_G: 4.1826\n",
      "[24/301][7/8] Loss_D: 0.8166 Loss_G: 8.6662\n",
      "[25/301][0/8] Loss_D: 0.2644 Loss_G: 6.8829\n",
      "[25/301][1/8] Loss_D: 0.3512 Loss_G: 3.1881\n",
      "[25/301][2/8] Loss_D: 1.3938 Loss_G: 11.0187\n",
      "[25/301][3/8] Loss_D: 2.5142 Loss_G: 4.8814\n",
      "[25/301][4/8] Loss_D: 0.4418 Loss_G: 4.9104\n",
      "[25/301][5/8] Loss_D: 0.5711 Loss_G: 8.6024\n",
      "[25/301][6/8] Loss_D: 0.6598 Loss_G: 5.2133\n",
      "[25/301][7/8] Loss_D: 0.5043 Loss_G: 5.4923\n",
      "[26/301][0/8] Loss_D: 0.4204 Loss_G: 7.2728\n",
      "[26/301][1/8] Loss_D: 0.5686 Loss_G: 3.4438\n",
      "[26/301][2/8] Loss_D: 1.1000 Loss_G: 11.3486\n",
      "[26/301][3/8] Loss_D: 1.9888 Loss_G: 5.5800\n",
      "[26/301][4/8] Loss_D: 0.2670 Loss_G: 3.5062\n",
      "[26/301][5/8] Loss_D: 1.0927 Loss_G: 12.6392\n",
      "[26/301][6/8] Loss_D: 1.8887 Loss_G: 7.9117\n",
      "[26/301][7/8] Loss_D: 0.2233 Loss_G: 2.4090\n",
      "[27/301][0/8] Loss_D: 2.2914 Loss_G: 14.3084\n",
      "[27/301][1/8] Loss_D: 3.2051 Loss_G: 7.6226\n",
      "[27/301][2/8] Loss_D: 0.1154 Loss_G: 3.7651\n",
      "[27/301][3/8] Loss_D: 1.8231 Loss_G: 14.7817\n",
      "[27/301][4/8] Loss_D: 5.7554 Loss_G: 6.2705\n",
      "[27/301][5/8] Loss_D: 0.2137 Loss_G: 3.6803\n",
      "[27/301][6/8] Loss_D: 1.6195 Loss_G: 13.6332\n",
      "[27/301][7/8] Loss_D: 4.5076 Loss_G: 5.7552\n",
      "[28/301][0/8] Loss_D: 0.2527 Loss_G: 3.5748\n",
      "[28/301][1/8] Loss_D: 1.3483 Loss_G: 10.5160\n",
      "[28/301][2/8] Loss_D: 1.5738 Loss_G: 7.2575\n",
      "[28/301][3/8] Loss_D: 0.7533 Loss_G: 1.6679\n",
      "[28/301][4/8] Loss_D: 2.4821 Loss_G: 9.9846\n",
      "[28/301][5/8] Loss_D: 1.9206 Loss_G: 6.6727\n",
      "[28/301][6/8] Loss_D: 0.3782 Loss_G: 2.2052\n",
      "[28/301][7/8] Loss_D: 2.0568 Loss_G: 10.0264\n",
      "[29/301][0/8] Loss_D: 1.0610 Loss_G: 8.1860\n",
      "[29/301][1/8] Loss_D: 0.2847 Loss_G: 4.0108\n",
      "[29/301][2/8] Loss_D: 0.3940 Loss_G: 4.3793\n",
      "[29/301][3/8] Loss_D: 0.5339 Loss_G: 6.9097\n",
      "[29/301][4/8] Loss_D: 0.9104 Loss_G: 2.6101\n",
      "[29/301][5/8] Loss_D: 1.2186 Loss_G: 8.9948\n",
      "[29/301][6/8] Loss_D: 1.1231 Loss_G: 5.8649\n",
      "[29/301][7/8] Loss_D: 0.1507 Loss_G: 3.0724\n",
      "[30/301][0/8] Loss_D: 0.8134 Loss_G: 8.0130\n",
      "[30/301][1/8] Loss_D: 0.7592 Loss_G: 5.1334\n",
      "[30/301][2/8] Loss_D: 0.3717 Loss_G: 2.3968\n",
      "[30/301][3/8] Loss_D: 1.1549 Loss_G: 9.6973\n",
      "[30/301][4/8] Loss_D: 1.9741 Loss_G: 5.7681\n",
      "[30/301][5/8] Loss_D: 0.1271 Loss_G: 2.6753\n",
      "[30/301][6/8] Loss_D: 0.9727 Loss_G: 8.1285\n",
      "[30/301][7/8] Loss_D: 0.4060 Loss_G: 7.8999\n",
      "[31/301][0/8] Loss_D: 0.4502 Loss_G: 4.1057\n",
      "[31/301][1/8] Loss_D: 0.3023 Loss_G: 3.4789\n",
      "[31/301][2/8] Loss_D: 0.5160 Loss_G: 7.5189\n",
      "[31/301][3/8] Loss_D: 0.5456 Loss_G: 5.2209\n",
      "[31/301][4/8] Loss_D: 0.5479 Loss_G: 1.6382\n",
      "[31/301][5/8] Loss_D: 1.4089 Loss_G: 10.2895\n",
      "[31/301][6/8] Loss_D: 1.1926 Loss_G: 7.5552\n",
      "[31/301][7/8] Loss_D: 0.6905 Loss_G: 1.4828\n",
      "[32/301][0/8] Loss_D: 2.0773 Loss_G: 9.8360\n",
      "[32/301][1/8] Loss_D: 0.0600 Loss_G: 11.8964\n",
      "[32/301][2/8] Loss_D: 1.1928 Loss_G: 7.0251\n",
      "[32/301][3/8] Loss_D: 0.0253 Loss_G: 3.4930\n",
      "[32/301][4/8] Loss_D: 0.4614 Loss_G: 5.3295\n",
      "[32/301][5/8] Loss_D: 0.1260 Loss_G: 6.1086\n",
      "[32/301][6/8] Loss_D: 0.5028 Loss_G: 2.4760\n",
      "[32/301][7/8] Loss_D: 1.1894 Loss_G: 6.8580\n",
      "[33/301][0/8] Loss_D: 0.0693 Loss_G: 7.4026\n",
      "[33/301][1/8] Loss_D: 0.3048 Loss_G: 3.8768\n",
      "[33/301][2/8] Loss_D: 0.9325 Loss_G: 9.0626\n",
      "[33/301][3/8] Loss_D: 1.7813 Loss_G: 2.7106\n",
      "[33/301][4/8] Loss_D: 1.8657 Loss_G: 11.7378\n",
      "[33/301][5/8] Loss_D: 1.7816 Loss_G: 7.9696\n",
      "[33/301][6/8] Loss_D: 0.6069 Loss_G: 1.8351\n",
      "[33/301][7/8] Loss_D: 2.4158 Loss_G: 11.9613\n",
      "[34/301][0/8] Loss_D: 1.3458 Loss_G: 10.5844\n",
      "[34/301][1/8] Loss_D: 0.8533 Loss_G: 5.3663\n",
      "[34/301][2/8] Loss_D: 0.2552 Loss_G: 3.4483\n",
      "[34/301][3/8] Loss_D: 0.8490 Loss_G: 9.2157\n",
      "[34/301][4/8] Loss_D: 1.1420 Loss_G: 6.4516\n",
      "[34/301][5/8] Loss_D: 0.2460 Loss_G: 3.0995\n",
      "[34/301][6/8] Loss_D: 1.1088 Loss_G: 9.5761\n",
      "[34/301][7/8] Loss_D: 1.2777 Loss_G: 5.7690\n",
      "[35/301][0/8] Loss_D: 0.2267 Loss_G: 3.1988\n",
      "[35/301][1/8] Loss_D: 1.1368 Loss_G: 11.4773\n",
      "[35/301][2/8] Loss_D: 1.3125 Loss_G: 9.0297\n",
      "[35/301][3/8] Loss_D: 0.3890 Loss_G: 4.3604\n",
      "[35/301][4/8] Loss_D: 0.5518 Loss_G: 7.1165\n",
      "[35/301][5/8] Loss_D: 0.1575 Loss_G: 6.2731\n",
      "[35/301][6/8] Loss_D: 0.7296 Loss_G: 2.5934\n",
      "[35/301][7/8] Loss_D: 1.7533 Loss_G: 11.3525\n",
      "[36/301][0/8] Loss_D: 0.8262 Loss_G: 7.6027\n",
      "[36/301][1/8] Loss_D: 0.1728 Loss_G: 3.5920\n",
      "[36/301][2/8] Loss_D: 1.1720 Loss_G: 9.2060\n",
      "[36/301][3/8] Loss_D: 0.9408 Loss_G: 6.2671\n",
      "[36/301][4/8] Loss_D: 0.2171 Loss_G: 3.0891\n",
      "[36/301][5/8] Loss_D: 0.8891 Loss_G: 8.0643\n",
      "[36/301][6/8] Loss_D: 0.9470 Loss_G: 4.8457\n",
      "[36/301][7/8] Loss_D: 0.2067 Loss_G: 3.9316\n",
      "[37/301][0/8] Loss_D: 0.5051 Loss_G: 6.9850\n",
      "[37/301][1/8] Loss_D: 0.7506 Loss_G: 3.1557\n",
      "[37/301][2/8] Loss_D: 0.6681 Loss_G: 5.8973\n",
      "[37/301][3/8] Loss_D: 0.5896 Loss_G: 3.0810\n",
      "[37/301][4/8] Loss_D: 0.8730 Loss_G: 7.9088\n",
      "[37/301][5/8] Loss_D: 0.8511 Loss_G: 3.9085\n",
      "[37/301][6/8] Loss_D: 0.4979 Loss_G: 4.1937\n",
      "[37/301][7/8] Loss_D: 0.3107 Loss_G: 6.4951\n",
      "[38/301][0/8] Loss_D: 0.7165 Loss_G: 2.1247\n",
      "[38/301][1/8] Loss_D: 1.2452 Loss_G: 8.2396\n",
      "[38/301][2/8] Loss_D: 1.5285 Loss_G: 3.3417\n",
      "[38/301][3/8] Loss_D: 0.6331 Loss_G: 4.2634\n",
      "[38/301][4/8] Loss_D: 0.4539 Loss_G: 4.4282\n",
      "[38/301][5/8] Loss_D: 0.3006 Loss_G: 3.8170\n",
      "[38/301][6/8] Loss_D: 0.4724 Loss_G: 3.8033\n",
      "[38/301][7/8] Loss_D: 0.3983 Loss_G: 4.0731\n",
      "[39/301][0/8] Loss_D: 0.4373 Loss_G: 4.0946\n",
      "[39/301][1/8] Loss_D: 0.4491 Loss_G: 3.8221\n",
      "[39/301][2/8] Loss_D: 0.5336 Loss_G: 2.8796\n",
      "[39/301][3/8] Loss_D: 0.6010 Loss_G: 5.3481\n",
      "[39/301][4/8] Loss_D: 0.8779 Loss_G: 0.9823\n",
      "[39/301][5/8] Loss_D: 1.2883 Loss_G: 7.6239\n",
      "[39/301][6/8] Loss_D: 1.3783 Loss_G: 2.9885\n",
      "[39/301][7/8] Loss_D: 0.3620 Loss_G: 3.7814\n",
      "[40/301][0/8] Loss_D: 0.4483 Loss_G: 5.7697\n",
      "[40/301][1/8] Loss_D: 0.7726 Loss_G: 1.3083\n",
      "[40/301][2/8] Loss_D: 1.3401 Loss_G: 7.8532\n",
      "[40/301][3/8] Loss_D: 1.3268 Loss_G: 3.1049\n",
      "[40/301][4/8] Loss_D: 0.7049 Loss_G: 4.3829\n",
      "[40/301][5/8] Loss_D: 0.4757 Loss_G: 3.0786\n",
      "[40/301][6/8] Loss_D: 0.5611 Loss_G: 4.2831\n",
      "[40/301][7/8] Loss_D: 0.6033 Loss_G: 1.6585\n",
      "[41/301][0/8] Loss_D: 1.2360 Loss_G: 8.5388\n",
      "[41/301][1/8] Loss_D: 2.5853 Loss_G: 1.6880\n",
      "[41/301][2/8] Loss_D: 0.8855 Loss_G: 7.1371\n",
      "[41/301][3/8] Loss_D: 2.2929 Loss_G: 0.0931\n",
      "[41/301][4/8] Loss_D: 3.3178 Loss_G: 9.2392\n",
      "[41/301][5/8] Loss_D: 2.4212 Loss_G: 3.1419\n",
      "[41/301][6/8] Loss_D: 0.3258 Loss_G: 3.1346\n",
      "[41/301][7/8] Loss_D: 0.4388 Loss_G: 6.3465\n",
      "[42/301][0/8] Loss_D: 0.8712 Loss_G: 1.1866\n",
      "[42/301][1/8] Loss_D: 1.4601 Loss_G: 6.5368\n",
      "[42/301][2/8] Loss_D: 1.0415 Loss_G: 3.2198\n",
      "[42/301][3/8] Loss_D: 0.4912 Loss_G: 2.9445\n",
      "[42/301][4/8] Loss_D: 0.6928 Loss_G: 4.9122\n",
      "[42/301][5/8] Loss_D: 1.0657 Loss_G: 1.2213\n",
      "[42/301][6/8] Loss_D: 1.5706 Loss_G: 6.7660\n",
      "[42/301][7/8] Loss_D: 2.1783 Loss_G: 0.9849\n",
      "[43/301][0/8] Loss_D: 1.4666 Loss_G: 6.2541\n",
      "[43/301][1/8] Loss_D: 0.9041 Loss_G: 3.2052\n",
      "[43/301][2/8] Loss_D: 0.4056 Loss_G: 3.5133\n",
      "[43/301][3/8] Loss_D: 0.3869 Loss_G: 4.8441\n",
      "[43/301][4/8] Loss_D: 0.5812 Loss_G: 2.2689\n",
      "[43/301][5/8] Loss_D: 0.7199 Loss_G: 4.7308\n",
      "[43/301][6/8] Loss_D: 0.9461 Loss_G: 1.2088\n",
      "[43/301][7/8] Loss_D: 1.4260 Loss_G: 6.8232\n",
      "[44/301][0/8] Loss_D: 1.5561 Loss_G: 1.8034\n",
      "[44/301][1/8] Loss_D: 0.8704 Loss_G: 4.5903\n",
      "[44/301][2/8] Loss_D: 0.3342 Loss_G: 4.0340\n",
      "[44/301][3/8] Loss_D: 0.3777 Loss_G: 2.6308\n",
      "[44/301][4/8] Loss_D: 0.5927 Loss_G: 5.0964\n",
      "[44/301][5/8] Loss_D: 0.9203 Loss_G: 1.1598\n",
      "[44/301][6/8] Loss_D: 1.2874 Loss_G: 6.7135\n",
      "[44/301][7/8] Loss_D: 1.2650 Loss_G: 2.6064\n",
      "[45/301][0/8] Loss_D: 0.5610 Loss_G: 3.6124\n",
      "[45/301][1/8] Loss_D: 0.5067 Loss_G: 4.3070\n",
      "[45/301][2/8] Loss_D: 0.5847 Loss_G: 2.1843\n",
      "[45/301][3/8] Loss_D: 0.9757 Loss_G: 6.5932\n",
      "[45/301][4/8] Loss_D: 1.3066 Loss_G: 1.7617\n",
      "[45/301][5/8] Loss_D: 1.3823 Loss_G: 7.8440\n",
      "[45/301][6/8] Loss_D: 1.9681 Loss_G: 1.1320\n",
      "[45/301][7/8] Loss_D: 1.4301 Loss_G: 8.7703\n",
      "[46/301][0/8] Loss_D: 0.8726 Loss_G: 4.0368\n",
      "[46/301][1/8] Loss_D: 0.4142 Loss_G: 3.3544\n",
      "[46/301][2/8] Loss_D: 0.4785 Loss_G: 5.0826\n",
      "[46/301][3/8] Loss_D: 0.5937 Loss_G: 2.4938\n",
      "[46/301][4/8] Loss_D: 0.8873 Loss_G: 5.7702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/301][5/8] Loss_D: 1.3201 Loss_G: 0.9762\n",
      "[46/301][6/8] Loss_D: 1.8026 Loss_G: 7.5510\n",
      "[46/301][7/8] Loss_D: 1.7382 Loss_G: 3.2130\n",
      "[47/301][0/8] Loss_D: 0.6688 Loss_G: 4.6313\n",
      "[47/301][1/8] Loss_D: 0.4573 Loss_G: 3.5055\n",
      "[47/301][2/8] Loss_D: 0.5092 Loss_G: 3.2576\n",
      "[47/301][3/8] Loss_D: 0.6347 Loss_G: 4.2549\n",
      "[47/301][4/8] Loss_D: 0.7104 Loss_G: 1.7998\n",
      "[47/301][5/8] Loss_D: 1.1471 Loss_G: 7.7910\n",
      "[47/301][6/8] Loss_D: 2.6654 Loss_G: 1.2896\n",
      "[47/301][7/8] Loss_D: 1.4518 Loss_G: 8.7115\n",
      "[48/301][0/8] Loss_D: 1.6560 Loss_G: 1.6448\n",
      "[48/301][1/8] Loss_D: 1.0734 Loss_G: 5.9298\n",
      "[48/301][2/8] Loss_D: 0.5668 Loss_G: 3.6805\n",
      "[48/301][3/8] Loss_D: 0.3513 Loss_G: 3.3790\n",
      "[48/301][4/8] Loss_D: 0.5532 Loss_G: 3.8059\n",
      "[48/301][5/8] Loss_D: 0.9141 Loss_G: 1.9893\n",
      "[48/301][6/8] Loss_D: 0.9451 Loss_G: 6.4794\n",
      "[48/301][7/8] Loss_D: 1.3062 Loss_G: 1.7704\n",
      "[49/301][0/8] Loss_D: 1.2292 Loss_G: 6.3170\n",
      "[49/301][1/8] Loss_D: 1.0365 Loss_G: 2.4784\n",
      "[49/301][2/8] Loss_D: 0.6891 Loss_G: 4.6786\n",
      "[49/301][3/8] Loss_D: 0.4715 Loss_G: 3.3548\n",
      "[49/301][4/8] Loss_D: 0.5940 Loss_G: 4.2787\n",
      "[49/301][5/8] Loss_D: 0.5208 Loss_G: 3.1931\n",
      "[49/301][6/8] Loss_D: 0.6422 Loss_G: 4.0834\n",
      "[49/301][7/8] Loss_D: 0.8191 Loss_G: 1.8724\n",
      "[50/301][0/8] Loss_D: 1.5524 Loss_G: 9.1098\n",
      "[50/301][1/8] Loss_D: 2.9514 Loss_G: 2.3179\n",
      "[50/301][2/8] Loss_D: 0.6944 Loss_G: 4.7409\n",
      "[50/301][3/8] Loss_D: 0.6352 Loss_G: 2.8719\n",
      "[50/301][4/8] Loss_D: 0.8429 Loss_G: 6.2562\n",
      "[50/301][5/8] Loss_D: 1.3687 Loss_G: 0.6250\n",
      "[50/301][6/8] Loss_D: 2.0821 Loss_G: 8.8892\n",
      "[50/301][7/8] Loss_D: 1.0612 Loss_G: 5.1498\n",
      "[51/301][0/8] Loss_D: 0.2529 Loss_G: 2.3544\n",
      "[51/301][1/8] Loss_D: 0.9449 Loss_G: 6.1353\n",
      "[51/301][2/8] Loss_D: 1.0323 Loss_G: 2.2942\n",
      "[51/301][3/8] Loss_D: 0.9084 Loss_G: 5.4857\n",
      "[51/301][4/8] Loss_D: 0.7378 Loss_G: 2.6044\n",
      "[51/301][5/8] Loss_D: 0.6974 Loss_G: 4.1173\n",
      "[51/301][6/8] Loss_D: 0.8572 Loss_G: 1.7838\n",
      "[51/301][7/8] Loss_D: 0.6608 Loss_G: 5.3925\n",
      "[52/301][0/8] Loss_D: 0.6863 Loss_G: 2.1717\n",
      "[52/301][1/8] Loss_D: 0.7030 Loss_G: 5.2535\n",
      "[52/301][2/8] Loss_D: 0.9402 Loss_G: 1.3300\n",
      "[52/301][3/8] Loss_D: 1.1439 Loss_G: 6.5263\n",
      "[52/301][4/8] Loss_D: 1.0981 Loss_G: 2.6054\n",
      "[52/301][5/8] Loss_D: 0.5202 Loss_G: 3.9149\n",
      "[52/301][6/8] Loss_D: 0.4222 Loss_G: 3.9142\n",
      "[52/301][7/8] Loss_D: 0.6541 Loss_G: 1.6538\n",
      "[53/301][0/8] Loss_D: 1.1654 Loss_G: 7.5346\n",
      "[53/301][1/8] Loss_D: 1.8716 Loss_G: 1.7946\n",
      "[53/301][2/8] Loss_D: 0.9391 Loss_G: 5.4059\n",
      "[53/301][3/8] Loss_D: 0.8351 Loss_G: 1.9462\n",
      "[53/301][4/8] Loss_D: 0.7231 Loss_G: 5.3549\n",
      "[53/301][5/8] Loss_D: 0.5859 Loss_G: 2.9955\n",
      "[53/301][6/8] Loss_D: 0.5635 Loss_G: 4.4525\n",
      "[53/301][7/8] Loss_D: 0.6306 Loss_G: 2.0514\n",
      "[54/301][0/8] Loss_D: 0.7145 Loss_G: 5.5353\n",
      "[54/301][1/8] Loss_D: 1.0744 Loss_G: 0.9748\n",
      "[54/301][2/8] Loss_D: 1.5184 Loss_G: 7.5138\n",
      "[54/301][3/8] Loss_D: 1.7086 Loss_G: 2.2123\n",
      "[54/301][4/8] Loss_D: 0.6470 Loss_G: 4.7995\n",
      "[54/301][5/8] Loss_D: 0.3361 Loss_G: 3.6430\n",
      "[54/301][6/8] Loss_D: 0.4674 Loss_G: 2.8168\n",
      "[54/301][7/8] Loss_D: 0.8224 Loss_G: 3.6570\n",
      "[55/301][0/8] Loss_D: 0.6278 Loss_G: 3.6164\n",
      "[55/301][1/8] Loss_D: 0.7185 Loss_G: 2.0793\n",
      "[55/301][2/8] Loss_D: 0.8642 Loss_G: 6.1122\n",
      "[55/301][3/8] Loss_D: 1.5664 Loss_G: 0.5276\n",
      "[55/301][4/8] Loss_D: 2.0287 Loss_G: 7.2886\n",
      "[55/301][5/8] Loss_D: 1.2429 Loss_G: 3.6076\n",
      "[55/301][6/8] Loss_D: 0.2843 Loss_G: 2.5855\n",
      "[55/301][7/8] Loss_D: 0.6149 Loss_G: 6.1850\n",
      "[56/301][0/8] Loss_D: 0.7861 Loss_G: 1.8535\n",
      "[56/301][1/8] Loss_D: 1.0731 Loss_G: 6.7061\n",
      "[56/301][2/8] Loss_D: 1.3067 Loss_G: 2.5718\n",
      "[56/301][3/8] Loss_D: 0.7000 Loss_G: 3.9932\n",
      "[56/301][4/8] Loss_D: 0.4867 Loss_G: 3.4398\n",
      "[56/301][5/8] Loss_D: 0.5995 Loss_G: 2.0319\n",
      "[56/301][6/8] Loss_D: 0.7288 Loss_G: 5.6082\n",
      "[56/301][7/8] Loss_D: 1.5388 Loss_G: 0.5491\n",
      "[57/301][0/8] Loss_D: 2.1258 Loss_G: 8.7332\n",
      "[57/301][1/8] Loss_D: 2.7419 Loss_G: 2.2130\n",
      "[57/301][2/8] Loss_D: 0.7995 Loss_G: 5.6411\n",
      "[57/301][3/8] Loss_D: 0.6409 Loss_G: 2.5594\n",
      "[57/301][4/8] Loss_D: 0.7535 Loss_G: 5.1999\n",
      "[57/301][5/8] Loss_D: 0.9294 Loss_G: 0.9835\n",
      "[57/301][6/8] Loss_D: 1.8845 Loss_G: 7.6129\n",
      "[57/301][7/8] Loss_D: 2.9070 Loss_G: 1.5619\n",
      "[58/301][0/8] Loss_D: 0.9308 Loss_G: 4.8974\n",
      "[58/301][1/8] Loss_D: 0.8468 Loss_G: 1.8480\n",
      "[58/301][2/8] Loss_D: 1.0178 Loss_G: 5.7961\n",
      "[58/301][3/8] Loss_D: 1.2711 Loss_G: 1.6497\n",
      "[58/301][4/8] Loss_D: 0.8536 Loss_G: 4.7225\n",
      "[58/301][5/8] Loss_D: 0.5006 Loss_G: 2.9346\n",
      "[58/301][6/8] Loss_D: 0.4916 Loss_G: 3.5444\n",
      "[58/301][7/8] Loss_D: 0.5454 Loss_G: 2.0993\n",
      "[59/301][0/8] Loss_D: 1.0338 Loss_G: 5.6085\n",
      "[59/301][1/8] Loss_D: 1.6048 Loss_G: 0.8017\n",
      "[59/301][2/8] Loss_D: 1.5276 Loss_G: 6.3842\n",
      "[59/301][3/8] Loss_D: 1.1067 Loss_G: 2.3000\n",
      "[59/301][4/8] Loss_D: 0.7666 Loss_G: 4.4078\n",
      "[59/301][5/8] Loss_D: 0.6604 Loss_G: 2.3975\n",
      "[59/301][6/8] Loss_D: 0.5389 Loss_G: 3.5658\n",
      "[59/301][7/8] Loss_D: 0.4299 Loss_G: 3.3540\n",
      "[60/301][0/8] Loss_D: 0.5660 Loss_G: 3.5197\n",
      "[60/301][1/8] Loss_D: 0.7238 Loss_G: 1.8721\n",
      "[60/301][2/8] Loss_D: 0.7687 Loss_G: 4.8430\n",
      "[60/301][3/8] Loss_D: 1.0884 Loss_G: 0.9071\n",
      "[60/301][4/8] Loss_D: 1.3009 Loss_G: 6.0428\n",
      "[60/301][5/8] Loss_D: 1.0136 Loss_G: 2.5924\n",
      "[60/301][6/8] Loss_D: 0.4624 Loss_G: 2.9811\n",
      "[60/301][7/8] Loss_D: 0.4432 Loss_G: 3.4293\n",
      "[61/301][0/8] Loss_D: 0.3804 Loss_G: 3.8196\n",
      "[61/301][1/8] Loss_D: 0.5938 Loss_G: 1.8078\n",
      "[61/301][2/8] Loss_D: 0.7677 Loss_G: 5.0490\n",
      "[61/301][3/8] Loss_D: 1.1466 Loss_G: 0.9299\n",
      "[61/301][4/8] Loss_D: 1.3467 Loss_G: 5.6983\n",
      "[61/301][5/8] Loss_D: 1.5801 Loss_G: 1.0784\n",
      "[61/301][6/8] Loss_D: 1.2131 Loss_G: 6.4547\n",
      "[61/301][7/8] Loss_D: 1.2120 Loss_G: 1.9025\n",
      "[62/301][0/8] Loss_D: 0.9374 Loss_G: 5.3601\n",
      "[62/301][1/8] Loss_D: 0.5864 Loss_G: 3.0495\n",
      "[62/301][2/8] Loss_D: 0.4699 Loss_G: 2.8328\n",
      "[62/301][3/8] Loss_D: 0.6838 Loss_G: 4.6457\n",
      "[62/301][4/8] Loss_D: 1.1914 Loss_G: 0.5248\n",
      "[62/301][5/8] Loss_D: 1.5670 Loss_G: 6.5137\n",
      "[62/301][6/8] Loss_D: 1.4904 Loss_G: 1.4907\n",
      "[62/301][7/8] Loss_D: 0.6572 Loss_G: 4.4463\n",
      "[63/301][0/8] Loss_D: 0.3852 Loss_G: 4.0515\n",
      "[63/301][1/8] Loss_D: 0.5315 Loss_G: 1.8712\n",
      "[63/301][2/8] Loss_D: 0.8986 Loss_G: 5.6910\n",
      "[63/301][3/8] Loss_D: 1.6455 Loss_G: 0.7570\n",
      "[63/301][4/8] Loss_D: 1.7292 Loss_G: 6.5022\n",
      "[63/301][5/8] Loss_D: 1.7885 Loss_G: 1.2658\n",
      "[63/301][6/8] Loss_D: 1.1149 Loss_G: 4.2683\n",
      "[63/301][7/8] Loss_D: 0.6586 Loss_G: 2.4946\n",
      "[64/301][0/8] Loss_D: 0.5520 Loss_G: 3.6112\n",
      "[64/301][1/8] Loss_D: 0.3721 Loss_G: 3.2020\n",
      "[64/301][2/8] Loss_D: 0.5851 Loss_G: 2.1726\n",
      "[64/301][3/8] Loss_D: 0.7326 Loss_G: 3.2751\n",
      "[64/301][4/8] Loss_D: 0.7893 Loss_G: 1.2151\n",
      "[64/301][5/8] Loss_D: 1.1683 Loss_G: 5.6127\n",
      "[64/301][6/8] Loss_D: 1.7981 Loss_G: 0.8785\n",
      "[64/301][7/8] Loss_D: 1.2528 Loss_G: 5.7532\n",
      "[65/301][0/8] Loss_D: 0.8577 Loss_G: 2.1644\n",
      "[65/301][1/8] Loss_D: 0.5244 Loss_G: 3.0302\n",
      "[65/301][2/8] Loss_D: 0.4643 Loss_G: 3.4135\n",
      "[65/301][3/8] Loss_D: 0.5590 Loss_G: 1.7110\n",
      "[65/301][4/8] Loss_D: 0.6804 Loss_G: 4.1028\n",
      "[65/301][5/8] Loss_D: 0.8900 Loss_G: 1.0204\n",
      "[65/301][6/8] Loss_D: 1.0478 Loss_G: 4.8281\n",
      "[65/301][7/8] Loss_D: 0.9308 Loss_G: 1.9208\n",
      "[66/301][0/8] Loss_D: 0.8617 Loss_G: 3.9987\n",
      "[66/301][1/8] Loss_D: 0.8663 Loss_G: 1.4857\n",
      "[66/301][2/8] Loss_D: 0.9471 Loss_G: 4.7993\n",
      "[66/301][3/8] Loss_D: 1.0690 Loss_G: 1.1707\n",
      "[66/301][4/8] Loss_D: 1.0344 Loss_G: 4.6815\n",
      "[66/301][5/8] Loss_D: 0.7685 Loss_G: 2.0392\n",
      "[66/301][6/8] Loss_D: 0.6743 Loss_G: 2.8495\n",
      "[66/301][7/8] Loss_D: 0.5612 Loss_G: 2.9481\n",
      "[67/301][0/8] Loss_D: 0.6034 Loss_G: 2.2335\n",
      "[67/301][1/8] Loss_D: 0.6326 Loss_G: 3.0883\n",
      "[67/301][2/8] Loss_D: 0.6646 Loss_G: 1.5669\n",
      "[67/301][3/8] Loss_D: 0.9296 Loss_G: 5.1066\n",
      "[67/301][4/8] Loss_D: 1.5077 Loss_G: 0.6956\n",
      "[67/301][5/8] Loss_D: 1.3101 Loss_G: 5.8676\n",
      "[67/301][6/8] Loss_D: 1.1660 Loss_G: 1.3300\n",
      "[67/301][7/8] Loss_D: 0.7745 Loss_G: 5.2885\n",
      "[68/301][0/8] Loss_D: 0.3584 Loss_G: 3.7969\n",
      "[68/301][1/8] Loss_D: 0.4624 Loss_G: 2.0365\n",
      "[68/301][2/8] Loss_D: 0.7035 Loss_G: 4.4610\n",
      "[68/301][3/8] Loss_D: 1.0838 Loss_G: 0.9498\n",
      "[68/301][4/8] Loss_D: 1.2784 Loss_G: 5.5635\n",
      "[68/301][5/8] Loss_D: 1.6599 Loss_G: 0.8444\n",
      "[68/301][6/8] Loss_D: 1.3017 Loss_G: 5.8218\n",
      "[68/301][7/8] Loss_D: 2.1875 Loss_G: 0.4959\n",
      "[69/301][0/8] Loss_D: 2.1699 Loss_G: 6.4631\n",
      "[69/301][1/8] Loss_D: 2.4878 Loss_G: 0.7029\n",
      "[69/301][2/8] Loss_D: 1.4173 Loss_G: 5.1354\n",
      "[69/301][3/8] Loss_D: 0.5201 Loss_G: 3.6188\n",
      "[69/301][4/8] Loss_D: 0.4395 Loss_G: 2.1990\n",
      "[69/301][5/8] Loss_D: 0.9060 Loss_G: 4.5689\n",
      "[69/301][6/8] Loss_D: 1.2180 Loss_G: 0.6887\n",
      "[69/301][7/8] Loss_D: 1.8092 Loss_G: 8.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/301][0/8] Loss_D: 1.8160 Loss_G: 1.6116\n",
      "[70/301][1/8] Loss_D: 1.1031 Loss_G: 2.7815\n",
      "[70/301][2/8] Loss_D: 0.5426 Loss_G: 3.7647\n",
      "[70/301][3/8] Loss_D: 0.7647 Loss_G: 1.3793\n",
      "[70/301][4/8] Loss_D: 0.8995 Loss_G: 3.7934\n",
      "[70/301][5/8] Loss_D: 0.7606 Loss_G: 1.9822\n",
      "[70/301][6/8] Loss_D: 0.6709 Loss_G: 2.4248\n",
      "[70/301][7/8] Loss_D: 0.4227 Loss_G: 2.8248\n",
      "[71/301][0/8] Loss_D: 0.7781 Loss_G: 3.8854\n",
      "[71/301][1/8] Loss_D: 0.9234 Loss_G: 1.0546\n",
      "[71/301][2/8] Loss_D: 0.9833 Loss_G: 4.6356\n",
      "[71/301][3/8] Loss_D: 1.0686 Loss_G: 1.3496\n",
      "[71/301][4/8] Loss_D: 0.8356 Loss_G: 4.0333\n",
      "[71/301][5/8] Loss_D: 0.5779 Loss_G: 2.3802\n",
      "[71/301][6/8] Loss_D: 0.4663 Loss_G: 2.9837\n",
      "[71/301][7/8] Loss_D: 0.5380 Loss_G: 3.0867\n",
      "[72/301][0/8] Loss_D: 0.5009 Loss_G: 2.0890\n",
      "[72/301][1/8] Loss_D: 0.7769 Loss_G: 3.2473\n",
      "[72/301][2/8] Loss_D: 0.6724 Loss_G: 1.8484\n",
      "[72/301][3/8] Loss_D: 0.8094 Loss_G: 3.7362\n",
      "[72/301][4/8] Loss_D: 0.6540 Loss_G: 1.7555\n",
      "[72/301][5/8] Loss_D: 0.8142 Loss_G: 3.4523\n",
      "[72/301][6/8] Loss_D: 0.6188 Loss_G: 2.1061\n",
      "[72/301][7/8] Loss_D: 0.5516 Loss_G: 4.3861\n",
      "[73/301][0/8] Loss_D: 0.6023 Loss_G: 1.7631\n",
      "[73/301][1/8] Loss_D: 0.6974 Loss_G: 3.5410\n",
      "[73/301][2/8] Loss_D: 0.6538 Loss_G: 1.7247\n",
      "[73/301][3/8] Loss_D: 0.7825 Loss_G: 4.4414\n",
      "[73/301][4/8] Loss_D: 0.8932 Loss_G: 1.2749\n",
      "[73/301][5/8] Loss_D: 0.9880 Loss_G: 4.7073\n",
      "[73/301][6/8] Loss_D: 0.9024 Loss_G: 1.5769\n",
      "[73/301][7/8] Loss_D: 0.9836 Loss_G: 5.6029\n",
      "[74/301][0/8] Loss_D: 1.1156 Loss_G: 1.1672\n",
      "[74/301][1/8] Loss_D: 0.9865 Loss_G: 5.3020\n",
      "[74/301][2/8] Loss_D: 1.2951 Loss_G: 0.6793\n",
      "[74/301][3/8] Loss_D: 1.6036 Loss_G: 7.2201\n",
      "[74/301][4/8] Loss_D: 2.4504 Loss_G: 0.5625\n",
      "[74/301][5/8] Loss_D: 1.6701 Loss_G: 5.4219\n",
      "[74/301][6/8] Loss_D: 0.7781 Loss_G: 2.8141\n",
      "[74/301][7/8] Loss_D: 0.5157 Loss_G: 3.0789\n",
      "[75/301][0/8] Loss_D: 0.4567 Loss_G: 3.1936\n",
      "[75/301][1/8] Loss_D: 0.5168 Loss_G: 2.6234\n",
      "[75/301][2/8] Loss_D: 0.7731 Loss_G: 1.8828\n",
      "[75/301][3/8] Loss_D: 0.8111 Loss_G: 3.5759\n",
      "[75/301][4/8] Loss_D: 1.2117 Loss_G: 0.4230\n",
      "[75/301][5/8] Loss_D: 1.7248 Loss_G: 5.6523\n",
      "[75/301][6/8] Loss_D: 1.7813 Loss_G: 1.3657\n",
      "[75/301][7/8] Loss_D: 0.5499 Loss_G: 3.1001\n",
      "[76/301][0/8] Loss_D: 0.6107 Loss_G: 5.0235\n",
      "[76/301][1/8] Loss_D: 1.0260 Loss_G: 0.6455\n",
      "[76/301][2/8] Loss_D: 1.2587 Loss_G: 5.3933\n",
      "[76/301][3/8] Loss_D: 0.7919 Loss_G: 2.2340\n",
      "[76/301][4/8] Loss_D: 0.5601 Loss_G: 2.6132\n",
      "[76/301][5/8] Loss_D: 0.4598 Loss_G: 3.1947\n",
      "[76/301][6/8] Loss_D: 0.5399 Loss_G: 2.3011\n",
      "[76/301][7/8] Loss_D: 0.7249 Loss_G: 2.9676\n",
      "[77/301][0/8] Loss_D: 0.7483 Loss_G: 1.9002\n",
      "[77/301][1/8] Loss_D: 0.7415 Loss_G: 2.9962\n",
      "[77/301][2/8] Loss_D: 0.7633 Loss_G: 1.6529\n",
      "[77/301][3/8] Loss_D: 0.8121 Loss_G: 4.1589\n",
      "[77/301][4/8] Loss_D: 1.1853 Loss_G: 0.5885\n",
      "[77/301][5/8] Loss_D: 1.6369 Loss_G: 5.9109\n",
      "[77/301][6/8] Loss_D: 1.5145 Loss_G: 1.2914\n",
      "[77/301][7/8] Loss_D: 0.6767 Loss_G: 4.3664\n",
      "[78/301][0/8] Loss_D: 0.4163 Loss_G: 3.2578\n",
      "[78/301][1/8] Loss_D: 0.5815 Loss_G: 1.6126\n",
      "[78/301][2/8] Loss_D: 0.8969 Loss_G: 4.1990\n",
      "[78/301][3/8] Loss_D: 1.0236 Loss_G: 1.1614\n",
      "[78/301][4/8] Loss_D: 1.1054 Loss_G: 4.6294\n",
      "[78/301][5/8] Loss_D: 1.0296 Loss_G: 1.4484\n",
      "[78/301][6/8] Loss_D: 0.7071 Loss_G: 3.6528\n",
      "[78/301][7/8] Loss_D: 0.5047 Loss_G: 2.5242\n",
      "[79/301][0/8] Loss_D: 0.5909 Loss_G: 2.3489\n",
      "[79/301][1/8] Loss_D: 0.5674 Loss_G: 3.1991\n",
      "[79/301][2/8] Loss_D: 0.8234 Loss_G: 1.0319\n",
      "[79/301][3/8] Loss_D: 1.0390 Loss_G: 4.7062\n",
      "[79/301][4/8] Loss_D: 1.2063 Loss_G: 1.0408\n",
      "[79/301][5/8] Loss_D: 0.9904 Loss_G: 4.2914\n",
      "[79/301][6/8] Loss_D: 0.7280 Loss_G: 1.7048\n",
      "[79/301][7/8] Loss_D: 0.5154 Loss_G: 2.8672\n",
      "[80/301][0/8] Loss_D: 0.5169 Loss_G: 4.2298\n",
      "[80/301][1/8] Loss_D: 0.7089 Loss_G: 1.4982\n",
      "[80/301][2/8] Loss_D: 0.7089 Loss_G: 3.2974\n",
      "[80/301][3/8] Loss_D: 0.6285 Loss_G: 1.8755\n",
      "[80/301][4/8] Loss_D: 0.5126 Loss_G: 2.9519\n",
      "[80/301][5/8] Loss_D: 0.5753 Loss_G: 2.7112\n",
      "[80/301][6/8] Loss_D: 0.8159 Loss_G: 1.2077\n",
      "[80/301][7/8] Loss_D: 0.7714 Loss_G: 5.4527\n",
      "[81/301][0/8] Loss_D: 1.3111 Loss_G: 0.7417\n",
      "[81/301][1/8] Loss_D: 1.4165 Loss_G: 5.8798\n",
      "[81/301][2/8] Loss_D: 1.9162 Loss_G: 0.5277\n",
      "[81/301][3/8] Loss_D: 1.5317 Loss_G: 6.2148\n",
      "[81/301][4/8] Loss_D: 1.7739 Loss_G: 0.5671\n",
      "[81/301][5/8] Loss_D: 1.6562 Loss_G: 5.1415\n",
      "[81/301][6/8] Loss_D: 0.7953 Loss_G: 2.3567\n",
      "[81/301][7/8] Loss_D: 0.3460 Loss_G: 2.7758\n",
      "[82/301][0/8] Loss_D: 0.6188 Loss_G: 3.6232\n",
      "[82/301][1/8] Loss_D: 0.8438 Loss_G: 1.0620\n",
      "[82/301][2/8] Loss_D: 1.1545 Loss_G: 4.4219\n",
      "[82/301][3/8] Loss_D: 1.1020 Loss_G: 1.4606\n",
      "[82/301][4/8] Loss_D: 0.9260 Loss_G: 3.3298\n",
      "[82/301][5/8] Loss_D: 0.7317 Loss_G: 1.8305\n",
      "[82/301][6/8] Loss_D: 0.6455 Loss_G: 3.1841\n",
      "[82/301][7/8] Loss_D: 0.8888 Loss_G: 1.3989\n",
      "[83/301][0/8] Loss_D: 0.8026 Loss_G: 3.8400\n",
      "[83/301][1/8] Loss_D: 0.9476 Loss_G: 0.9530\n",
      "[83/301][2/8] Loss_D: 0.9890 Loss_G: 4.4084\n",
      "[83/301][3/8] Loss_D: 0.9102 Loss_G: 1.3466\n",
      "[83/301][4/8] Loss_D: 0.7408 Loss_G: 3.6629\n",
      "[83/301][5/8] Loss_D: 0.6254 Loss_G: 1.8096\n",
      "[83/301][6/8] Loss_D: 0.5794 Loss_G: 2.6041\n",
      "[83/301][7/8] Loss_D: 0.6203 Loss_G: 1.6205\n",
      "[84/301][0/8] Loss_D: 0.8679 Loss_G: 4.3165\n",
      "[84/301][1/8] Loss_D: 0.9293 Loss_G: 1.2955\n",
      "[84/301][2/8] Loss_D: 0.8035 Loss_G: 3.0446\n",
      "[84/301][3/8] Loss_D: 0.4682 Loss_G: 2.3344\n",
      "[84/301][4/8] Loss_D: 0.6144 Loss_G: 2.0251\n",
      "[84/301][5/8] Loss_D: 0.6044 Loss_G: 2.6852\n",
      "[84/301][6/8] Loss_D: 0.5288 Loss_G: 2.4570\n",
      "[84/301][7/8] Loss_D: 0.6529 Loss_G: 1.3213\n",
      "[85/301][0/8] Loss_D: 0.8618 Loss_G: 4.6516\n",
      "[85/301][1/8] Loss_D: 1.4998 Loss_G: 0.6913\n",
      "[85/301][2/8] Loss_D: 1.1656 Loss_G: 6.2350\n",
      "[85/301][3/8] Loss_D: 2.4388 Loss_G: 0.2467\n",
      "[85/301][4/8] Loss_D: 1.9615 Loss_G: 7.9499\n",
      "[85/301][5/8] Loss_D: 3.1813 Loss_G: 0.5097\n",
      "[85/301][6/8] Loss_D: 1.5260 Loss_G: 5.4678\n",
      "[85/301][7/8] Loss_D: 1.6886 Loss_G: 0.4083\n",
      "[86/301][0/8] Loss_D: 1.8446 Loss_G: 4.6324\n",
      "[86/301][1/8] Loss_D: 0.5670 Loss_G: 3.4534\n",
      "[86/301][2/8] Loss_D: 0.6543 Loss_G: 1.1831\n",
      "[86/301][3/8] Loss_D: 1.3621 Loss_G: 4.2953\n",
      "[86/301][4/8] Loss_D: 1.0474 Loss_G: 1.6966\n",
      "[86/301][5/8] Loss_D: 0.8859 Loss_G: 3.5637\n",
      "[86/301][6/8] Loss_D: 0.7406 Loss_G: 2.2441\n",
      "[86/301][7/8] Loss_D: 0.7740 Loss_G: 2.1155\n",
      "[87/301][0/8] Loss_D: 0.8301 Loss_G: 3.3920\n",
      "[87/301][1/8] Loss_D: 0.7576 Loss_G: 1.5861\n",
      "[87/301][2/8] Loss_D: 0.7323 Loss_G: 3.1204\n",
      "[87/301][3/8] Loss_D: 0.5675 Loss_G: 2.1812\n",
      "[87/301][4/8] Loss_D: 0.5650 Loss_G: 2.7245\n",
      "[87/301][5/8] Loss_D: 0.5564 Loss_G: 1.7236\n",
      "[87/301][6/8] Loss_D: 0.6687 Loss_G: 3.2113\n",
      "[87/301][7/8] Loss_D: 1.0387 Loss_G: 0.8503\n",
      "[88/301][0/8] Loss_D: 1.2139 Loss_G: 4.4943\n",
      "[88/301][1/8] Loss_D: 0.8423 Loss_G: 2.1259\n",
      "[88/301][2/8] Loss_D: 0.5863 Loss_G: 1.8840\n",
      "[88/301][3/8] Loss_D: 0.4537 Loss_G: 3.2139\n",
      "[88/301][4/8] Loss_D: 0.6069 Loss_G: 2.0028\n",
      "[88/301][5/8] Loss_D: 0.5255 Loss_G: 2.0045\n",
      "[88/301][6/8] Loss_D: 0.6196 Loss_G: 2.7881\n",
      "[88/301][7/8] Loss_D: 0.4243 Loss_G: 2.9116\n",
      "[89/301][0/8] Loss_D: 0.5569 Loss_G: 1.6998\n",
      "[89/301][1/8] Loss_D: 0.7986 Loss_G: 3.7636\n",
      "[89/301][2/8] Loss_D: 1.1161 Loss_G: 0.7185\n",
      "[89/301][3/8] Loss_D: 1.2167 Loss_G: 4.7206\n",
      "[89/301][4/8] Loss_D: 0.9824 Loss_G: 1.7828\n",
      "[89/301][5/8] Loss_D: 0.4401 Loss_G: 2.5906\n",
      "[89/301][6/8] Loss_D: 0.4208 Loss_G: 3.3192\n",
      "[89/301][7/8] Loss_D: 0.6010 Loss_G: 1.4688\n",
      "[90/301][0/8] Loss_D: 0.6649 Loss_G: 3.5268\n",
      "[90/301][1/8] Loss_D: 0.5337 Loss_G: 2.2918\n",
      "[90/301][2/8] Loss_D: 0.5449 Loss_G: 2.2356\n",
      "[90/301][3/8] Loss_D: 0.6359 Loss_G: 2.4240\n",
      "[90/301][4/8] Loss_D: 0.7114 Loss_G: 1.5427\n",
      "[90/301][5/8] Loss_D: 0.8207 Loss_G: 3.7628\n",
      "[90/301][6/8] Loss_D: 0.9488 Loss_G: 0.9431\n",
      "[90/301][7/8] Loss_D: 1.1742 Loss_G: 4.7699\n",
      "[91/301][0/8] Loss_D: 0.8279 Loss_G: 1.5212\n",
      "[91/301][1/8] Loss_D: 0.6686 Loss_G: 3.3229\n",
      "[91/301][2/8] Loss_D: 0.5200 Loss_G: 2.3443\n",
      "[91/301][3/8] Loss_D: 0.5764 Loss_G: 2.3532\n",
      "[91/301][4/8] Loss_D: 0.5709 Loss_G: 2.7733\n",
      "[91/301][5/8] Loss_D: 0.5573 Loss_G: 1.8372\n",
      "[91/301][6/8] Loss_D: 0.5847 Loss_G: 3.7929\n",
      "[91/301][7/8] Loss_D: 0.9280 Loss_G: 1.0321\n",
      "[92/301][0/8] Loss_D: 1.3550 Loss_G: 5.2792\n",
      "[92/301][1/8] Loss_D: 1.5658 Loss_G: 0.9633\n",
      "[92/301][2/8] Loss_D: 1.0438 Loss_G: 4.6388\n",
      "[92/301][3/8] Loss_D: 0.9558 Loss_G: 1.1808\n",
      "[92/301][4/8] Loss_D: 0.9446 Loss_G: 4.0283\n",
      "[92/301][5/8] Loss_D: 0.4685 Loss_G: 2.8041\n",
      "[92/301][6/8] Loss_D: 0.5724 Loss_G: 1.1791\n",
      "[92/301][7/8] Loss_D: 0.8698 Loss_G: 5.3517\n",
      "[93/301][0/8] Loss_D: 1.2556 Loss_G: 0.9234\n",
      "[93/301][1/8] Loss_D: 0.9749 Loss_G: 4.5045\n",
      "[93/301][2/8] Loss_D: 0.9879 Loss_G: 1.1335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93/301][3/8] Loss_D: 0.9086 Loss_G: 4.3234\n",
      "[93/301][4/8] Loss_D: 0.7527 Loss_G: 1.8251\n",
      "[93/301][5/8] Loss_D: 0.6307 Loss_G: 3.1629\n",
      "[93/301][6/8] Loss_D: 0.5757 Loss_G: 2.1402\n",
      "[93/301][7/8] Loss_D: 0.5944 Loss_G: 2.2783\n",
      "[94/301][0/8] Loss_D: 0.6727 Loss_G: 3.9897\n",
      "[94/301][1/8] Loss_D: 0.9804 Loss_G: 1.0134\n",
      "[94/301][2/8] Loss_D: 0.9423 Loss_G: 3.8493\n",
      "[94/301][3/8] Loss_D: 0.6090 Loss_G: 2.1980\n",
      "[94/301][4/8] Loss_D: 0.5396 Loss_G: 2.1275\n",
      "[94/301][5/8] Loss_D: 0.5753 Loss_G: 3.0476\n",
      "[94/301][6/8] Loss_D: 0.6369 Loss_G: 1.4253\n",
      "[94/301][7/8] Loss_D: 0.4281 Loss_G: 4.0566\n",
      "[95/301][0/8] Loss_D: 0.4801 Loss_G: 3.0760\n",
      "[95/301][1/8] Loss_D: 0.7048 Loss_G: 1.0747\n",
      "[95/301][2/8] Loss_D: 0.8068 Loss_G: 4.7144\n",
      "[95/301][3/8] Loss_D: 1.2493 Loss_G: 0.8213\n",
      "[95/301][4/8] Loss_D: 1.0234 Loss_G: 4.8345\n",
      "[95/301][5/8] Loss_D: 1.1709 Loss_G: 0.6423\n",
      "[95/301][6/8] Loss_D: 1.1713 Loss_G: 5.2010\n",
      "[95/301][7/8] Loss_D: 1.1447 Loss_G: 0.8456\n",
      "[96/301][0/8] Loss_D: 1.0413 Loss_G: 4.4505\n",
      "[96/301][1/8] Loss_D: 0.8397 Loss_G: 1.4945\n",
      "[96/301][2/8] Loss_D: 0.7212 Loss_G: 3.3480\n",
      "[96/301][3/8] Loss_D: 0.6154 Loss_G: 2.0865\n",
      "[96/301][4/8] Loss_D: 0.6979 Loss_G: 2.9062\n",
      "[96/301][5/8] Loss_D: 0.7969 Loss_G: 1.0509\n",
      "[96/301][6/8] Loss_D: 0.8765 Loss_G: 3.2290\n",
      "[96/301][7/8] Loss_D: 0.8493 Loss_G: 1.2104\n",
      "[97/301][0/8] Loss_D: 0.9945 Loss_G: 4.5619\n",
      "[97/301][1/8] Loss_D: 1.0902 Loss_G: 1.0926\n",
      "[97/301][2/8] Loss_D: 1.0843 Loss_G: 4.4086\n",
      "[97/301][3/8] Loss_D: 1.0701 Loss_G: 1.0604\n",
      "[97/301][4/8] Loss_D: 0.8358 Loss_G: 4.1663\n",
      "[97/301][5/8] Loss_D: 0.7701 Loss_G: 1.3268\n",
      "[97/301][6/8] Loss_D: 0.7357 Loss_G: 3.3560\n",
      "[97/301][7/8] Loss_D: 0.7543 Loss_G: 1.7082\n",
      "[98/301][0/8] Loss_D: 0.5722 Loss_G: 3.3793\n",
      "[98/301][1/8] Loss_D: 0.4678 Loss_G: 2.4229\n",
      "[98/301][2/8] Loss_D: 0.6475 Loss_G: 1.3341\n",
      "[98/301][3/8] Loss_D: 0.9017 Loss_G: 4.8284\n",
      "[98/301][4/8] Loss_D: 1.5761 Loss_G: 0.6279\n",
      "[98/301][5/8] Loss_D: 1.3568 Loss_G: 4.9925\n",
      "[98/301][6/8] Loss_D: 1.2975 Loss_G: 0.7580\n",
      "[98/301][7/8] Loss_D: 0.9631 Loss_G: 4.9229\n",
      "[99/301][0/8] Loss_D: 0.5123 Loss_G: 2.4085\n",
      "[99/301][1/8] Loss_D: 0.5635 Loss_G: 2.4090\n",
      "[99/301][2/8] Loss_D: 0.5487 Loss_G: 2.3235\n",
      "[99/301][3/8] Loss_D: 0.6220 Loss_G: 2.0559\n",
      "[99/301][4/8] Loss_D: 0.5816 Loss_G: 3.2243\n",
      "[99/301][5/8] Loss_D: 0.9428 Loss_G: 0.6516\n",
      "[99/301][6/8] Loss_D: 1.2445 Loss_G: 4.3481\n",
      "[99/301][7/8] Loss_D: 0.6909 Loss_G: 2.7922\n",
      "[100/301][0/8] Loss_D: 0.7031 Loss_G: 0.9640\n",
      "[100/301][1/8] Loss_D: 0.9509 Loss_G: 4.2175\n",
      "[100/301][2/8] Loss_D: 0.9526 Loss_G: 1.1993\n",
      "[100/301][3/8] Loss_D: 0.7482 Loss_G: 3.2480\n",
      "[100/301][4/8] Loss_D: 0.4663 Loss_G: 2.7364\n",
      "[100/301][5/8] Loss_D: 0.6199 Loss_G: 1.1481\n",
      "[100/301][6/8] Loss_D: 0.6468 Loss_G: 3.4376\n",
      "[100/301][7/8] Loss_D: 0.9102 Loss_G: 0.8977\n",
      "[101/301][0/8] Loss_D: 0.9114 Loss_G: 4.0650\n",
      "[101/301][1/8] Loss_D: 0.9334 Loss_G: 1.4375\n",
      "[101/301][2/8] Loss_D: 0.7321 Loss_G: 3.6221\n",
      "[101/301][3/8] Loss_D: 0.7401 Loss_G: 1.6466\n",
      "[101/301][4/8] Loss_D: 0.7384 Loss_G: 2.6039\n",
      "[101/301][5/8] Loss_D: 0.4718 Loss_G: 2.7506\n",
      "[101/301][6/8] Loss_D: 0.6262 Loss_G: 1.4524\n",
      "[101/301][7/8] Loss_D: 1.1355 Loss_G: 4.2539\n",
      "[102/301][0/8] Loss_D: 0.6578 Loss_G: 1.8499\n",
      "[102/301][1/8] Loss_D: 0.7438 Loss_G: 2.8156\n",
      "[102/301][2/8] Loss_D: 0.7537 Loss_G: 1.3725\n",
      "[102/301][3/8] Loss_D: 0.7086 Loss_G: 3.8339\n",
      "[102/301][4/8] Loss_D: 0.7016 Loss_G: 1.4871\n",
      "[102/301][5/8] Loss_D: 0.5487 Loss_G: 2.9850\n",
      "[102/301][6/8] Loss_D: 0.6544 Loss_G: 1.5577\n",
      "[102/301][7/8] Loss_D: 0.5738 Loss_G: 3.6086\n",
      "[103/301][0/8] Loss_D: 0.4504 Loss_G: 2.3817\n",
      "[103/301][1/8] Loss_D: 0.5213 Loss_G: 1.6827\n",
      "[103/301][2/8] Loss_D: 0.5688 Loss_G: 2.9423\n",
      "[103/301][3/8] Loss_D: 0.6387 Loss_G: 1.3343\n",
      "[103/301][4/8] Loss_D: 0.6831 Loss_G: 3.4675\n",
      "[103/301][5/8] Loss_D: 0.5844 Loss_G: 1.6242\n",
      "[103/301][6/8] Loss_D: 0.7371 Loss_G: 3.4251\n",
      "[103/301][7/8] Loss_D: 0.3597 Loss_G: 3.0298\n",
      "[104/301][0/8] Loss_D: 0.6915 Loss_G: 0.7420\n",
      "[104/301][1/8] Loss_D: 1.0202 Loss_G: 5.1496\n",
      "[104/301][2/8] Loss_D: 1.4036 Loss_G: 0.6454\n",
      "[104/301][3/8] Loss_D: 1.1806 Loss_G: 6.0549\n",
      "[104/301][4/8] Loss_D: 2.0006 Loss_G: 0.2248\n",
      "[104/301][5/8] Loss_D: 2.0280 Loss_G: 8.1029\n",
      "[104/301][6/8] Loss_D: 3.8968 Loss_G: 0.3186\n",
      "[104/301][7/8] Loss_D: 2.5883 Loss_G: 6.9890\n",
      "[105/301][0/8] Loss_D: 2.0528 Loss_G: 0.5164\n",
      "[105/301][1/8] Loss_D: 1.3384 Loss_G: 3.2838\n",
      "[105/301][2/8] Loss_D: 0.5645 Loss_G: 4.4897\n",
      "[105/301][3/8] Loss_D: 1.3142 Loss_G: 0.6094\n",
      "[105/301][4/8] Loss_D: 1.7264 Loss_G: 3.9814\n",
      "[105/301][5/8] Loss_D: 0.9894 Loss_G: 2.0121\n",
      "[105/301][6/8] Loss_D: 0.6137 Loss_G: 2.6250\n",
      "[105/301][7/8] Loss_D: 0.5651 Loss_G: 2.4813\n",
      "[106/301][0/8] Loss_D: 0.6750 Loss_G: 2.6786\n",
      "[106/301][1/8] Loss_D: 0.9022 Loss_G: 1.1162\n",
      "[106/301][2/8] Loss_D: 1.0678 Loss_G: 4.2857\n",
      "[106/301][3/8] Loss_D: 1.0478 Loss_G: 1.3660\n",
      "[106/301][4/8] Loss_D: 0.6624 Loss_G: 2.7608\n",
      "[106/301][5/8] Loss_D: 0.6175 Loss_G: 2.5950\n",
      "[106/301][6/8] Loss_D: 0.5684 Loss_G: 1.8579\n",
      "[106/301][7/8] Loss_D: 0.8779 Loss_G: 2.9142\n",
      "[107/301][0/8] Loss_D: 0.6341 Loss_G: 1.8779\n",
      "[107/301][1/8] Loss_D: 0.5923 Loss_G: 2.4628\n",
      "[107/301][2/8] Loss_D: 0.4983 Loss_G: 2.5890\n",
      "[107/301][3/8] Loss_D: 0.4871 Loss_G: 2.0355\n",
      "[107/301][4/8] Loss_D: 0.5348 Loss_G: 2.9245\n",
      "[107/301][5/8] Loss_D: 0.5281 Loss_G: 2.0319\n",
      "[107/301][6/8] Loss_D: 0.6858 Loss_G: 1.4127\n",
      "[107/301][7/8] Loss_D: 0.7994 Loss_G: 4.5876\n",
      "[108/301][0/8] Loss_D: 1.0195 Loss_G: 1.0007\n",
      "[108/301][1/8] Loss_D: 0.8377 Loss_G: 3.3619\n",
      "[108/301][2/8] Loss_D: 0.6370 Loss_G: 1.7404\n",
      "[108/301][3/8] Loss_D: 0.4742 Loss_G: 2.7698\n",
      "[108/301][4/8] Loss_D: 0.3514 Loss_G: 3.0591\n",
      "[108/301][5/8] Loss_D: 0.5630 Loss_G: 1.2653\n",
      "[108/301][6/8] Loss_D: 0.7993 Loss_G: 3.4324\n",
      "[108/301][7/8] Loss_D: 0.9063 Loss_G: 1.1160\n",
      "[109/301][0/8] Loss_D: 0.9071 Loss_G: 4.1333\n",
      "[109/301][1/8] Loss_D: 0.7651 Loss_G: 1.4530\n",
      "[109/301][2/8] Loss_D: 0.6355 Loss_G: 3.2787\n",
      "[109/301][3/8] Loss_D: 0.6025 Loss_G: 1.5225\n",
      "[109/301][4/8] Loss_D: 0.6055 Loss_G: 3.2827\n",
      "[109/301][5/8] Loss_D: 0.6572 Loss_G: 1.4992\n",
      "[109/301][6/8] Loss_D: 0.6397 Loss_G: 3.0711\n",
      "[109/301][7/8] Loss_D: 0.8822 Loss_G: 1.2816\n",
      "[110/301][0/8] Loss_D: 1.4446 Loss_G: 5.0371\n",
      "[110/301][1/8] Loss_D: 1.3186 Loss_G: 1.3234\n",
      "[110/301][2/8] Loss_D: 0.7329 Loss_G: 3.1781\n",
      "[110/301][3/8] Loss_D: 0.5459 Loss_G: 2.1643\n",
      "[110/301][4/8] Loss_D: 0.5279 Loss_G: 2.6202\n",
      "[110/301][5/8] Loss_D: 0.6818 Loss_G: 1.0816\n",
      "[110/301][6/8] Loss_D: 0.8582 Loss_G: 3.7841\n",
      "[110/301][7/8] Loss_D: 0.5248 Loss_G: 2.3544\n",
      "[111/301][0/8] Loss_D: 0.4747 Loss_G: 2.0227\n",
      "[111/301][1/8] Loss_D: 0.4610 Loss_G: 2.9928\n",
      "[111/301][2/8] Loss_D: 0.5585 Loss_G: 1.7164\n",
      "[111/301][3/8] Loss_D: 0.5596 Loss_G: 2.7358\n",
      "[111/301][4/8] Loss_D: 0.5250 Loss_G: 1.7929\n",
      "[111/301][5/8] Loss_D: 0.5872 Loss_G: 2.6724\n",
      "[111/301][6/8] Loss_D: 0.6803 Loss_G: 1.1517\n",
      "[111/301][7/8] Loss_D: 0.7727 Loss_G: 4.8363\n",
      "[112/301][0/8] Loss_D: 0.7520 Loss_G: 1.3587\n",
      "[112/301][1/8] Loss_D: 0.5301 Loss_G: 3.1107\n",
      "[112/301][2/8] Loss_D: 0.3976 Loss_G: 2.3811\n",
      "[112/301][3/8] Loss_D: 0.5447 Loss_G: 1.8455\n",
      "[112/301][4/8] Loss_D: 0.5319 Loss_G: 2.2044\n",
      "[112/301][5/8] Loss_D: 0.5595 Loss_G: 1.8949\n",
      "[112/301][6/8] Loss_D: 0.5031 Loss_G: 3.1952\n",
      "[112/301][7/8] Loss_D: 1.1291 Loss_G: 0.4213\n",
      "[113/301][0/8] Loss_D: 2.2862 Loss_G: 5.6887\n",
      "[113/301][1/8] Loss_D: 2.0414 Loss_G: 1.0675\n",
      "[113/301][2/8] Loss_D: 0.8850 Loss_G: 6.0144\n",
      "[113/301][3/8] Loss_D: 2.0217 Loss_G: 0.1647\n",
      "[113/301][4/8] Loss_D: 2.4778 Loss_G: 5.7531\n",
      "[113/301][5/8] Loss_D: 1.7143 Loss_G: 0.5332\n",
      "[113/301][6/8] Loss_D: 1.4789 Loss_G: 4.4459\n",
      "[113/301][7/8] Loss_D: 1.2658 Loss_G: 1.5726\n",
      "[114/301][0/8] Loss_D: 0.8368 Loss_G: 3.2363\n",
      "[114/301][1/8] Loss_D: 0.6282 Loss_G: 2.1309\n",
      "[114/301][2/8] Loss_D: 0.6344 Loss_G: 2.0563\n",
      "[114/301][3/8] Loss_D: 0.6928 Loss_G: 2.4946\n",
      "[114/301][4/8] Loss_D: 0.4583 Loss_G: 2.8756\n",
      "[114/301][5/8] Loss_D: 0.6814 Loss_G: 1.1731\n",
      "[114/301][6/8] Loss_D: 0.9579 Loss_G: 3.5541\n",
      "[114/301][7/8] Loss_D: 0.7377 Loss_G: 1.7447\n",
      "[115/301][0/8] Loss_D: 0.9117 Loss_G: 3.6200\n",
      "[115/301][1/8] Loss_D: 0.6203 Loss_G: 2.1842\n",
      "[115/301][2/8] Loss_D: 0.4134 Loss_G: 1.8727\n",
      "[115/301][3/8] Loss_D: 0.5936 Loss_G: 3.0752\n",
      "[115/301][4/8] Loss_D: 0.4343 Loss_G: 2.6640\n",
      "[115/301][5/8] Loss_D: 0.5491 Loss_G: 1.5131\n",
      "[115/301][6/8] Loss_D: 0.6744 Loss_G: 2.9106\n",
      "[115/301][7/8] Loss_D: 0.9429 Loss_G: 1.1267\n",
      "[116/301][0/8] Loss_D: 0.8467 Loss_G: 3.6408\n",
      "[116/301][1/8] Loss_D: 0.7452 Loss_G: 1.8074\n",
      "[116/301][2/8] Loss_D: 0.4651 Loss_G: 2.7078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116/301][3/8] Loss_D: 0.4397 Loss_G: 2.8458\n",
      "[116/301][4/8] Loss_D: 0.4001 Loss_G: 2.2953\n",
      "[116/301][5/8] Loss_D: 0.5242 Loss_G: 1.9276\n",
      "[116/301][6/8] Loss_D: 0.6119 Loss_G: 2.0673\n",
      "[116/301][7/8] Loss_D: 0.3761 Loss_G: 3.7338\n",
      "[117/301][0/8] Loss_D: 0.7219 Loss_G: 0.9790\n",
      "[117/301][1/8] Loss_D: 0.7922 Loss_G: 4.4356\n",
      "[117/301][2/8] Loss_D: 0.7988 Loss_G: 1.8992\n",
      "[117/301][3/8] Loss_D: 0.5062 Loss_G: 2.0516\n",
      "[117/301][4/8] Loss_D: 0.5074 Loss_G: 3.3381\n",
      "[117/301][5/8] Loss_D: 0.8065 Loss_G: 0.8039\n",
      "[117/301][6/8] Loss_D: 0.9953 Loss_G: 4.5504\n",
      "[117/301][7/8] Loss_D: 1.2171 Loss_G: 0.7573\n",
      "[118/301][0/8] Loss_D: 1.1255 Loss_G: 4.7928\n",
      "[118/301][1/8] Loss_D: 0.9472 Loss_G: 1.0755\n",
      "[118/301][2/8] Loss_D: 1.1402 Loss_G: 4.2124\n",
      "[118/301][3/8] Loss_D: 0.6408 Loss_G: 2.4011\n",
      "[118/301][4/8] Loss_D: 0.4400 Loss_G: 1.8494\n",
      "[118/301][5/8] Loss_D: 0.5103 Loss_G: 3.1683\n",
      "[118/301][6/8] Loss_D: 0.5969 Loss_G: 1.5971\n",
      "[118/301][7/8] Loss_D: 0.7042 Loss_G: 2.9351\n",
      "[119/301][0/8] Loss_D: 0.5936 Loss_G: 2.5649\n",
      "[119/301][1/8] Loss_D: 0.3793 Loss_G: 2.8549\n",
      "[119/301][2/8] Loss_D: 0.5638 Loss_G: 1.5227\n",
      "[119/301][3/8] Loss_D: 0.5379 Loss_G: 2.6968\n",
      "[119/301][4/8] Loss_D: 0.4680 Loss_G: 2.4919\n",
      "[119/301][5/8] Loss_D: 0.5082 Loss_G: 2.1227\n",
      "[119/301][6/8] Loss_D: 0.5574 Loss_G: 2.0384\n",
      "[119/301][7/8] Loss_D: 0.4894 Loss_G: 3.9542\n",
      "[120/301][0/8] Loss_D: 0.6930 Loss_G: 1.1960\n",
      "[120/301][1/8] Loss_D: 0.7787 Loss_G: 3.9015\n",
      "[120/301][2/8] Loss_D: 0.5174 Loss_G: 2.1071\n",
      "[120/301][3/8] Loss_D: 0.4194 Loss_G: 2.1272\n",
      "[120/301][4/8] Loss_D: 0.5330 Loss_G: 2.8061\n",
      "[120/301][5/8] Loss_D: 0.6454 Loss_G: 1.5931\n",
      "[120/301][6/8] Loss_D: 0.5322 Loss_G: 3.5832\n",
      "[120/301][7/8] Loss_D: 0.9088 Loss_G: 0.8773\n",
      "[121/301][0/8] Loss_D: 0.9759 Loss_G: 5.1649\n",
      "[121/301][1/8] Loss_D: 1.6537 Loss_G: 0.6499\n",
      "[121/301][2/8] Loss_D: 1.0267 Loss_G: 7.1109\n",
      "[121/301][3/8] Loss_D: 2.5406 Loss_G: 0.0859\n",
      "[121/301][4/8] Loss_D: 2.8321 Loss_G: 6.9032\n",
      "[121/301][5/8] Loss_D: 1.4016 Loss_G: 1.5150\n",
      "[121/301][6/8] Loss_D: 0.6534 Loss_G: 3.2022\n",
      "[121/301][7/8] Loss_D: 0.2637 Loss_G: 4.4644\n",
      "[122/301][0/8] Loss_D: 0.9206 Loss_G: 0.6998\n",
      "[122/301][1/8] Loss_D: 1.4254 Loss_G: 4.3838\n",
      "[122/301][2/8] Loss_D: 0.6662 Loss_G: 2.6672\n",
      "[122/301][3/8] Loss_D: 0.5296 Loss_G: 1.8905\n",
      "[122/301][4/8] Loss_D: 0.7079 Loss_G: 2.3105\n",
      "[122/301][5/8] Loss_D: 0.4792 Loss_G: 3.2744\n",
      "[122/301][6/8] Loss_D: 0.8650 Loss_G: 0.9287\n",
      "[122/301][7/8] Loss_D: 0.9182 Loss_G: 4.2611\n",
      "[123/301][0/8] Loss_D: 0.6594 Loss_G: 1.9736\n",
      "[123/301][1/8] Loss_D: 0.5578 Loss_G: 2.3328\n",
      "[123/301][2/8] Loss_D: 0.5118 Loss_G: 2.3742\n",
      "[123/301][3/8] Loss_D: 0.3891 Loss_G: 3.4144\n",
      "[123/301][4/8] Loss_D: 0.8161 Loss_G: 0.8402\n",
      "[123/301][5/8] Loss_D: 1.1999 Loss_G: 4.4654\n",
      "[123/301][6/8] Loss_D: 0.7814 Loss_G: 2.3351\n",
      "[123/301][7/8] Loss_D: 0.4862 Loss_G: 1.3230\n",
      "[124/301][0/8] Loss_D: 0.8532 Loss_G: 4.2435\n",
      "[124/301][1/8] Loss_D: 0.7431 Loss_G: 1.7858\n",
      "[124/301][2/8] Loss_D: 0.5180 Loss_G: 2.9836\n",
      "[124/301][3/8] Loss_D: 0.5332 Loss_G: 2.0209\n",
      "[124/301][4/8] Loss_D: 0.4724 Loss_G: 3.0374\n",
      "[124/301][5/8] Loss_D: 0.5869 Loss_G: 1.5792\n",
      "[124/301][6/8] Loss_D: 0.6135 Loss_G: 3.0310\n",
      "[124/301][7/8] Loss_D: 0.2179 Loss_G: 4.1079\n",
      "[125/301][0/8] Loss_D: 1.0581 Loss_G: 0.3956\n",
      "[125/301][1/8] Loss_D: 1.5552 Loss_G: 4.8026\n",
      "[125/301][2/8] Loss_D: 1.1177 Loss_G: 1.4686\n",
      "[125/301][3/8] Loss_D: 0.5218 Loss_G: 3.2020\n",
      "[125/301][4/8] Loss_D: 0.3632 Loss_G: 2.8150\n",
      "[125/301][5/8] Loss_D: 0.4113 Loss_G: 2.0247\n",
      "[125/301][6/8] Loss_D: 0.4680 Loss_G: 2.6335\n",
      "[125/301][7/8] Loss_D: 0.5568 Loss_G: 2.1190\n",
      "[126/301][0/8] Loss_D: 0.5485 Loss_G: 3.1297\n",
      "[126/301][1/8] Loss_D: 0.5927 Loss_G: 1.6077\n",
      "[126/301][2/8] Loss_D: 0.4575 Loss_G: 3.1797\n",
      "[126/301][3/8] Loss_D: 0.6148 Loss_G: 1.6147\n",
      "[126/301][4/8] Loss_D: 0.5706 Loss_G: 2.2580\n",
      "[126/301][5/8] Loss_D: 0.4467 Loss_G: 2.7518\n",
      "[126/301][6/8] Loss_D: 0.5360 Loss_G: 1.5126\n",
      "[126/301][7/8] Loss_D: 0.5359 Loss_G: 3.3867\n",
      "[127/301][0/8] Loss_D: 0.4909 Loss_G: 2.1919\n",
      "[127/301][1/8] Loss_D: 0.3772 Loss_G: 2.9311\n",
      "[127/301][2/8] Loss_D: 0.4605 Loss_G: 1.7647\n",
      "[127/301][3/8] Loss_D: 0.4189 Loss_G: 3.3229\n",
      "[127/301][4/8] Loss_D: 0.6786 Loss_G: 0.9831\n",
      "[127/301][5/8] Loss_D: 0.8989 Loss_G: 4.4128\n",
      "[127/301][6/8] Loss_D: 1.1723 Loss_G: 0.8893\n",
      "[127/301][7/8] Loss_D: 0.8012 Loss_G: 5.9267\n",
      "[128/301][0/8] Loss_D: 1.2511 Loss_G: 0.3236\n",
      "[128/301][1/8] Loss_D: 1.6481 Loss_G: 6.8363\n",
      "[128/301][2/8] Loss_D: 1.3949 Loss_G: 1.3882\n",
      "[128/301][3/8] Loss_D: 0.6832 Loss_G: 3.2435\n",
      "[128/301][4/8] Loss_D: 0.5026 Loss_G: 2.3827\n",
      "[128/301][5/8] Loss_D: 0.4854 Loss_G: 2.0491\n",
      "[128/301][6/8] Loss_D: 0.6379 Loss_G: 2.8588\n",
      "[128/301][7/8] Loss_D: 0.4339 Loss_G: 3.0910\n",
      "[129/301][0/8] Loss_D: 0.5352 Loss_G: 1.5693\n",
      "[129/301][1/8] Loss_D: 0.6970 Loss_G: 3.4575\n",
      "[129/301][2/8] Loss_D: 0.5483 Loss_G: 1.7502\n",
      "[129/301][3/8] Loss_D: 0.6102 Loss_G: 2.4723\n",
      "[129/301][4/8] Loss_D: 0.5664 Loss_G: 2.3575\n",
      "[129/301][5/8] Loss_D: 0.4689 Loss_G: 2.1310\n",
      "[129/301][6/8] Loss_D: 0.3563 Loss_G: 3.5081\n",
      "[129/301][7/8] Loss_D: 1.5351 Loss_G: 0.1617\n",
      "[130/301][0/8] Loss_D: 2.5819 Loss_G: 6.0293\n",
      "[130/301][1/8] Loss_D: 1.8938 Loss_G: 1.1974\n",
      "[130/301][2/8] Loss_D: 1.2282 Loss_G: 6.1988\n",
      "[130/301][3/8] Loss_D: 2.4072 Loss_G: 0.2614\n",
      "[130/301][4/8] Loss_D: 1.9544 Loss_G: 6.7567\n",
      "[130/301][5/8] Loss_D: 2.5336 Loss_G: 0.2785\n",
      "[130/301][6/8] Loss_D: 2.0032 Loss_G: 3.7980\n",
      "[130/301][7/8] Loss_D: 0.6774 Loss_G: 3.5931\n",
      "[131/301][0/8] Loss_D: 0.8474 Loss_G: 0.7580\n",
      "[131/301][1/8] Loss_D: 1.2207 Loss_G: 4.1572\n",
      "[131/301][2/8] Loss_D: 1.0286 Loss_G: 1.6240\n",
      "[131/301][3/8] Loss_D: 0.7574 Loss_G: 2.8967\n",
      "[131/301][4/8] Loss_D: 0.5787 Loss_G: 2.5282\n",
      "[131/301][5/8] Loss_D: 0.7052 Loss_G: 1.9560\n",
      "[131/301][6/8] Loss_D: 0.6432 Loss_G: 2.8615\n",
      "[131/301][7/8] Loss_D: 0.7738 Loss_G: 1.2216\n",
      "[132/301][0/8] Loss_D: 1.1889 Loss_G: 4.5421\n",
      "[132/301][1/8] Loss_D: 1.3495 Loss_G: 1.3312\n",
      "[132/301][2/8] Loss_D: 0.6192 Loss_G: 2.5193\n",
      "[132/301][3/8] Loss_D: 0.4357 Loss_G: 2.8139\n",
      "[132/301][4/8] Loss_D: 0.4019 Loss_G: 2.6946\n",
      "[132/301][5/8] Loss_D: 0.3669 Loss_G: 2.3763\n",
      "[132/301][6/8] Loss_D: 0.5231 Loss_G: 1.6650\n",
      "[132/301][7/8] Loss_D: 0.5151 Loss_G: 2.7105\n",
      "[133/301][0/8] Loss_D: 0.4749 Loss_G: 2.4372\n",
      "[133/301][1/8] Loss_D: 0.4850 Loss_G: 2.3940\n",
      "[133/301][2/8] Loss_D: 0.3447 Loss_G: 3.1163\n",
      "[133/301][3/8] Loss_D: 0.5747 Loss_G: 1.3747\n",
      "[133/301][4/8] Loss_D: 0.5519 Loss_G: 2.4578\n",
      "[133/301][5/8] Loss_D: 0.4144 Loss_G: 2.8306\n",
      "[133/301][6/8] Loss_D: 0.4382 Loss_G: 2.0071\n",
      "[133/301][7/8] Loss_D: 0.7401 Loss_G: 1.3778\n",
      "[134/301][0/8] Loss_D: 0.7345 Loss_G: 4.2622\n",
      "[134/301][1/8] Loss_D: 0.8653 Loss_G: 1.4050\n",
      "[134/301][2/8] Loss_D: 0.5719 Loss_G: 3.0560\n",
      "[134/301][3/8] Loss_D: 0.2483 Loss_G: 3.5604\n",
      "[134/301][4/8] Loss_D: 0.8149 Loss_G: 0.6518\n",
      "[134/301][5/8] Loss_D: 1.2099 Loss_G: 4.3779\n",
      "[134/301][6/8] Loss_D: 1.1740 Loss_G: 1.2652\n",
      "[134/301][7/8] Loss_D: 0.5732 Loss_G: 3.3590\n",
      "[135/301][0/8] Loss_D: 0.3606 Loss_G: 2.9371\n",
      "[135/301][1/8] Loss_D: 0.3995 Loss_G: 2.3921\n",
      "[135/301][2/8] Loss_D: 0.4466 Loss_G: 1.7389\n",
      "[135/301][3/8] Loss_D: 0.4485 Loss_G: 3.6573\n",
      "[135/301][4/8] Loss_D: 0.8994 Loss_G: 0.8107\n",
      "[135/301][5/8] Loss_D: 0.9861 Loss_G: 3.9041\n",
      "[135/301][6/8] Loss_D: 0.6184 Loss_G: 2.0729\n",
      "[135/301][7/8] Loss_D: 0.4956 Loss_G: 1.7040\n",
      "[136/301][0/8] Loss_D: 0.5453 Loss_G: 3.7461\n",
      "[136/301][1/8] Loss_D: 0.6303 Loss_G: 1.6695\n",
      "[136/301][2/8] Loss_D: 0.5606 Loss_G: 2.3981\n",
      "[136/301][3/8] Loss_D: 0.2859 Loss_G: 3.5783\n",
      "[136/301][4/8] Loss_D: 0.7096 Loss_G: 0.9813\n",
      "[136/301][5/8] Loss_D: 0.7778 Loss_G: 3.4610\n",
      "[136/301][6/8] Loss_D: 0.7694 Loss_G: 1.2478\n",
      "[136/301][7/8] Loss_D: 0.4772 Loss_G: 3.0394\n",
      "[137/301][0/8] Loss_D: 0.5036 Loss_G: 3.3680\n",
      "[137/301][1/8] Loss_D: 0.4903 Loss_G: 1.8057\n",
      "[137/301][2/8] Loss_D: 0.3930 Loss_G: 3.2904\n",
      "[137/301][3/8] Loss_D: 0.7316 Loss_G: 0.9854\n",
      "[137/301][4/8] Loss_D: 0.8305 Loss_G: 4.1290\n",
      "[137/301][5/8] Loss_D: 0.9278 Loss_G: 1.2611\n",
      "[137/301][6/8] Loss_D: 0.6968 Loss_G: 3.4164\n",
      "[137/301][7/8] Loss_D: 0.4543 Loss_G: 2.7719\n",
      "[138/301][0/8] Loss_D: 0.3878 Loss_G: 2.7553\n",
      "[138/301][1/8] Loss_D: 0.2843 Loss_G: 3.1190\n",
      "[138/301][2/8] Loss_D: 0.4403 Loss_G: 1.6561\n",
      "[138/301][3/8] Loss_D: 0.6071 Loss_G: 2.6754\n",
      "[138/301][4/8] Loss_D: 0.5580 Loss_G: 1.9548\n",
      "[138/301][5/8] Loss_D: 0.5612 Loss_G: 2.2610\n",
      "[138/301][6/8] Loss_D: 0.5124 Loss_G: 2.2885\n",
      "[138/301][7/8] Loss_D: 0.6876 Loss_G: 1.2579\n",
      "[139/301][0/8] Loss_D: 1.0888 Loss_G: 5.2360\n",
      "[139/301][1/8] Loss_D: 1.3954 Loss_G: 1.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139/301][2/8] Loss_D: 0.6231 Loss_G: 4.3983\n",
      "[139/301][3/8] Loss_D: 1.2554 Loss_G: 0.1119\n",
      "[139/301][4/8] Loss_D: 2.6036 Loss_G: 6.1475\n",
      "[139/301][5/8] Loss_D: 1.9786 Loss_G: 0.7441\n",
      "[139/301][6/8] Loss_D: 0.9600 Loss_G: 4.0500\n",
      "[139/301][7/8] Loss_D: 0.6858 Loss_G: 1.9879\n",
      "[140/301][0/8] Loss_D: 0.3710 Loss_G: 2.9052\n",
      "[140/301][1/8] Loss_D: 0.6090 Loss_G: 1.5315\n",
      "[140/301][2/8] Loss_D: 0.6974 Loss_G: 4.0765\n",
      "[140/301][3/8] Loss_D: 1.3453 Loss_G: 0.7020\n",
      "[140/301][4/8] Loss_D: 1.1460 Loss_G: 3.9908\n",
      "[140/301][5/8] Loss_D: 0.9037 Loss_G: 1.4763\n",
      "[140/301][6/8] Loss_D: 0.6760 Loss_G: 2.7806\n",
      "[140/301][7/8] Loss_D: 0.6472 Loss_G: 1.9474\n",
      "[141/301][0/8] Loss_D: 0.4545 Loss_G: 2.9278\n",
      "[141/301][1/8] Loss_D: 0.3483 Loss_G: 3.1348\n",
      "[141/301][2/8] Loss_D: 0.4133 Loss_G: 1.9907\n",
      "[141/301][3/8] Loss_D: 0.4323 Loss_G: 2.4064\n",
      "[141/301][4/8] Loss_D: 0.4876 Loss_G: 2.0270\n",
      "[141/301][5/8] Loss_D: 0.6269 Loss_G: 2.3681\n",
      "[141/301][6/8] Loss_D: 0.4802 Loss_G: 2.0647\n",
      "[141/301][7/8] Loss_D: 0.4801 Loss_G: 2.2918\n",
      "[142/301][0/8] Loss_D: 0.3928 Loss_G: 2.9045\n",
      "[142/301][1/8] Loss_D: 0.4342 Loss_G: 2.0191\n",
      "[142/301][2/8] Loss_D: 0.3388 Loss_G: 3.1584\n",
      "[142/301][3/8] Loss_D: 0.3118 Loss_G: 2.5158\n",
      "[142/301][4/8] Loss_D: 0.3735 Loss_G: 1.9180\n",
      "[142/301][5/8] Loss_D: 0.4987 Loss_G: 2.7487\n",
      "[142/301][6/8] Loss_D: 0.5990 Loss_G: 1.3171\n",
      "[142/301][7/8] Loss_D: 0.5594 Loss_G: 2.8524\n",
      "[143/301][0/8] Loss_D: 0.3760 Loss_G: 2.9463\n",
      "[143/301][1/8] Loss_D: 0.2950 Loss_G: 2.7618\n",
      "[143/301][2/8] Loss_D: 0.4613 Loss_G: 1.3927\n",
      "[143/301][3/8] Loss_D: 0.6265 Loss_G: 3.7173\n",
      "[143/301][4/8] Loss_D: 0.4269 Loss_G: 2.3322\n",
      "[143/301][5/8] Loss_D: 0.4181 Loss_G: 1.7473\n",
      "[143/301][6/8] Loss_D: 0.4465 Loss_G: 4.1156\n",
      "[143/301][7/8] Loss_D: 0.7037 Loss_G: 1.2322\n",
      "[144/301][0/8] Loss_D: 0.6238 Loss_G: 3.6764\n",
      "[144/301][1/8] Loss_D: 0.3343 Loss_G: 2.6592\n",
      "[144/301][2/8] Loss_D: 0.4692 Loss_G: 1.5721\n",
      "[144/301][3/8] Loss_D: 0.5365 Loss_G: 3.5728\n",
      "[144/301][4/8] Loss_D: 0.5708 Loss_G: 1.4468\n",
      "[144/301][5/8] Loss_D: 0.5423 Loss_G: 3.9572\n",
      "[144/301][6/8] Loss_D: 0.7264 Loss_G: 1.1583\n",
      "[144/301][7/8] Loss_D: 0.5449 Loss_G: 4.7210\n",
      "[145/301][0/8] Loss_D: 0.2861 Loss_G: 3.7441\n",
      "[145/301][1/8] Loss_D: 0.6106 Loss_G: 1.1863\n",
      "[145/301][2/8] Loss_D: 0.7643 Loss_G: 4.4506\n",
      "[145/301][3/8] Loss_D: 0.8620 Loss_G: 1.1167\n",
      "[145/301][4/8] Loss_D: 0.7174 Loss_G: 4.0331\n",
      "[145/301][5/8] Loss_D: 0.2311 Loss_G: 4.1595\n",
      "[145/301][6/8] Loss_D: 0.8226 Loss_G: 0.6311\n",
      "[145/301][7/8] Loss_D: 1.4797 Loss_G: 8.1499\n",
      "[146/301][0/8] Loss_D: 2.0966 Loss_G: 0.6415\n",
      "[146/301][1/8] Loss_D: 0.9081 Loss_G: 5.3865\n",
      "[146/301][2/8] Loss_D: 1.0721 Loss_G: 0.3526\n",
      "[146/301][3/8] Loss_D: 2.1400 Loss_G: 5.2899\n",
      "[146/301][4/8] Loss_D: 1.1502 Loss_G: 1.8892\n",
      "[146/301][5/8] Loss_D: 0.5257 Loss_G: 3.0849\n",
      "[146/301][6/8] Loss_D: 0.2842 Loss_G: 3.6131\n",
      "[146/301][7/8] Loss_D: 1.1535 Loss_G: 0.5054\n",
      "[147/301][0/8] Loss_D: 2.3463 Loss_G: 5.1193\n",
      "[147/301][1/8] Loss_D: 1.4141 Loss_G: 1.6079\n",
      "[147/301][2/8] Loss_D: 0.5949 Loss_G: 2.8714\n",
      "[147/301][3/8] Loss_D: 0.4542 Loss_G: 2.7100\n",
      "[147/301][4/8] Loss_D: 0.3056 Loss_G: 3.3248\n",
      "[147/301][5/8] Loss_D: 0.3759 Loss_G: 2.3596\n",
      "[147/301][6/8] Loss_D: 0.4524 Loss_G: 1.7167\n",
      "[147/301][7/8] Loss_D: 0.6820 Loss_G: 4.8157\n",
      "[148/301][0/8] Loss_D: 0.9629 Loss_G: 1.1786\n",
      "[148/301][1/8] Loss_D: 0.5929 Loss_G: 2.7312\n",
      "[148/301][2/8] Loss_D: 0.5390 Loss_G: 2.8074\n",
      "[148/301][3/8] Loss_D: 0.5519 Loss_G: 1.6840\n",
      "[148/301][4/8] Loss_D: 0.4260 Loss_G: 2.7018\n",
      "[148/301][5/8] Loss_D: 0.2577 Loss_G: 3.3405\n",
      "[148/301][6/8] Loss_D: 0.2900 Loss_G: 2.7396\n",
      "[148/301][7/8] Loss_D: 0.3518 Loss_G: 2.1005\n",
      "[149/301][0/8] Loss_D: 0.5899 Loss_G: 3.2915\n",
      "[149/301][1/8] Loss_D: 0.2423 Loss_G: 3.5410\n",
      "[149/301][2/8] Loss_D: 0.2419 Loss_G: 2.7865\n",
      "[149/301][3/8] Loss_D: 0.4682 Loss_G: 1.3608\n",
      "[149/301][4/8] Loss_D: 0.6169 Loss_G: 3.1642\n",
      "[149/301][5/8] Loss_D: 0.6050 Loss_G: 1.5807\n",
      "[149/301][6/8] Loss_D: 0.6694 Loss_G: 3.0553\n",
      "[149/301][7/8] Loss_D: 0.4818 Loss_G: 2.2051\n",
      "[150/301][0/8] Loss_D: 0.2365 Loss_G: 3.3895\n",
      "[150/301][1/8] Loss_D: 0.5862 Loss_G: 1.2075\n",
      "[150/301][2/8] Loss_D: 0.6912 Loss_G: 3.9745\n",
      "[150/301][3/8] Loss_D: 0.5974 Loss_G: 1.9148\n",
      "[150/301][4/8] Loss_D: 0.4504 Loss_G: 2.3729\n",
      "[150/301][5/8] Loss_D: 0.4246 Loss_G: 2.3306\n",
      "[150/301][6/8] Loss_D: 0.3184 Loss_G: 3.4131\n",
      "[150/301][7/8] Loss_D: 1.2660 Loss_G: 0.3866\n",
      "[151/301][0/8] Loss_D: 1.7018 Loss_G: 6.2742\n",
      "[151/301][1/8] Loss_D: 2.2168 Loss_G: 0.8948\n",
      "[151/301][2/8] Loss_D: 1.1777 Loss_G: 6.7059\n",
      "[151/301][3/8] Loss_D: 1.7525 Loss_G: 0.3967\n",
      "[151/301][4/8] Loss_D: 1.7016 Loss_G: 4.6617\n",
      "[151/301][5/8] Loss_D: 0.8227 Loss_G: 1.9885\n",
      "[151/301][6/8] Loss_D: 0.6213 Loss_G: 3.5148\n",
      "[151/301][7/8] Loss_D: 1.3938 Loss_G: 0.5131\n",
      "[152/301][0/8] Loss_D: 2.1351 Loss_G: 4.7365\n",
      "[152/301][1/8] Loss_D: 0.8898 Loss_G: 2.3320\n",
      "[152/301][2/8] Loss_D: 0.4302 Loss_G: 2.6963\n",
      "[152/301][3/8] Loss_D: 0.7491 Loss_G: 1.7487\n",
      "[152/301][4/8] Loss_D: 0.4650 Loss_G: 3.2921\n",
      "[152/301][5/8] Loss_D: 0.7323 Loss_G: 1.4841\n",
      "[152/301][6/8] Loss_D: 0.7831 Loss_G: 3.6211\n",
      "[152/301][7/8] Loss_D: 0.6648 Loss_G: 2.0271\n",
      "[153/301][0/8] Loss_D: 0.4832 Loss_G: 2.6310\n",
      "[153/301][1/8] Loss_D: 0.4646 Loss_G: 3.0477\n",
      "[153/301][2/8] Loss_D: 0.4577 Loss_G: 2.2733\n",
      "[153/301][3/8] Loss_D: 0.4315 Loss_G: 3.2459\n",
      "[153/301][4/8] Loss_D: 0.4173 Loss_G: 2.4863\n",
      "[153/301][5/8] Loss_D: 0.4815 Loss_G: 2.2087\n",
      "[153/301][6/8] Loss_D: 0.3673 Loss_G: 2.4979\n",
      "[153/301][7/8] Loss_D: 0.3765 Loss_G: 2.8847\n",
      "[154/301][0/8] Loss_D: 0.3904 Loss_G: 3.0586\n",
      "[154/301][1/8] Loss_D: 0.4094 Loss_G: 2.2019\n",
      "[154/301][2/8] Loss_D: 0.4227 Loss_G: 1.7732\n",
      "[154/301][3/8] Loss_D: 0.6457 Loss_G: 3.6917\n",
      "[154/301][4/8] Loss_D: 0.5812 Loss_G: 1.9301\n",
      "[154/301][5/8] Loss_D: 0.4704 Loss_G: 2.6890\n",
      "[154/301][6/8] Loss_D: 0.3923 Loss_G: 2.4387\n",
      "[154/301][7/8] Loss_D: 0.4080 Loss_G: 1.9311\n",
      "[155/301][0/8] Loss_D: 0.4704 Loss_G: 3.5869\n",
      "[155/301][1/8] Loss_D: 0.4979 Loss_G: 1.9635\n",
      "[155/301][2/8] Loss_D: 0.3481 Loss_G: 3.1252\n",
      "[155/301][3/8] Loss_D: 0.3965 Loss_G: 2.3444\n",
      "[155/301][4/8] Loss_D: 0.4079 Loss_G: 2.4015\n",
      "[155/301][5/8] Loss_D: 0.4178 Loss_G: 2.5391\n",
      "[155/301][6/8] Loss_D: 0.3402 Loss_G: 2.7668\n",
      "[155/301][7/8] Loss_D: 0.2156 Loss_G: 2.9923\n",
      "[156/301][0/8] Loss_D: 0.3847 Loss_G: 2.5573\n",
      "[156/301][1/8] Loss_D: 0.3110 Loss_G: 3.1443\n",
      "[156/301][2/8] Loss_D: 0.3042 Loss_G: 2.6681\n",
      "[156/301][3/8] Loss_D: 0.4658 Loss_G: 1.6682\n",
      "[156/301][4/8] Loss_D: 0.4514 Loss_G: 3.0262\n",
      "[156/301][5/8] Loss_D: 0.3992 Loss_G: 2.3347\n",
      "[156/301][6/8] Loss_D: 0.3267 Loss_G: 3.0415\n",
      "[156/301][7/8] Loss_D: 0.2333 Loss_G: 2.9635\n",
      "[157/301][0/8] Loss_D: 0.2989 Loss_G: 3.0173\n",
      "[157/301][1/8] Loss_D: 0.3945 Loss_G: 1.8900\n",
      "[157/301][2/8] Loss_D: 0.4568 Loss_G: 3.7656\n",
      "[157/301][3/8] Loss_D: 0.5331 Loss_G: 1.4224\n",
      "[157/301][4/8] Loss_D: 0.5450 Loss_G: 4.3912\n",
      "[157/301][5/8] Loss_D: 0.3433 Loss_G: 2.8512\n",
      "[157/301][6/8] Loss_D: 0.2915 Loss_G: 1.7824\n",
      "[157/301][7/8] Loss_D: 0.4122 Loss_G: 4.9067\n",
      "[158/301][0/8] Loss_D: 0.3259 Loss_G: 2.6538\n",
      "[158/301][1/8] Loss_D: 0.3514 Loss_G: 2.2438\n",
      "[158/301][2/8] Loss_D: 0.4189 Loss_G: 3.1558\n",
      "[158/301][3/8] Loss_D: 0.2250 Loss_G: 3.7688\n",
      "[158/301][4/8] Loss_D: 0.4905 Loss_G: 1.2389\n",
      "[158/301][5/8] Loss_D: 0.6129 Loss_G: 4.5820\n",
      "[158/301][6/8] Loss_D: 0.4963 Loss_G: 1.9442\n",
      "[158/301][7/8] Loss_D: 0.4101 Loss_G: 2.2695\n",
      "[159/301][0/8] Loss_D: 0.4569 Loss_G: 5.0619\n",
      "[159/301][1/8] Loss_D: 0.4335 Loss_G: 2.5235\n",
      "[159/301][2/8] Loss_D: 0.3321 Loss_G: 2.8166\n",
      "[159/301][3/8] Loss_D: 0.3523 Loss_G: 2.4507\n",
      "[159/301][4/8] Loss_D: 0.3343 Loss_G: 3.5275\n",
      "[159/301][5/8] Loss_D: 0.6842 Loss_G: 0.6337\n",
      "[159/301][6/8] Loss_D: 1.0130 Loss_G: 6.4349\n",
      "[159/301][7/8] Loss_D: 1.8398 Loss_G: 0.7122\n",
      "[160/301][0/8] Loss_D: 1.1461 Loss_G: 10.0310\n",
      "[160/301][1/8] Loss_D: 4.3778 Loss_G: 0.0813\n",
      "[160/301][2/8] Loss_D: 3.4015 Loss_G: 7.6296\n",
      "[160/301][3/8] Loss_D: 2.9293 Loss_G: 0.3775\n",
      "[160/301][4/8] Loss_D: 2.2156 Loss_G: 6.1831\n",
      "[160/301][5/8] Loss_D: 1.2909 Loss_G: 1.8359\n",
      "[160/301][6/8] Loss_D: 0.8126 Loss_G: 3.3626\n",
      "[160/301][7/8] Loss_D: 1.0263 Loss_G: 1.0033\n",
      "[161/301][0/8] Loss_D: 1.6126 Loss_G: 5.7961\n",
      "[161/301][1/8] Loss_D: 1.6076 Loss_G: 1.8015\n",
      "[161/301][2/8] Loss_D: 0.9199 Loss_G: 3.4323\n",
      "[161/301][3/8] Loss_D: 0.2859 Loss_G: 4.5073\n",
      "[161/301][4/8] Loss_D: 1.0378 Loss_G: 0.8662\n",
      "[161/301][5/8] Loss_D: 1.0638 Loss_G: 3.4281\n",
      "[161/301][6/8] Loss_D: 0.4596 Loss_G: 3.0302\n",
      "[161/301][7/8] Loss_D: 0.5353 Loss_G: 2.1455\n",
      "[162/301][0/8] Loss_D: 0.7472 Loss_G: 3.1804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162/301][1/8] Loss_D: 0.4235 Loss_G: 3.2198\n",
      "[162/301][2/8] Loss_D: 0.2895 Loss_G: 3.1737\n",
      "[162/301][3/8] Loss_D: 0.7760 Loss_G: 1.0223\n",
      "[162/301][4/8] Loss_D: 1.2703 Loss_G: 4.6385\n",
      "[162/301][5/8] Loss_D: 1.1801 Loss_G: 2.1076\n",
      "[162/301][6/8] Loss_D: 0.5599 Loss_G: 2.3774\n",
      "[162/301][7/8] Loss_D: 0.4208 Loss_G: 3.3764\n",
      "[163/301][0/8] Loss_D: 0.4896 Loss_G: 3.7195\n",
      "[163/301][1/8] Loss_D: 0.5818 Loss_G: 2.2128\n",
      "[163/301][2/8] Loss_D: 0.4634 Loss_G: 2.4941\n",
      "[163/301][3/8] Loss_D: 0.4971 Loss_G: 2.5944\n",
      "[163/301][4/8] Loss_D: 0.5574 Loss_G: 2.5198\n",
      "[163/301][5/8] Loss_D: 0.3871 Loss_G: 2.7250\n",
      "[163/301][6/8] Loss_D: 0.4556 Loss_G: 2.8827\n",
      "[163/301][7/8] Loss_D: 0.3273 Loss_G: 2.7653\n",
      "[164/301][0/8] Loss_D: 0.4223 Loss_G: 3.3910\n",
      "[164/301][1/8] Loss_D: 0.5101 Loss_G: 1.8954\n",
      "[164/301][2/8] Loss_D: 0.5041 Loss_G: 3.0539\n",
      "[164/301][3/8] Loss_D: 0.4450 Loss_G: 2.3458\n",
      "[164/301][4/8] Loss_D: 0.3173 Loss_G: 2.8004\n",
      "[164/301][5/8] Loss_D: 0.4613 Loss_G: 2.3772\n",
      "[164/301][6/8] Loss_D: 0.4835 Loss_G: 2.0650\n",
      "[164/301][7/8] Loss_D: 0.4255 Loss_G: 3.8038\n",
      "[165/301][0/8] Loss_D: 0.4814 Loss_G: 1.6042\n",
      "[165/301][1/8] Loss_D: 0.5737 Loss_G: 3.6780\n",
      "[165/301][2/8] Loss_D: 0.5470 Loss_G: 1.8367\n",
      "[165/301][3/8] Loss_D: 0.4497 Loss_G: 3.0921\n",
      "[165/301][4/8] Loss_D: 0.3446 Loss_G: 2.8301\n",
      "[165/301][5/8] Loss_D: 0.3571 Loss_G: 2.3500\n",
      "[165/301][6/8] Loss_D: 0.3939 Loss_G: 2.7941\n",
      "[165/301][7/8] Loss_D: 0.4219 Loss_G: 2.6037\n",
      "[166/301][0/8] Loss_D: 0.3721 Loss_G: 2.6683\n",
      "[166/301][1/8] Loss_D: 0.2969 Loss_G: 2.6962\n",
      "[166/301][2/8] Loss_D: 0.3800 Loss_G: 2.3189\n",
      "[166/301][3/8] Loss_D: 0.3609 Loss_G: 3.2116\n",
      "[166/301][4/8] Loss_D: 0.5489 Loss_G: 1.6932\n",
      "[166/301][5/8] Loss_D: 0.5250 Loss_G: 3.0661\n",
      "[166/301][6/8] Loss_D: 0.2782 Loss_G: 2.9663\n",
      "[166/301][7/8] Loss_D: 0.3182 Loss_G: 2.3685\n",
      "[167/301][0/8] Loss_D: 0.3511 Loss_G: 3.4707\n",
      "[167/301][1/8] Loss_D: 0.4133 Loss_G: 2.1279\n",
      "[167/301][2/8] Loss_D: 0.2780 Loss_G: 3.0192\n",
      "[167/301][3/8] Loss_D: 0.3758 Loss_G: 2.8508\n",
      "[167/301][4/8] Loss_D: 0.2728 Loss_G: 2.6088\n",
      "[167/301][5/8] Loss_D: 0.4597 Loss_G: 1.8980\n",
      "[167/301][6/8] Loss_D: 0.4936 Loss_G: 3.9392\n",
      "[167/301][7/8] Loss_D: 0.7236 Loss_G: 1.1787\n",
      "[168/301][0/8] Loss_D: 0.8792 Loss_G: 5.4297\n",
      "[168/301][1/8] Loss_D: 1.0466 Loss_G: 1.1539\n",
      "[168/301][2/8] Loss_D: 0.9585 Loss_G: 6.5453\n",
      "[168/301][3/8] Loss_D: 1.4667 Loss_G: 0.8052\n",
      "[168/301][4/8] Loss_D: 1.3624 Loss_G: 6.3898\n",
      "[168/301][5/8] Loss_D: 1.2055 Loss_G: 0.5665\n",
      "[168/301][6/8] Loss_D: 1.3828 Loss_G: 6.0138\n",
      "[168/301][7/8] Loss_D: 0.8446 Loss_G: 2.4567\n",
      "[169/301][0/8] Loss_D: 0.5291 Loss_G: 2.6621\n",
      "[169/301][1/8] Loss_D: 0.4600 Loss_G: 3.4950\n",
      "[169/301][2/8] Loss_D: 0.6116 Loss_G: 1.6431\n",
      "[169/301][3/8] Loss_D: 0.5883 Loss_G: 4.2221\n",
      "[169/301][4/8] Loss_D: 0.6830 Loss_G: 1.5911\n",
      "[169/301][5/8] Loss_D: 0.5366 Loss_G: 3.8887\n",
      "[169/301][6/8] Loss_D: 0.4263 Loss_G: 2.5264\n",
      "[169/301][7/8] Loss_D: 0.4606 Loss_G: 1.5819\n",
      "[170/301][0/8] Loss_D: 0.8929 Loss_G: 5.2522\n",
      "[170/301][1/8] Loss_D: 0.5986 Loss_G: 2.7508\n",
      "[170/301][2/8] Loss_D: 0.3416 Loss_G: 2.3783\n",
      "[170/301][3/8] Loss_D: 0.3392 Loss_G: 3.3765\n",
      "[170/301][4/8] Loss_D: 0.3886 Loss_G: 2.2649\n",
      "[170/301][5/8] Loss_D: 0.3752 Loss_G: 3.6041\n",
      "[170/301][6/8] Loss_D: 0.5505 Loss_G: 1.7388\n",
      "[170/301][7/8] Loss_D: 0.5468 Loss_G: 4.1347\n",
      "[171/301][0/8] Loss_D: 0.2575 Loss_G: 3.4949\n",
      "[171/301][1/8] Loss_D: 0.4304 Loss_G: 1.9313\n",
      "[171/301][2/8] Loss_D: 0.4165 Loss_G: 3.1753\n",
      "[171/301][3/8] Loss_D: 0.3221 Loss_G: 3.0196\n",
      "[171/301][4/8] Loss_D: 0.3057 Loss_G: 2.7066\n",
      "[171/301][5/8] Loss_D: 0.5266 Loss_G: 1.5537\n",
      "[171/301][6/8] Loss_D: 0.4293 Loss_G: 4.1563\n",
      "[171/301][7/8] Loss_D: 0.7766 Loss_G: 1.1072\n",
      "[172/301][0/8] Loss_D: 0.6234 Loss_G: 4.1066\n",
      "[172/301][1/8] Loss_D: 0.4256 Loss_G: 2.6830\n",
      "[172/301][2/8] Loss_D: 0.3211 Loss_G: 2.2668\n",
      "[172/301][3/8] Loss_D: 0.3340 Loss_G: 3.4447\n",
      "[172/301][4/8] Loss_D: 0.5237 Loss_G: 1.4453\n",
      "[172/301][5/8] Loss_D: 0.7779 Loss_G: 4.8269\n",
      "[172/301][6/8] Loss_D: 0.4580 Loss_G: 3.0812\n",
      "[172/301][7/8] Loss_D: 0.3164 Loss_G: 1.7501\n",
      "[173/301][0/8] Loss_D: 0.4908 Loss_G: 3.4129\n",
      "[173/301][1/8] Loss_D: 0.2818 Loss_G: 3.2122\n",
      "[173/301][2/8] Loss_D: 0.3079 Loss_G: 2.6003\n",
      "[173/301][3/8] Loss_D: 0.2631 Loss_G: 2.7264\n",
      "[173/301][4/8] Loss_D: 0.4246 Loss_G: 1.7716\n",
      "[173/301][5/8] Loss_D: 0.3947 Loss_G: 4.0112\n",
      "[173/301][6/8] Loss_D: 0.5525 Loss_G: 1.5588\n",
      "[173/301][7/8] Loss_D: 0.5185 Loss_G: 4.0983\n",
      "[174/301][0/8] Loss_D: 0.2157 Loss_G: 3.5357\n",
      "[174/301][1/8] Loss_D: 0.2653 Loss_G: 3.1402\n",
      "[174/301][2/8] Loss_D: 0.3944 Loss_G: 1.6512\n",
      "[174/301][3/8] Loss_D: 0.5559 Loss_G: 3.5945\n",
      "[174/301][4/8] Loss_D: 0.4467 Loss_G: 1.9770\n",
      "[174/301][5/8] Loss_D: 0.4290 Loss_G: 3.6992\n",
      "[174/301][6/8] Loss_D: 0.5639 Loss_G: 1.5793\n",
      "[174/301][7/8] Loss_D: 0.3947 Loss_G: 4.1052\n",
      "[175/301][0/8] Loss_D: 0.3751 Loss_G: 2.5316\n",
      "[175/301][1/8] Loss_D: 0.2618 Loss_G: 3.3140\n",
      "[175/301][2/8] Loss_D: 0.3675 Loss_G: 2.3535\n",
      "[175/301][3/8] Loss_D: 0.3517 Loss_G: 2.7836\n",
      "[175/301][4/8] Loss_D: 0.1712 Loss_G: 3.4777\n",
      "[175/301][5/8] Loss_D: 0.3019 Loss_G: 2.2019\n",
      "[175/301][6/8] Loss_D: 0.3531 Loss_G: 2.8333\n",
      "[175/301][7/8] Loss_D: 0.5190 Loss_G: 2.5738\n",
      "[176/301][0/8] Loss_D: 0.2647 Loss_G: 3.8858\n",
      "[176/301][1/8] Loss_D: 0.3975 Loss_G: 1.8011\n",
      "[176/301][2/8] Loss_D: 0.4086 Loss_G: 3.5772\n",
      "[176/301][3/8] Loss_D: 0.3770 Loss_G: 2.3149\n",
      "[176/301][4/8] Loss_D: 0.3754 Loss_G: 2.9376\n",
      "[176/301][5/8] Loss_D: 0.2471 Loss_G: 3.0273\n",
      "[176/301][6/8] Loss_D: 0.2271 Loss_G: 3.0193\n",
      "[176/301][7/8] Loss_D: 0.2922 Loss_G: 2.1402\n",
      "[177/301][0/8] Loss_D: 0.5460 Loss_G: 4.9473\n",
      "[177/301][1/8] Loss_D: 0.8135 Loss_G: 1.1323\n",
      "[177/301][2/8] Loss_D: 0.6228 Loss_G: 4.6937\n",
      "[177/301][3/8] Loss_D: 0.8356 Loss_G: 0.6652\n",
      "[177/301][4/8] Loss_D: 1.3537 Loss_G: 8.4434\n",
      "[177/301][5/8] Loss_D: 3.2806 Loss_G: 0.6194\n",
      "[177/301][6/8] Loss_D: 1.5923 Loss_G: 9.1969\n",
      "[177/301][7/8] Loss_D: 4.5013 Loss_G: 0.2104\n",
      "[178/301][0/8] Loss_D: 2.7868 Loss_G: 7.0837\n",
      "[178/301][1/8] Loss_D: 2.1339 Loss_G: 0.6966\n",
      "[178/301][2/8] Loss_D: 1.8846 Loss_G: 4.2084\n",
      "[178/301][3/8] Loss_D: 0.9031 Loss_G: 2.3949\n",
      "[178/301][4/8] Loss_D: 0.7119 Loss_G: 2.3348\n",
      "[178/301][5/8] Loss_D: 0.6156 Loss_G: 3.4581\n",
      "[178/301][6/8] Loss_D: 0.8468 Loss_G: 1.6647\n",
      "[178/301][7/8] Loss_D: 0.9875 Loss_G: 3.5197\n",
      "[179/301][0/8] Loss_D: 0.6359 Loss_G: 3.9205\n",
      "[179/301][1/8] Loss_D: 0.4713 Loss_G: 2.6272\n",
      "[179/301][2/8] Loss_D: 0.5606 Loss_G: 1.5226\n",
      "[179/301][3/8] Loss_D: 0.7078 Loss_G: 3.9645\n",
      "[179/301][4/8] Loss_D: 0.5311 Loss_G: 2.4615\n",
      "[179/301][5/8] Loss_D: 0.3392 Loss_G: 2.6166\n",
      "[179/301][6/8] Loss_D: 0.3631 Loss_G: 2.9909\n",
      "[179/301][7/8] Loss_D: 0.6388 Loss_G: 2.1240\n",
      "[180/301][0/8] Loss_D: 0.4187 Loss_G: 3.3728\n",
      "[180/301][1/8] Loss_D: 0.4698 Loss_G: 2.2265\n",
      "[180/301][2/8] Loss_D: 0.3011 Loss_G: 3.2294\n",
      "[180/301][3/8] Loss_D: 0.3329 Loss_G: 2.4990\n",
      "[180/301][4/8] Loss_D: 0.5373 Loss_G: 2.2714\n",
      "[180/301][5/8] Loss_D: 0.4991 Loss_G: 2.2053\n",
      "[180/301][6/8] Loss_D: 0.5350 Loss_G: 3.2167\n",
      "[180/301][7/8] Loss_D: 0.4817 Loss_G: 1.9469\n",
      "[181/301][0/8] Loss_D: 0.4650 Loss_G: 3.2330\n",
      "[181/301][1/8] Loss_D: 0.2357 Loss_G: 4.0680\n",
      "[181/301][2/8] Loss_D: 0.4285 Loss_G: 2.1029\n",
      "[181/301][3/8] Loss_D: 0.5326 Loss_G: 2.2345\n",
      "[181/301][4/8] Loss_D: 0.3808 Loss_G: 3.7172\n",
      "[181/301][5/8] Loss_D: 0.6360 Loss_G: 1.2355\n",
      "[181/301][6/8] Loss_D: 0.6425 Loss_G: 4.1912\n",
      "[181/301][7/8] Loss_D: 0.4968 Loss_G: 2.7559\n",
      "[182/301][0/8] Loss_D: 0.3299 Loss_G: 2.7900\n",
      "[182/301][1/8] Loss_D: 0.4620 Loss_G: 3.2135\n",
      "[182/301][2/8] Loss_D: 0.3286 Loss_G: 2.8266\n",
      "[182/301][3/8] Loss_D: 0.1761 Loss_G: 3.6201\n",
      "[182/301][4/8] Loss_D: 0.3668 Loss_G: 1.8265\n",
      "[182/301][5/8] Loss_D: 0.5055 Loss_G: 4.5227\n",
      "[182/301][6/8] Loss_D: 0.8076 Loss_G: 1.0773\n",
      "[182/301][7/8] Loss_D: 0.6740 Loss_G: 5.6848\n",
      "[183/301][0/8] Loss_D: 0.5269 Loss_G: 2.5640\n",
      "[183/301][1/8] Loss_D: 0.2316 Loss_G: 3.3147\n",
      "[183/301][2/8] Loss_D: 0.3307 Loss_G: 2.5899\n",
      "[183/301][3/8] Loss_D: 0.3577 Loss_G: 3.4443\n",
      "[183/301][4/8] Loss_D: 0.4312 Loss_G: 2.1701\n",
      "[183/301][5/8] Loss_D: 0.3679 Loss_G: 2.7297\n",
      "[183/301][6/8] Loss_D: 0.3456 Loss_G: 3.4475\n",
      "[183/301][7/8] Loss_D: 0.4802 Loss_G: 1.7423\n",
      "[184/301][0/8] Loss_D: 0.5339 Loss_G: 4.0045\n",
      "[184/301][1/8] Loss_D: 0.1691 Loss_G: 4.5720\n",
      "[184/301][2/8] Loss_D: 0.3277 Loss_G: 2.4725\n",
      "[184/301][3/8] Loss_D: 0.2622 Loss_G: 2.5492\n",
      "[184/301][4/8] Loss_D: 0.3670 Loss_G: 3.2526\n",
      "[184/301][5/8] Loss_D: 0.3769 Loss_G: 2.0755\n",
      "[184/301][6/8] Loss_D: 0.4145 Loss_G: 3.3527\n",
      "[184/301][7/8] Loss_D: 0.5448 Loss_G: 2.1832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185/301][0/8] Loss_D: 0.3675 Loss_G: 3.8679\n",
      "[185/301][1/8] Loss_D: 0.3693 Loss_G: 2.5184\n",
      "[185/301][2/8] Loss_D: 0.2846 Loss_G: 3.8441\n",
      "[185/301][3/8] Loss_D: 0.1463 Loss_G: 4.0107\n",
      "[185/301][4/8] Loss_D: 0.2609 Loss_G: 2.4238\n",
      "[185/301][5/8] Loss_D: 0.4139 Loss_G: 2.0570\n",
      "[185/301][6/8] Loss_D: 0.3849 Loss_G: 3.3928\n",
      "[185/301][7/8] Loss_D: 0.7215 Loss_G: 1.2878\n",
      "[186/301][0/8] Loss_D: 0.6606 Loss_G: 5.5215\n",
      "[186/301][1/8] Loss_D: 0.8402 Loss_G: 1.6344\n",
      "[186/301][2/8] Loss_D: 0.5551 Loss_G: 5.1722\n",
      "[186/301][3/8] Loss_D: 0.9860 Loss_G: 0.3446\n",
      "[186/301][4/8] Loss_D: 1.6725 Loss_G: 8.7362\n",
      "[186/301][5/8] Loss_D: 2.5609 Loss_G: 1.0406\n",
      "[186/301][6/8] Loss_D: 0.9618 Loss_G: 6.8761\n",
      "[186/301][7/8] Loss_D: 2.6022 Loss_G: 0.2035\n",
      "[187/301][0/8] Loss_D: 3.4204 Loss_G: 7.8244\n",
      "[187/301][1/8] Loss_D: 2.7419 Loss_G: 0.4833\n",
      "[187/301][2/8] Loss_D: 2.3308 Loss_G: 5.2719\n",
      "[187/301][3/8] Loss_D: 1.0122 Loss_G: 2.4214\n",
      "[187/301][4/8] Loss_D: 0.7560 Loss_G: 3.6215\n",
      "[187/301][5/8] Loss_D: 0.6934 Loss_G: 2.1955\n",
      "[187/301][6/8] Loss_D: 0.7718 Loss_G: 3.9034\n",
      "[187/301][7/8] Loss_D: 1.2195 Loss_G: 0.9412\n",
      "[188/301][0/8] Loss_D: 1.5093 Loss_G: 5.8425\n",
      "[188/301][1/8] Loss_D: 0.9091 Loss_G: 3.5231\n",
      "[188/301][2/8] Loss_D: 0.4468 Loss_G: 2.2893\n",
      "[188/301][3/8] Loss_D: 0.5682 Loss_G: 3.8930\n",
      "[188/301][4/8] Loss_D: 0.4994 Loss_G: 2.5799\n",
      "[188/301][5/8] Loss_D: 0.5231 Loss_G: 2.2799\n",
      "[188/301][6/8] Loss_D: 0.4545 Loss_G: 3.2754\n",
      "[188/301][7/8] Loss_D: 0.6851 Loss_G: 1.5660\n",
      "[189/301][0/8] Loss_D: 0.9533 Loss_G: 4.3263\n",
      "[189/301][1/8] Loss_D: 0.3039 Loss_G: 4.3686\n",
      "[189/301][2/8] Loss_D: 0.4304 Loss_G: 1.8614\n",
      "[189/301][3/8] Loss_D: 0.4941 Loss_G: 2.3392\n",
      "[189/301][4/8] Loss_D: 0.4853 Loss_G: 3.1232\n",
      "[189/301][5/8] Loss_D: 0.4553 Loss_G: 2.6187\n",
      "[189/301][6/8] Loss_D: 0.2961 Loss_G: 2.9311\n",
      "[189/301][7/8] Loss_D: 0.5835 Loss_G: 1.5113\n",
      "[190/301][0/8] Loss_D: 0.6370 Loss_G: 3.8331\n",
      "[190/301][1/8] Loss_D: 0.2634 Loss_G: 4.0165\n",
      "[190/301][2/8] Loss_D: 0.4652 Loss_G: 1.6936\n",
      "[190/301][3/8] Loss_D: 0.6728 Loss_G: 3.2910\n",
      "[190/301][4/8] Loss_D: 0.3450 Loss_G: 3.1149\n",
      "[190/301][5/8] Loss_D: 0.4606 Loss_G: 1.6613\n",
      "[190/301][6/8] Loss_D: 0.5006 Loss_G: 4.1568\n",
      "[190/301][7/8] Loss_D: 1.0749 Loss_G: 1.1702\n",
      "[191/301][0/8] Loss_D: 1.1982 Loss_G: 4.6433\n",
      "[191/301][1/8] Loss_D: 0.3575 Loss_G: 4.2590\n",
      "[191/301][2/8] Loss_D: 0.3964 Loss_G: 2.2748\n",
      "[191/301][3/8] Loss_D: 0.4613 Loss_G: 2.3423\n",
      "[191/301][4/8] Loss_D: 0.4617 Loss_G: 3.1506\n",
      "[191/301][5/8] Loss_D: 0.3389 Loss_G: 3.0075\n",
      "[191/301][6/8] Loss_D: 0.3718 Loss_G: 2.4042\n",
      "[191/301][7/8] Loss_D: 0.3403 Loss_G: 2.6148\n",
      "[192/301][0/8] Loss_D: 0.4224 Loss_G: 4.2867\n",
      "[192/301][1/8] Loss_D: 0.2339 Loss_G: 3.9254\n",
      "[192/301][2/8] Loss_D: 0.4869 Loss_G: 1.3155\n",
      "[192/301][3/8] Loss_D: 0.5077 Loss_G: 3.8532\n",
      "[192/301][4/8] Loss_D: 0.3747 Loss_G: 2.9560\n",
      "[192/301][5/8] Loss_D: 0.3445 Loss_G: 2.2188\n",
      "[192/301][6/8] Loss_D: 0.4505 Loss_G: 3.1950\n",
      "[192/301][7/8] Loss_D: 0.3877 Loss_G: 4.2284\n",
      "[193/301][0/8] Loss_D: 0.8130 Loss_G: 0.7574\n",
      "[193/301][1/8] Loss_D: 0.4757 Loss_G: 3.3444\n",
      "[193/301][2/8] Loss_D: 0.2187 Loss_G: 4.7208\n",
      "[193/301][3/8] Loss_D: 0.9073 Loss_G: 0.8070\n",
      "[193/301][4/8] Loss_D: 0.8444 Loss_G: 5.4230\n",
      "[193/301][5/8] Loss_D: 0.5892 Loss_G: 2.5553\n",
      "[193/301][6/8] Loss_D: 0.3270 Loss_G: 3.2503\n",
      "[193/301][7/8] Loss_D: 0.3732 Loss_G: 3.4075\n",
      "[194/301][0/8] Loss_D: 0.3162 Loss_G: 3.7816\n",
      "[194/301][1/8] Loss_D: 0.3283 Loss_G: 2.8018\n",
      "[194/301][2/8] Loss_D: 0.4029 Loss_G: 2.6141\n",
      "[194/301][3/8] Loss_D: 0.3983 Loss_G: 3.3143\n",
      "[194/301][4/8] Loss_D: 0.3297 Loss_G: 2.4848\n",
      "[194/301][5/8] Loss_D: 0.3967 Loss_G: 2.2485\n",
      "[194/301][6/8] Loss_D: 0.3793 Loss_G: 3.9795\n",
      "[194/301][7/8] Loss_D: 0.9233 Loss_G: 0.5342\n",
      "[195/301][0/8] Loss_D: 1.4848 Loss_G: 6.7382\n",
      "[195/301][1/8] Loss_D: 1.3455 Loss_G: 1.7470\n",
      "[195/301][2/8] Loss_D: 0.5149 Loss_G: 4.0319\n",
      "[195/301][3/8] Loss_D: 0.2615 Loss_G: 3.8134\n",
      "[195/301][4/8] Loss_D: 0.4818 Loss_G: 1.1301\n",
      "[195/301][5/8] Loss_D: 0.6848 Loss_G: 5.0545\n",
      "[195/301][6/8] Loss_D: 0.8146 Loss_G: 1.0230\n",
      "[195/301][7/8] Loss_D: 0.7295 Loss_G: 6.1424\n",
      "[196/301][0/8] Loss_D: 0.4277 Loss_G: 3.1290\n",
      "[196/301][1/8] Loss_D: 0.3471 Loss_G: 2.4163\n",
      "[196/301][2/8] Loss_D: 0.3248 Loss_G: 3.5766\n",
      "[196/301][3/8] Loss_D: 0.3259 Loss_G: 3.1543\n",
      "[196/301][4/8] Loss_D: 0.3117 Loss_G: 2.2896\n",
      "[196/301][5/8] Loss_D: 0.4482 Loss_G: 4.0344\n",
      "[196/301][6/8] Loss_D: 0.7047 Loss_G: 1.3242\n",
      "[196/301][7/8] Loss_D: 0.4931 Loss_G: 5.3214\n",
      "[197/301][0/8] Loss_D: 0.3112 Loss_G: 3.0896\n",
      "[197/301][1/8] Loss_D: 0.3084 Loss_G: 2.5037\n",
      "[197/301][2/8] Loss_D: 0.2480 Loss_G: 3.3761\n",
      "[197/301][3/8] Loss_D: 0.3645 Loss_G: 2.6291\n",
      "[197/301][4/8] Loss_D: 0.2502 Loss_G: 2.5649\n",
      "[197/301][5/8] Loss_D: 0.3890 Loss_G: 3.6783\n",
      "[197/301][6/8] Loss_D: 0.3651 Loss_G: 2.3424\n",
      "[197/301][7/8] Loss_D: 0.5230 Loss_G: 2.3108\n",
      "[198/301][0/8] Loss_D: 0.3482 Loss_G: 4.0201\n",
      "[198/301][1/8] Loss_D: 0.2118 Loss_G: 3.8322\n",
      "[198/301][2/8] Loss_D: 0.3107 Loss_G: 2.0051\n",
      "[198/301][3/8] Loss_D: 0.3447 Loss_G: 3.0660\n",
      "[198/301][4/8] Loss_D: 0.3282 Loss_G: 2.7420\n",
      "[198/301][5/8] Loss_D: 0.2819 Loss_G: 2.8408\n",
      "[198/301][6/8] Loss_D: 0.3112 Loss_G: 2.3035\n",
      "[198/301][7/8] Loss_D: 0.4854 Loss_G: 5.3896\n",
      "[199/301][0/8] Loss_D: 0.4321 Loss_G: 2.3752\n",
      "[199/301][1/8] Loss_D: 0.3310 Loss_G: 3.1919\n",
      "[199/301][2/8] Loss_D: 0.2035 Loss_G: 3.9140\n",
      "[199/301][3/8] Loss_D: 0.4837 Loss_G: 1.3697\n",
      "[199/301][4/8] Loss_D: 0.4278 Loss_G: 3.6708\n",
      "[199/301][5/8] Loss_D: 0.2799 Loss_G: 3.0436\n",
      "[199/301][6/8] Loss_D: 0.2573 Loss_G: 2.5799\n",
      "[199/301][7/8] Loss_D: 0.3074 Loss_G: 2.8406\n",
      "[200/301][0/8] Loss_D: 0.3358 Loss_G: 4.1830\n",
      "[200/301][1/8] Loss_D: 0.2914 Loss_G: 2.9276\n",
      "[200/301][2/8] Loss_D: 0.2685 Loss_G: 2.4725\n",
      "[200/301][3/8] Loss_D: 0.3016 Loss_G: 3.1786\n",
      "[200/301][4/8] Loss_D: 0.2403 Loss_G: 2.8257\n",
      "[200/301][5/8] Loss_D: 0.2651 Loss_G: 2.0495\n",
      "[200/301][6/8] Loss_D: 0.4413 Loss_G: 3.8735\n",
      "[200/301][7/8] Loss_D: 0.4515 Loss_G: 2.2767\n",
      "[201/301][0/8] Loss_D: 0.4661 Loss_G: 3.7091\n",
      "[201/301][1/8] Loss_D: 0.3270 Loss_G: 3.0268\n",
      "[201/301][2/8] Loss_D: 0.2439 Loss_G: 2.6426\n",
      "[201/301][3/8] Loss_D: 0.2765 Loss_G: 3.0154\n",
      "[201/301][4/8] Loss_D: 0.4191 Loss_G: 3.6958\n",
      "[201/301][5/8] Loss_D: 0.3622 Loss_G: 2.2425\n",
      "[201/301][6/8] Loss_D: 0.3318 Loss_G: 2.7259\n",
      "[201/301][7/8] Loss_D: 0.2349 Loss_G: 3.3303\n",
      "[202/301][0/8] Loss_D: 0.2417 Loss_G: 3.2486\n",
      "[202/301][1/8] Loss_D: 0.1993 Loss_G: 3.0469\n",
      "[202/301][2/8] Loss_D: 0.2644 Loss_G: 2.7489\n",
      "[202/301][3/8] Loss_D: 0.2724 Loss_G: 2.5387\n",
      "[202/301][4/8] Loss_D: 0.3381 Loss_G: 3.7694\n",
      "[202/301][5/8] Loss_D: 0.4137 Loss_G: 1.8198\n",
      "[202/301][6/8] Loss_D: 0.3682 Loss_G: 3.4623\n",
      "[202/301][7/8] Loss_D: 0.1642 Loss_G: 3.9189\n",
      "[203/301][0/8] Loss_D: 0.2373 Loss_G: 2.5887\n",
      "[203/301][1/8] Loss_D: 0.3003 Loss_G: 3.3985\n",
      "[203/301][2/8] Loss_D: 0.1833 Loss_G: 3.1841\n",
      "[203/301][3/8] Loss_D: 0.2579 Loss_G: 3.0643\n",
      "[203/301][4/8] Loss_D: 0.2534 Loss_G: 2.5584\n",
      "[203/301][5/8] Loss_D: 0.2546 Loss_G: 3.0517\n",
      "[203/301][6/8] Loss_D: 0.2414 Loss_G: 2.9168\n",
      "[203/301][7/8] Loss_D: 0.3150 Loss_G: 2.1628\n",
      "[204/301][0/8] Loss_D: 0.4396 Loss_G: 4.6001\n",
      "[204/301][1/8] Loss_D: 0.4831 Loss_G: 1.8799\n",
      "[204/301][2/8] Loss_D: 0.4120 Loss_G: 4.1351\n",
      "[204/301][3/8] Loss_D: 0.3534 Loss_G: 2.0187\n",
      "[204/301][4/8] Loss_D: 0.3163 Loss_G: 3.8543\n",
      "[204/301][5/8] Loss_D: 0.2585 Loss_G: 2.9606\n",
      "[204/301][6/8] Loss_D: 0.2509 Loss_G: 2.2339\n",
      "[204/301][7/8] Loss_D: 0.3508 Loss_G: 4.3305\n",
      "[205/301][0/8] Loss_D: 0.3048 Loss_G: 2.2292\n",
      "[205/301][1/8] Loss_D: 0.2345 Loss_G: 3.2498\n",
      "[205/301][2/8] Loss_D: 0.2929 Loss_G: 2.8656\n",
      "[205/301][3/8] Loss_D: 0.2530 Loss_G: 2.6954\n",
      "[205/301][4/8] Loss_D: 0.2263 Loss_G: 3.0691\n",
      "[205/301][5/8] Loss_D: 0.2339 Loss_G: 2.7473\n",
      "[205/301][6/8] Loss_D: 0.2382 Loss_G: 3.2088\n",
      "[205/301][7/8] Loss_D: 0.1628 Loss_G: 3.3015\n",
      "[206/301][0/8] Loss_D: 0.2142 Loss_G: 2.8873\n",
      "[206/301][1/8] Loss_D: 0.1717 Loss_G: 3.2944\n",
      "[206/301][2/8] Loss_D: 0.2010 Loss_G: 2.9981\n",
      "[206/301][3/8] Loss_D: 0.1816 Loss_G: 3.2005\n",
      "[206/301][4/8] Loss_D: 0.1822 Loss_G: 3.0324\n",
      "[206/301][5/8] Loss_D: 0.2066 Loss_G: 3.3264\n",
      "[206/301][6/8] Loss_D: 0.1956 Loss_G: 2.8142\n",
      "[206/301][7/8] Loss_D: 0.2281 Loss_G: 3.1816\n",
      "[207/301][0/8] Loss_D: 0.2036 Loss_G: 3.1638\n",
      "[207/301][1/8] Loss_D: 0.2089 Loss_G: 3.7352\n",
      "[207/301][2/8] Loss_D: 0.2448 Loss_G: 2.1687\n",
      "[207/301][3/8] Loss_D: 0.2866 Loss_G: 3.8947\n",
      "[207/301][4/8] Loss_D: 0.2864 Loss_G: 2.5648\n",
      "[207/301][5/8] Loss_D: 0.1986 Loss_G: 3.2799\n",
      "[207/301][6/8] Loss_D: 0.2120 Loss_G: 2.9249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207/301][7/8] Loss_D: 0.3563 Loss_G: 1.8134\n",
      "[208/301][0/8] Loss_D: 0.3568 Loss_G: 4.6596\n",
      "[208/301][1/8] Loss_D: 0.2273 Loss_G: 3.2105\n",
      "[208/301][2/8] Loss_D: 0.2190 Loss_G: 2.2730\n",
      "[208/301][3/8] Loss_D: 0.3289 Loss_G: 4.6314\n",
      "[208/301][4/8] Loss_D: 0.4765 Loss_G: 1.2698\n",
      "[208/301][5/8] Loss_D: 0.6490 Loss_G: 6.8496\n",
      "[208/301][6/8] Loss_D: 2.0277 Loss_G: 0.4317\n",
      "[208/301][7/8] Loss_D: 0.8293 Loss_G: 15.7649\n",
      "[209/301][0/8] Loss_D: 7.9441 Loss_G: 0.0048\n",
      "[209/301][1/8] Loss_D: 5.8790 Loss_G: 7.5746\n",
      "[209/301][2/8] Loss_D: 2.1387 Loss_G: 1.1944\n",
      "[209/301][3/8] Loss_D: 1.8366 Loss_G: 6.9153\n",
      "[209/301][4/8] Loss_D: 1.4794 Loss_G: 1.7598\n",
      "[209/301][5/8] Loss_D: 1.8381 Loss_G: 6.2367\n",
      "[209/301][6/8] Loss_D: 0.9427 Loss_G: 2.1716\n",
      "[209/301][7/8] Loss_D: 0.6236 Loss_G: 4.0170\n",
      "[210/301][0/8] Loss_D: 0.8471 Loss_G: 4.4292\n",
      "[210/301][1/8] Loss_D: 1.2009 Loss_G: 0.7611\n",
      "[210/301][2/8] Loss_D: 1.5984 Loss_G: 6.8835\n",
      "[210/301][3/8] Loss_D: 1.2969 Loss_G: 1.8403\n",
      "[210/301][4/8] Loss_D: 0.7324 Loss_G: 3.9979\n",
      "[210/301][5/8] Loss_D: 0.3729 Loss_G: 3.3070\n",
      "[210/301][6/8] Loss_D: 0.4750 Loss_G: 2.4266\n",
      "[210/301][7/8] Loss_D: 0.5737 Loss_G: 3.2836\n",
      "[211/301][0/8] Loss_D: 0.5246 Loss_G: 4.4176\n",
      "[211/301][1/8] Loss_D: 0.6408 Loss_G: 1.6315\n",
      "[211/301][2/8] Loss_D: 0.5770 Loss_G: 3.5258\n",
      "[211/301][3/8] Loss_D: 0.4964 Loss_G: 2.3362\n",
      "[211/301][4/8] Loss_D: 0.3933 Loss_G: 3.0127\n",
      "[211/301][5/8] Loss_D: 0.4198 Loss_G: 2.2139\n",
      "[211/301][6/8] Loss_D: 0.3996 Loss_G: 3.5455\n",
      "[211/301][7/8] Loss_D: 0.3237 Loss_G: 3.1705\n",
      "[212/301][0/8] Loss_D: 0.4415 Loss_G: 1.7392\n",
      "[212/301][1/8] Loss_D: 0.4121 Loss_G: 3.3898\n",
      "[212/301][2/8] Loss_D: 0.3175 Loss_G: 3.0294\n",
      "[212/301][3/8] Loss_D: 0.2559 Loss_G: 2.4346\n",
      "[212/301][4/8] Loss_D: 0.3743 Loss_G: 3.5936\n",
      "[212/301][5/8] Loss_D: 0.3457 Loss_G: 2.5531\n",
      "[212/301][6/8] Loss_D: 0.4211 Loss_G: 2.5512\n",
      "[212/301][7/8] Loss_D: 0.1791 Loss_G: 3.4570\n",
      "[213/301][0/8] Loss_D: 0.3145 Loss_G: 2.4789\n",
      "[213/301][1/8] Loss_D: 0.4399 Loss_G: 3.6957\n",
      "[213/301][2/8] Loss_D: 0.2575 Loss_G: 3.1412\n",
      "[213/301][3/8] Loss_D: 0.3851 Loss_G: 1.3467\n",
      "[213/301][4/8] Loss_D: 0.7050 Loss_G: 5.1731\n",
      "[213/301][5/8] Loss_D: 0.8044 Loss_G: 1.6599\n",
      "[213/301][6/8] Loss_D: 0.4657 Loss_G: 2.9644\n",
      "[213/301][7/8] Loss_D: 0.1845 Loss_G: 4.7561\n",
      "[214/301][0/8] Loss_D: 0.5416 Loss_G: 1.7423\n",
      "[214/301][1/8] Loss_D: 0.3567 Loss_G: 3.3177\n",
      "[214/301][2/8] Loss_D: 0.2515 Loss_G: 3.7660\n",
      "[214/301][3/8] Loss_D: 0.4518 Loss_G: 1.4057\n",
      "[214/301][4/8] Loss_D: 0.6461 Loss_G: 4.4413\n",
      "[214/301][5/8] Loss_D: 0.5136 Loss_G: 2.2094\n",
      "[214/301][6/8] Loss_D: 0.3463 Loss_G: 2.7542\n",
      "[214/301][7/8] Loss_D: 0.4973 Loss_G: 2.8566\n",
      "[215/301][0/8] Loss_D: 0.2196 Loss_G: 3.4052\n",
      "[215/301][1/8] Loss_D: 0.2588 Loss_G: 3.2565\n",
      "[215/301][2/8] Loss_D: 0.2618 Loss_G: 2.4420\n",
      "[215/301][3/8] Loss_D: 0.3208 Loss_G: 3.5197\n",
      "[215/301][4/8] Loss_D: 0.2905 Loss_G: 2.4743\n",
      "[215/301][5/8] Loss_D: 0.3651 Loss_G: 2.6913\n",
      "[215/301][6/8] Loss_D: 0.2962 Loss_G: 2.9757\n",
      "[215/301][7/8] Loss_D: 0.3153 Loss_G: 2.3298\n",
      "[216/301][0/8] Loss_D: 0.3920 Loss_G: 3.7701\n",
      "[216/301][1/8] Loss_D: 0.2857 Loss_G: 2.7831\n",
      "[216/301][2/8] Loss_D: 0.2176 Loss_G: 2.7295\n",
      "[216/301][3/8] Loss_D: 0.2494 Loss_G: 3.4802\n",
      "[216/301][4/8] Loss_D: 0.3334 Loss_G: 2.1700\n",
      "[216/301][5/8] Loss_D: 0.2889 Loss_G: 3.2143\n",
      "[216/301][6/8] Loss_D: 0.3239 Loss_G: 2.5909\n",
      "[216/301][7/8] Loss_D: 0.1209 Loss_G: 3.4898\n",
      "[217/301][0/8] Loss_D: 0.1861 Loss_G: 3.2469\n",
      "[217/301][1/8] Loss_D: 0.2497 Loss_G: 3.0853\n",
      "[217/301][2/8] Loss_D: 0.3205 Loss_G: 1.9022\n",
      "[217/301][3/8] Loss_D: 0.3840 Loss_G: 4.1671\n",
      "[217/301][4/8] Loss_D: 0.4130 Loss_G: 1.8283\n",
      "[217/301][5/8] Loss_D: 0.2770 Loss_G: 3.0862\n",
      "[217/301][6/8] Loss_D: 0.2540 Loss_G: 3.3541\n",
      "[217/301][7/8] Loss_D: 0.2048 Loss_G: 3.4373\n",
      "[218/301][0/8] Loss_D: 0.2267 Loss_G: 2.6155\n",
      "[218/301][1/8] Loss_D: 0.2461 Loss_G: 2.8506\n",
      "[218/301][2/8] Loss_D: 0.2267 Loss_G: 3.3795\n",
      "[218/301][3/8] Loss_D: 0.2394 Loss_G: 2.5508\n",
      "[218/301][4/8] Loss_D: 0.2270 Loss_G: 3.4494\n",
      "[218/301][5/8] Loss_D: 0.2442 Loss_G: 2.3861\n",
      "[218/301][6/8] Loss_D: 0.3257 Loss_G: 4.1957\n",
      "[218/301][7/8] Loss_D: 0.8564 Loss_G: 0.5071\n",
      "[219/301][0/8] Loss_D: 1.5800 Loss_G: 6.6941\n",
      "[219/301][1/8] Loss_D: 1.9298 Loss_G: 0.5815\n",
      "[219/301][2/8] Loss_D: 1.7793 Loss_G: 10.5840\n",
      "[219/301][3/8] Loss_D: 5.4389 Loss_G: 0.5873\n",
      "[219/301][4/8] Loss_D: 2.1804 Loss_G: 6.6099\n",
      "[219/301][5/8] Loss_D: 2.2622 Loss_G: 0.2518\n",
      "[219/301][6/8] Loss_D: 2.4297 Loss_G: 5.3829\n",
      "[219/301][7/8] Loss_D: 1.0377 Loss_G: 2.0516\n",
      "[220/301][0/8] Loss_D: 0.7822 Loss_G: 4.3636\n",
      "[220/301][1/8] Loss_D: 0.6734 Loss_G: 2.1607\n",
      "[220/301][2/8] Loss_D: 0.6173 Loss_G: 3.5747\n",
      "[220/301][3/8] Loss_D: 0.7324 Loss_G: 1.6357\n",
      "[220/301][4/8] Loss_D: 0.7691 Loss_G: 4.3975\n",
      "[220/301][5/8] Loss_D: 0.7661 Loss_G: 1.5542\n",
      "[220/301][6/8] Loss_D: 0.5996 Loss_G: 3.5999\n",
      "[220/301][7/8] Loss_D: 0.4612 Loss_G: 2.6420\n",
      "[221/301][0/8] Loss_D: 0.3199 Loss_G: 3.3352\n",
      "[221/301][1/8] Loss_D: 0.3103 Loss_G: 2.9387\n",
      "[221/301][2/8] Loss_D: 0.4865 Loss_G: 1.8756\n",
      "[221/301][3/8] Loss_D: 0.5014 Loss_G: 3.8836\n",
      "[221/301][4/8] Loss_D: 0.3637 Loss_G: 2.8408\n",
      "[221/301][5/8] Loss_D: 0.3954 Loss_G: 1.9970\n",
      "[221/301][6/8] Loss_D: 0.3934 Loss_G: 4.0089\n",
      "[221/301][7/8] Loss_D: 0.3412 Loss_G: 2.7981\n",
      "[222/301][0/8] Loss_D: 0.3845 Loss_G: 2.2442\n",
      "[222/301][1/8] Loss_D: 0.2840 Loss_G: 3.8309\n",
      "[222/301][2/8] Loss_D: 0.4353 Loss_G: 1.9100\n",
      "[222/301][3/8] Loss_D: 0.3995 Loss_G: 3.4091\n",
      "[222/301][4/8] Loss_D: 0.2589 Loss_G: 3.2697\n",
      "[222/301][5/8] Loss_D: 0.3866 Loss_G: 1.7777\n",
      "[222/301][6/8] Loss_D: 0.4893 Loss_G: 3.8416\n",
      "[222/301][7/8] Loss_D: 0.9305 Loss_G: 0.7459\n",
      "[223/301][0/8] Loss_D: 1.4154 Loss_G: 5.6944\n",
      "[223/301][1/8] Loss_D: 0.6278 Loss_G: 3.5053\n",
      "[223/301][2/8] Loss_D: 0.2620 Loss_G: 2.0636\n",
      "[223/301][3/8] Loss_D: 0.4167 Loss_G: 3.6274\n",
      "[223/301][4/8] Loss_D: 0.2727 Loss_G: 3.2359\n",
      "[223/301][5/8] Loss_D: 0.1600 Loss_G: 3.0915\n",
      "[223/301][6/8] Loss_D: 0.3704 Loss_G: 2.2539\n",
      "[223/301][7/8] Loss_D: 0.2835 Loss_G: 3.6476\n",
      "[224/301][0/8] Loss_D: 0.3027 Loss_G: 2.4893\n",
      "[224/301][1/8] Loss_D: 0.2660 Loss_G: 3.6470\n",
      "[224/301][2/8] Loss_D: 0.2578 Loss_G: 3.0359\n",
      "[224/301][3/8] Loss_D: 0.2806 Loss_G: 1.9825\n",
      "[224/301][4/8] Loss_D: 0.5238 Loss_G: 3.9452\n",
      "[224/301][5/8] Loss_D: 0.4030 Loss_G: 2.3689\n",
      "[224/301][6/8] Loss_D: 0.2913 Loss_G: 2.4869\n",
      "[224/301][7/8] Loss_D: 0.2065 Loss_G: 3.9179\n",
      "[225/301][0/8] Loss_D: 0.2418 Loss_G: 3.5459\n",
      "[225/301][1/8] Loss_D: 0.2180 Loss_G: 3.1228\n",
      "[225/301][2/8] Loss_D: 0.1516 Loss_G: 3.1570\n",
      "[225/301][3/8] Loss_D: 0.3436 Loss_G: 2.1183\n",
      "[225/301][4/8] Loss_D: 0.3618 Loss_G: 3.5106\n",
      "[225/301][5/8] Loss_D: 0.3524 Loss_G: 2.1581\n",
      "[225/301][6/8] Loss_D: 0.4070 Loss_G: 3.5393\n",
      "[225/301][7/8] Loss_D: 0.3225 Loss_G: 2.4197\n",
      "[226/301][0/8] Loss_D: 0.1554 Loss_G: 3.1350\n",
      "[226/301][1/8] Loss_D: 0.2323 Loss_G: 3.6124\n",
      "[226/301][2/8] Loss_D: 0.2209 Loss_G: 2.9660\n",
      "[226/301][3/8] Loss_D: 0.2527 Loss_G: 2.3912\n",
      "[226/301][4/8] Loss_D: 0.3364 Loss_G: 3.9264\n",
      "[226/301][5/8] Loss_D: 0.3470 Loss_G: 2.1784\n",
      "[226/301][6/8] Loss_D: 0.3555 Loss_G: 3.0402\n",
      "[226/301][7/8] Loss_D: 0.3217 Loss_G: 2.7670\n",
      "[227/301][0/8] Loss_D: 0.2363 Loss_G: 2.8966\n",
      "[227/301][1/8] Loss_D: 0.1905 Loss_G: 3.3526\n",
      "[227/301][2/8] Loss_D: 0.2370 Loss_G: 2.8526\n",
      "[227/301][3/8] Loss_D: 0.2970 Loss_G: 2.8583\n",
      "[227/301][4/8] Loss_D: 0.2460 Loss_G: 3.0421\n",
      "[227/301][5/8] Loss_D: 0.2652 Loss_G: 2.5973\n",
      "[227/301][6/8] Loss_D: 0.2087 Loss_G: 3.2362\n",
      "[227/301][7/8] Loss_D: 0.2756 Loss_G: 2.6871\n",
      "[228/301][0/8] Loss_D: 0.2259 Loss_G: 2.9737\n",
      "[228/301][1/8] Loss_D: 0.2412 Loss_G: 3.7121\n",
      "[228/301][2/8] Loss_D: 0.3561 Loss_G: 1.8878\n",
      "[228/301][3/8] Loss_D: 0.2894 Loss_G: 3.5121\n",
      "[228/301][4/8] Loss_D: 0.2730 Loss_G: 3.2808\n",
      "[228/301][5/8] Loss_D: 0.2837 Loss_G: 1.9604\n",
      "[228/301][6/8] Loss_D: 0.4299 Loss_G: 4.4879\n",
      "[228/301][7/8] Loss_D: 0.6480 Loss_G: 0.9199\n",
      "[229/301][0/8] Loss_D: 0.8321 Loss_G: 6.3473\n",
      "[229/301][1/8] Loss_D: 1.0064 Loss_G: 1.3367\n",
      "[229/301][2/8] Loss_D: 0.6716 Loss_G: 5.6024\n",
      "[229/301][3/8] Loss_D: 0.8767 Loss_G: 0.4410\n",
      "[229/301][4/8] Loss_D: 1.3928 Loss_G: 7.4266\n",
      "[229/301][5/8] Loss_D: 1.5529 Loss_G: 1.3532\n",
      "[229/301][6/8] Loss_D: 0.5903 Loss_G: 3.9881\n",
      "[229/301][7/8] Loss_D: 0.2196 Loss_G: 5.4834\n",
      "[230/301][0/8] Loss_D: 0.6898 Loss_G: 1.2992\n",
      "[230/301][1/8] Loss_D: 0.8926 Loss_G: 4.9241\n",
      "[230/301][2/8] Loss_D: 0.3296 Loss_G: 3.5432\n",
      "[230/301][3/8] Loss_D: 0.3499 Loss_G: 2.0210\n",
      "[230/301][4/8] Loss_D: 0.3164 Loss_G: 3.9327\n",
      "[230/301][5/8] Loss_D: 0.4277 Loss_G: 1.8991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230/301][6/8] Loss_D: 0.4904 Loss_G: 3.8490\n",
      "[230/301][7/8] Loss_D: 0.3745 Loss_G: 2.9898\n",
      "[231/301][0/8] Loss_D: 0.3582 Loss_G: 2.7674\n",
      "[231/301][1/8] Loss_D: 0.3181 Loss_G: 3.3640\n",
      "[231/301][2/8] Loss_D: 0.2197 Loss_G: 3.5350\n",
      "[231/301][3/8] Loss_D: 0.2330 Loss_G: 2.6867\n",
      "[231/301][4/8] Loss_D: 0.2348 Loss_G: 3.3854\n",
      "[231/301][5/8] Loss_D: 0.2630 Loss_G: 2.8463\n",
      "[231/301][6/8] Loss_D: 0.1664 Loss_G: 3.0320\n",
      "[231/301][7/8] Loss_D: 0.2701 Loss_G: 2.2628\n",
      "[232/301][0/8] Loss_D: 0.5554 Loss_G: 4.8263\n",
      "[232/301][1/8] Loss_D: 0.3954 Loss_G: 2.8560\n",
      "[232/301][2/8] Loss_D: 0.1687 Loss_G: 2.7755\n",
      "[232/301][3/8] Loss_D: 0.1770 Loss_G: 3.6378\n",
      "[232/301][4/8] Loss_D: 0.2323 Loss_G: 3.0002\n",
      "[232/301][5/8] Loss_D: 0.1959 Loss_G: 2.8446\n",
      "[232/301][6/8] Loss_D: 0.2807 Loss_G: 3.6616\n",
      "[232/301][7/8] Loss_D: 0.2213 Loss_G: 3.3257\n",
      "[233/301][0/8] Loss_D: 0.2133 Loss_G: 2.5137\n",
      "[233/301][1/8] Loss_D: 0.3666 Loss_G: 4.2485\n",
      "[233/301][2/8] Loss_D: 0.4038 Loss_G: 2.0616\n",
      "[233/301][3/8] Loss_D: 0.3296 Loss_G: 3.5963\n",
      "[233/301][4/8] Loss_D: 0.3066 Loss_G: 2.6094\n",
      "[233/301][5/8] Loss_D: 0.2308 Loss_G: 3.1202\n",
      "[233/301][6/8] Loss_D: 0.1539 Loss_G: 3.6008\n",
      "[233/301][7/8] Loss_D: 0.4907 Loss_G: 2.0427\n",
      "[234/301][0/8] Loss_D: 0.5687 Loss_G: 5.0362\n",
      "[234/301][1/8] Loss_D: 0.3831 Loss_G: 2.7009\n",
      "[234/301][2/8] Loss_D: 0.2013 Loss_G: 3.1453\n",
      "[234/301][3/8] Loss_D: 0.2822 Loss_G: 3.3652\n",
      "[234/301][4/8] Loss_D: 0.2792 Loss_G: 2.4998\n",
      "[234/301][5/8] Loss_D: 0.2296 Loss_G: 3.5882\n",
      "[234/301][6/8] Loss_D: 0.2178 Loss_G: 2.9909\n",
      "[234/301][7/8] Loss_D: 0.4129 Loss_G: 0.9721\n",
      "[235/301][0/8] Loss_D: 2.0426 Loss_G: 8.9816\n",
      "[235/301][1/8] Loss_D: 2.0940 Loss_G: 2.7993\n",
      "[235/301][2/8] Loss_D: 0.2114 Loss_G: 3.9113\n",
      "[235/301][3/8] Loss_D: 0.5370 Loss_G: 0.7055\n",
      "[235/301][4/8] Loss_D: 1.5050 Loss_G: 9.7972\n",
      "[235/301][5/8] Loss_D: 5.4276 Loss_G: 0.7703\n",
      "[235/301][6/8] Loss_D: 1.1340 Loss_G: 8.7640\n",
      "[235/301][7/8] Loss_D: 3.5973 Loss_G: 0.2201\n",
      "[236/301][0/8] Loss_D: 3.8154 Loss_G: 6.6486\n",
      "[236/301][1/8] Loss_D: 0.8984 Loss_G: 2.5641\n",
      "[236/301][2/8] Loss_D: 0.6537 Loss_G: 2.5450\n",
      "[236/301][3/8] Loss_D: 0.9131 Loss_G: 3.5469\n",
      "[236/301][4/8] Loss_D: 0.8851 Loss_G: 1.3587\n",
      "[236/301][5/8] Loss_D: 1.0543 Loss_G: 5.4986\n",
      "[236/301][6/8] Loss_D: 1.3042 Loss_G: 1.1380\n",
      "[236/301][7/8] Loss_D: 0.6682 Loss_G: 4.4969\n",
      "[237/301][0/8] Loss_D: 0.4579 Loss_G: 4.7801\n",
      "[237/301][1/8] Loss_D: 0.4101 Loss_G: 3.0402\n",
      "[237/301][2/8] Loss_D: 0.3093 Loss_G: 2.5933\n",
      "[237/301][3/8] Loss_D: 0.5272 Loss_G: 3.5798\n",
      "[237/301][4/8] Loss_D: 0.4859 Loss_G: 2.3289\n",
      "[237/301][5/8] Loss_D: 0.5039 Loss_G: 2.8309\n",
      "[237/301][6/8] Loss_D: 0.4495 Loss_G: 2.5697\n",
      "[237/301][7/8] Loss_D: 0.3332 Loss_G: 3.5512\n",
      "[238/301][0/8] Loss_D: 0.3011 Loss_G: 3.4479\n",
      "[238/301][1/8] Loss_D: 0.1913 Loss_G: 3.5706\n",
      "[238/301][2/8] Loss_D: 0.4418 Loss_G: 1.6051\n",
      "[238/301][3/8] Loss_D: 0.7467 Loss_G: 4.3003\n",
      "[238/301][4/8] Loss_D: 0.8405 Loss_G: 1.5312\n",
      "[238/301][5/8] Loss_D: 0.5378 Loss_G: 3.3308\n",
      "[238/301][6/8] Loss_D: 0.2264 Loss_G: 3.7464\n",
      "[238/301][7/8] Loss_D: 0.4433 Loss_G: 2.0322\n",
      "[239/301][0/8] Loss_D: 0.6795 Loss_G: 4.4218\n",
      "[239/301][1/8] Loss_D: 0.4159 Loss_G: 2.8863\n",
      "[239/301][2/8] Loss_D: 0.2260 Loss_G: 2.9627\n",
      "[239/301][3/8] Loss_D: 0.2353 Loss_G: 3.2381\n",
      "[239/301][4/8] Loss_D: 0.2169 Loss_G: 3.3905\n",
      "[239/301][5/8] Loss_D: 0.2906 Loss_G: 2.4899\n",
      "[239/301][6/8] Loss_D: 0.3647 Loss_G: 3.0256\n",
      "[239/301][7/8] Loss_D: 0.3609 Loss_G: 2.4287\n",
      "[240/301][0/8] Loss_D: 0.3213 Loss_G: 3.6029\n",
      "[240/301][1/8] Loss_D: 0.3217 Loss_G: 2.6814\n",
      "[240/301][2/8] Loss_D: 0.2275 Loss_G: 2.8168\n",
      "[240/301][3/8] Loss_D: 0.2296 Loss_G: 3.2190\n",
      "[240/301][4/8] Loss_D: 0.3032 Loss_G: 2.8556\n",
      "[240/301][5/8] Loss_D: 0.3084 Loss_G: 2.4794\n",
      "[240/301][6/8] Loss_D: 0.3145 Loss_G: 3.1004\n",
      "[240/301][7/8] Loss_D: 0.2680 Loss_G: 4.5491\n",
      "[241/301][0/8] Loss_D: 0.3618 Loss_G: 2.4781\n",
      "[241/301][1/8] Loss_D: 0.2505 Loss_G: 2.8225\n",
      "[241/301][2/8] Loss_D: 0.3271 Loss_G: 4.0013\n",
      "[241/301][3/8] Loss_D: 0.5142 Loss_G: 2.2948\n",
      "[241/301][4/8] Loss_D: 0.2643 Loss_G: 2.8184\n",
      "[241/301][5/8] Loss_D: 0.1653 Loss_G: 3.5971\n",
      "[241/301][6/8] Loss_D: 0.2387 Loss_G: 2.8353\n",
      "[241/301][7/8] Loss_D: 0.4405 Loss_G: 2.2043\n",
      "[242/301][0/8] Loss_D: 0.4422 Loss_G: 3.8992\n",
      "[242/301][1/8] Loss_D: 0.3876 Loss_G: 3.2703\n",
      "[242/301][2/8] Loss_D: 0.2954 Loss_G: 2.1540\n",
      "[242/301][3/8] Loss_D: 0.3056 Loss_G: 3.7537\n",
      "[242/301][4/8] Loss_D: 0.3446 Loss_G: 3.1738\n",
      "[242/301][5/8] Loss_D: 0.1877 Loss_G: 2.9133\n",
      "[242/301][6/8] Loss_D: 0.2407 Loss_G: 2.9531\n",
      "[242/301][7/8] Loss_D: 0.1137 Loss_G: 3.9276\n",
      "[243/301][0/8] Loss_D: 0.2240 Loss_G: 3.0354\n",
      "[243/301][1/8] Loss_D: 0.2048 Loss_G: 3.1678\n",
      "[243/301][2/8] Loss_D: 0.2426 Loss_G: 2.8567\n",
      "[243/301][3/8] Loss_D: 0.2640 Loss_G: 2.8651\n",
      "[243/301][4/8] Loss_D: 0.2488 Loss_G: 3.0859\n",
      "[243/301][5/8] Loss_D: 0.2509 Loss_G: 2.2864\n",
      "[243/301][6/8] Loss_D: 0.2264 Loss_G: 3.0952\n",
      "[243/301][7/8] Loss_D: 0.3006 Loss_G: 2.1167\n",
      "[244/301][0/8] Loss_D: 0.5428 Loss_G: 5.0951\n",
      "[244/301][1/8] Loss_D: 0.5746 Loss_G: 2.1642\n",
      "[244/301][2/8] Loss_D: 0.3899 Loss_G: 3.6363\n",
      "[244/301][3/8] Loss_D: 0.2234 Loss_G: 3.3321\n",
      "[244/301][4/8] Loss_D: 0.2249 Loss_G: 2.5301\n",
      "[244/301][5/8] Loss_D: 0.2408 Loss_G: 3.5880\n",
      "[244/301][6/8] Loss_D: 0.2112 Loss_G: 3.5138\n",
      "[244/301][7/8] Loss_D: 0.2028 Loss_G: 2.7288\n",
      "[245/301][0/8] Loss_D: 0.2271 Loss_G: 3.4369\n",
      "[245/301][1/8] Loss_D: 0.2135 Loss_G: 3.3117\n",
      "[245/301][2/8] Loss_D: 0.2454 Loss_G: 2.5884\n",
      "[245/301][3/8] Loss_D: 0.2526 Loss_G: 3.2661\n",
      "[245/301][4/8] Loss_D: 0.2546 Loss_G: 2.6412\n",
      "[245/301][5/8] Loss_D: 0.2111 Loss_G: 3.2774\n",
      "[245/301][6/8] Loss_D: 0.2517 Loss_G: 3.3603\n",
      "[245/301][7/8] Loss_D: 0.1621 Loss_G: 3.9190\n",
      "[246/301][0/8] Loss_D: 0.1702 Loss_G: 2.8367\n",
      "[246/301][1/8] Loss_D: 0.1684 Loss_G: 3.0756\n",
      "[246/301][2/8] Loss_D: 0.2470 Loss_G: 3.6248\n",
      "[246/301][3/8] Loss_D: 0.1767 Loss_G: 3.1538\n",
      "[246/301][4/8] Loss_D: 0.1929 Loss_G: 2.9724\n",
      "[246/301][5/8] Loss_D: 0.1399 Loss_G: 3.3934\n",
      "[246/301][6/8] Loss_D: 0.2318 Loss_G: 2.7388\n",
      "[246/301][7/8] Loss_D: 0.2352 Loss_G: 2.1190\n",
      "[247/301][0/8] Loss_D: 0.3372 Loss_G: 4.4671\n",
      "[247/301][1/8] Loss_D: 0.1725 Loss_G: 4.0353\n",
      "[247/301][2/8] Loss_D: 0.1963 Loss_G: 2.6763\n",
      "[247/301][3/8] Loss_D: 0.2050 Loss_G: 3.1302\n",
      "[247/301][4/8] Loss_D: 0.1849 Loss_G: 3.9142\n",
      "[247/301][5/8] Loss_D: 0.2213 Loss_G: 2.7356\n",
      "[247/301][6/8] Loss_D: 0.2237 Loss_G: 3.2603\n",
      "[247/301][7/8] Loss_D: 0.2746 Loss_G: 2.3830\n",
      "[248/301][0/8] Loss_D: 0.2334 Loss_G: 3.6505\n",
      "[248/301][1/8] Loss_D: 0.1215 Loss_G: 3.8785\n",
      "[248/301][2/8] Loss_D: 0.1642 Loss_G: 2.9386\n",
      "[248/301][3/8] Loss_D: 0.1712 Loss_G: 2.8505\n",
      "[248/301][4/8] Loss_D: 0.2098 Loss_G: 3.3949\n",
      "[248/301][5/8] Loss_D: 0.1803 Loss_G: 3.4242\n",
      "[248/301][6/8] Loss_D: 0.1631 Loss_G: 3.0011\n",
      "[248/301][7/8] Loss_D: 0.1582 Loss_G: 3.1790\n",
      "[249/301][0/8] Loss_D: 0.2118 Loss_G: 3.2368\n",
      "[249/301][1/8] Loss_D: 0.1398 Loss_G: 3.4918\n",
      "[249/301][2/8] Loss_D: 0.2386 Loss_G: 2.1521\n",
      "[249/301][3/8] Loss_D: 0.2717 Loss_G: 4.0450\n",
      "[249/301][4/8] Loss_D: 0.1709 Loss_G: 3.5111\n",
      "[249/301][5/8] Loss_D: 0.0937 Loss_G: 3.5124\n",
      "[249/301][6/8] Loss_D: 0.2301 Loss_G: 2.0774\n",
      "[249/301][7/8] Loss_D: 0.3229 Loss_G: 4.7739\n",
      "[250/301][0/8] Loss_D: 0.0829 Loss_G: 4.5179\n",
      "[250/301][1/8] Loss_D: 0.2322 Loss_G: 2.2458\n",
      "[250/301][2/8] Loss_D: 0.2145 Loss_G: 3.4337\n",
      "[250/301][3/8] Loss_D: 0.2182 Loss_G: 3.3449\n",
      "[250/301][4/8] Loss_D: 0.2325 Loss_G: 2.3880\n",
      "[250/301][5/8] Loss_D: 0.1803 Loss_G: 3.7459\n",
      "[250/301][6/8] Loss_D: 0.2739 Loss_G: 2.5038\n",
      "[250/301][7/8] Loss_D: 0.1414 Loss_G: 2.9424\n",
      "[251/301][0/8] Loss_D: 0.2180 Loss_G: 4.3905\n",
      "[251/301][1/8] Loss_D: 0.1783 Loss_G: 3.3305\n",
      "[251/301][2/8] Loss_D: 0.1775 Loss_G: 3.1000\n",
      "[251/301][3/8] Loss_D: 0.1562 Loss_G: 3.2567\n",
      "[251/301][4/8] Loss_D: 0.1127 Loss_G: 3.8542\n",
      "[251/301][5/8] Loss_D: 0.1546 Loss_G: 2.9557\n",
      "[251/301][6/8] Loss_D: 0.2180 Loss_G: 3.0488\n",
      "[251/301][7/8] Loss_D: 0.2077 Loss_G: 3.2433\n",
      "[252/301][0/8] Loss_D: 0.2008 Loss_G: 3.4090\n",
      "[252/301][1/8] Loss_D: 0.1678 Loss_G: 3.3105\n",
      "[252/301][2/8] Loss_D: 0.2178 Loss_G: 2.6268\n",
      "[252/301][3/8] Loss_D: 0.1689 Loss_G: 4.0402\n",
      "[252/301][4/8] Loss_D: 0.1940 Loss_G: 2.6342\n",
      "[252/301][5/8] Loss_D: 0.1793 Loss_G: 3.1221\n",
      "[252/301][6/8] Loss_D: 0.1668 Loss_G: 3.3512\n",
      "[252/301][7/8] Loss_D: 0.1862 Loss_G: 2.6907\n",
      "[253/301][0/8] Loss_D: 0.1962 Loss_G: 3.8414\n",
      "[253/301][1/8] Loss_D: 0.1615 Loss_G: 3.5405\n",
      "[253/301][2/8] Loss_D: 0.1645 Loss_G: 2.9488\n",
      "[253/301][3/8] Loss_D: 0.1432 Loss_G: 3.2336\n",
      "[253/301][4/8] Loss_D: 0.1494 Loss_G: 3.2809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253/301][5/8] Loss_D: 0.1403 Loss_G: 3.0128\n",
      "[253/301][6/8] Loss_D: 0.1543 Loss_G: 3.4719\n",
      "[253/301][7/8] Loss_D: 0.0936 Loss_G: 3.7564\n",
      "[254/301][0/8] Loss_D: 0.0955 Loss_G: 3.5651\n",
      "[254/301][1/8] Loss_D: 0.1266 Loss_G: 3.3904\n",
      "[254/301][2/8] Loss_D: 0.1598 Loss_G: 3.5481\n",
      "[254/301][3/8] Loss_D: 0.1314 Loss_G: 3.6166\n",
      "[254/301][4/8] Loss_D: 0.1552 Loss_G: 2.8984\n",
      "[254/301][5/8] Loss_D: 0.1423 Loss_G: 3.2637\n",
      "[254/301][6/8] Loss_D: 0.2102 Loss_G: 2.9808\n",
      "[254/301][7/8] Loss_D: 0.1721 Loss_G: 2.6201\n",
      "[255/301][0/8] Loss_D: 0.1822 Loss_G: 4.2008\n",
      "[255/301][1/8] Loss_D: 0.1139 Loss_G: 4.0128\n",
      "[255/301][2/8] Loss_D: 0.1993 Loss_G: 2.2781\n",
      "[255/301][3/8] Loss_D: 0.2816 Loss_G: 4.6626\n",
      "[255/301][4/8] Loss_D: 0.5277 Loss_G: 0.8339\n",
      "[255/301][5/8] Loss_D: 1.0458 Loss_G: 10.0668\n",
      "[255/301][6/8] Loss_D: 4.0646 Loss_G: 1.4712\n",
      "[255/301][7/8] Loss_D: 0.9976 Loss_G: 14.7106\n",
      "[256/301][0/8] Loss_D: 7.7246 Loss_G: 0.0822\n",
      "[256/301][1/8] Loss_D: 5.5777 Loss_G: 6.1552\n",
      "[256/301][2/8] Loss_D: 1.5857 Loss_G: 2.0163\n",
      "[256/301][3/8] Loss_D: 1.2555 Loss_G: 3.8786\n",
      "[256/301][4/8] Loss_D: 0.8039 Loss_G: 3.1977\n",
      "[256/301][5/8] Loss_D: 1.1198 Loss_G: 1.5727\n",
      "[256/301][6/8] Loss_D: 1.2802 Loss_G: 7.7593\n",
      "[256/301][7/8] Loss_D: 2.9074 Loss_G: 0.8648\n",
      "[257/301][0/8] Loss_D: 1.8178 Loss_G: 5.0860\n",
      "[257/301][1/8] Loss_D: 0.5606 Loss_G: 3.9189\n",
      "[257/301][2/8] Loss_D: 0.6474 Loss_G: 3.9950\n",
      "[257/301][3/8] Loss_D: 0.7033 Loss_G: 1.7811\n",
      "[257/301][4/8] Loss_D: 0.9323 Loss_G: 5.4974\n",
      "[257/301][5/8] Loss_D: 0.7716 Loss_G: 1.9643\n",
      "[257/301][6/8] Loss_D: 0.5934 Loss_G: 4.5930\n",
      "[257/301][7/8] Loss_D: 0.8431 Loss_G: 1.1440\n",
      "[258/301][0/8] Loss_D: 1.2923 Loss_G: 5.7427\n",
      "[258/301][1/8] Loss_D: 0.6526 Loss_G: 3.4077\n",
      "[258/301][2/8] Loss_D: 0.3060 Loss_G: 2.6943\n",
      "[258/301][3/8] Loss_D: 0.4564 Loss_G: 4.8433\n",
      "[258/301][4/8] Loss_D: 0.4263 Loss_G: 2.9169\n",
      "[258/301][5/8] Loss_D: 0.3265 Loss_G: 2.2995\n",
      "[258/301][6/8] Loss_D: 0.3898 Loss_G: 3.5832\n",
      "[258/301][7/8] Loss_D: 0.9411 Loss_G: 1.2892\n",
      "[259/301][0/8] Loss_D: 1.1196 Loss_G: 6.1685\n",
      "[259/301][1/8] Loss_D: 0.5988 Loss_G: 4.1231\n",
      "[259/301][2/8] Loss_D: 0.2676 Loss_G: 2.0952\n",
      "[259/301][3/8] Loss_D: 0.4608 Loss_G: 3.5668\n",
      "[259/301][4/8] Loss_D: 0.2673 Loss_G: 3.9473\n",
      "[259/301][5/8] Loss_D: 0.2389 Loss_G: 3.0265\n",
      "[259/301][6/8] Loss_D: 0.2590 Loss_G: 2.5006\n",
      "[259/301][7/8] Loss_D: 0.4160 Loss_G: 4.9153\n",
      "[260/301][0/8] Loss_D: 0.4974 Loss_G: 1.9097\n",
      "[260/301][1/8] Loss_D: 0.3660 Loss_G: 3.3332\n",
      "[260/301][2/8] Loss_D: 0.1511 Loss_G: 4.4626\n",
      "[260/301][3/8] Loss_D: 0.4492 Loss_G: 2.1350\n",
      "[260/301][4/8] Loss_D: 0.3907 Loss_G: 3.1370\n",
      "[260/301][5/8] Loss_D: 0.2475 Loss_G: 3.3239\n",
      "[260/301][6/8] Loss_D: 0.2222 Loss_G: 2.9457\n",
      "[260/301][7/8] Loss_D: 0.3245 Loss_G: 3.3025\n",
      "[261/301][0/8] Loss_D: 0.2515 Loss_G: 3.5816\n",
      "[261/301][1/8] Loss_D: 0.4684 Loss_G: 1.6569\n",
      "[261/301][2/8] Loss_D: 0.4853 Loss_G: 4.0038\n",
      "[261/301][3/8] Loss_D: 0.1676 Loss_G: 4.3027\n",
      "[261/301][4/8] Loss_D: 0.2989 Loss_G: 2.3546\n",
      "[261/301][5/8] Loss_D: 0.3533 Loss_G: 2.7675\n",
      "[261/301][6/8] Loss_D: 0.2150 Loss_G: 3.8019\n",
      "[261/301][7/8] Loss_D: 0.1710 Loss_G: 3.8122\n",
      "[262/301][0/8] Loss_D: 0.1749 Loss_G: 2.8793\n",
      "[262/301][1/8] Loss_D: 0.1871 Loss_G: 2.8334\n",
      "[262/301][2/8] Loss_D: 0.3238 Loss_G: 3.0680\n",
      "[262/301][3/8] Loss_D: 0.2437 Loss_G: 3.0215\n",
      "[262/301][4/8] Loss_D: 0.2319 Loss_G: 2.4161\n",
      "[262/301][5/8] Loss_D: 0.2768 Loss_G: 3.1387\n",
      "[262/301][6/8] Loss_D: 0.3789 Loss_G: 4.7228\n",
      "[262/301][7/8] Loss_D: 0.4668 Loss_G: 2.6535\n",
      "[263/301][0/8] Loss_D: 0.4166 Loss_G: 2.7299\n",
      "[263/301][1/8] Loss_D: 0.3076 Loss_G: 3.6539\n",
      "[263/301][2/8] Loss_D: 0.2332 Loss_G: 3.0839\n",
      "[263/301][3/8] Loss_D: 0.2670 Loss_G: 3.2458\n",
      "[263/301][4/8] Loss_D: 0.2908 Loss_G: 2.6450\n",
      "[263/301][5/8] Loss_D: 0.2755 Loss_G: 3.9552\n",
      "[263/301][6/8] Loss_D: 0.2981 Loss_G: 2.5209\n",
      "[263/301][7/8] Loss_D: 0.3014 Loss_G: 2.2785\n",
      "[264/301][0/8] Loss_D: 0.5038 Loss_G: 5.4145\n",
      "[264/301][1/8] Loss_D: 0.2700 Loss_G: 4.0126\n",
      "[264/301][2/8] Loss_D: 0.2551 Loss_G: 2.0205\n",
      "[264/301][3/8] Loss_D: 0.2845 Loss_G: 3.7384\n",
      "[264/301][4/8] Loss_D: 0.2865 Loss_G: 3.0461\n",
      "[264/301][5/8] Loss_D: 0.2033 Loss_G: 3.0692\n",
      "[264/301][6/8] Loss_D: 0.2066 Loss_G: 3.8928\n",
      "[264/301][7/8] Loss_D: 0.6088 Loss_G: 0.9235\n",
      "[265/301][0/8] Loss_D: 1.0238 Loss_G: 6.8950\n",
      "[265/301][1/8] Loss_D: 1.0871 Loss_G: 1.3512\n",
      "[265/301][2/8] Loss_D: 0.7246 Loss_G: 6.7391\n",
      "[265/301][3/8] Loss_D: 1.3835 Loss_G: 0.3501\n",
      "[265/301][4/8] Loss_D: 1.7149 Loss_G: 8.0938\n",
      "[265/301][5/8] Loss_D: 1.9488 Loss_G: 0.7382\n",
      "[265/301][6/8] Loss_D: 1.4757 Loss_G: 6.0905\n",
      "[265/301][7/8] Loss_D: 0.7971 Loss_G: 2.4375\n",
      "[266/301][0/8] Loss_D: 0.6470 Loss_G: 4.2127\n",
      "[266/301][1/8] Loss_D: 0.4156 Loss_G: 3.0228\n",
      "[266/301][2/8] Loss_D: 0.3342 Loss_G: 3.4687\n",
      "[266/301][3/8] Loss_D: 0.4501 Loss_G: 3.0996\n",
      "[266/301][4/8] Loss_D: 0.3287 Loss_G: 2.7254\n",
      "[266/301][5/8] Loss_D: 0.3162 Loss_G: 3.5224\n",
      "[266/301][6/8] Loss_D: 0.2157 Loss_G: 3.2802\n",
      "[266/301][7/8] Loss_D: 0.3641 Loss_G: 1.8408\n",
      "[267/301][0/8] Loss_D: 0.6376 Loss_G: 5.7380\n",
      "[267/301][1/8] Loss_D: 0.4571 Loss_G: 3.1078\n",
      "[267/301][2/8] Loss_D: 0.1817 Loss_G: 2.4482\n",
      "[267/301][3/8] Loss_D: 0.3330 Loss_G: 4.0070\n",
      "[267/301][4/8] Loss_D: 0.2224 Loss_G: 3.5112\n",
      "[267/301][5/8] Loss_D: 0.1672 Loss_G: 3.0146\n",
      "[267/301][6/8] Loss_D: 0.2850 Loss_G: 2.4587\n",
      "[267/301][7/8] Loss_D: 0.2675 Loss_G: 3.7049\n",
      "[268/301][0/8] Loss_D: 0.2213 Loss_G: 3.5214\n",
      "[268/301][1/8] Loss_D: 0.2782 Loss_G: 2.6215\n",
      "[268/301][2/8] Loss_D: 0.1943 Loss_G: 3.3490\n",
      "[268/301][3/8] Loss_D: 0.2048 Loss_G: 3.2983\n",
      "[268/301][4/8] Loss_D: 0.1991 Loss_G: 2.8329\n",
      "[268/301][5/8] Loss_D: 0.2599 Loss_G: 3.3603\n",
      "[268/301][6/8] Loss_D: 0.1630 Loss_G: 3.5237\n",
      "[268/301][7/8] Loss_D: 0.2621 Loss_G: 2.1977\n",
      "[269/301][0/8] Loss_D: 0.3019 Loss_G: 3.7397\n",
      "[269/301][1/8] Loss_D: 0.1742 Loss_G: 3.7048\n",
      "[269/301][2/8] Loss_D: 0.2613 Loss_G: 2.2639\n",
      "[269/301][3/8] Loss_D: 0.2181 Loss_G: 3.4390\n",
      "[269/301][4/8] Loss_D: 0.1758 Loss_G: 3.8873\n",
      "[269/301][5/8] Loss_D: 0.2806 Loss_G: 2.1696\n",
      "[269/301][6/8] Loss_D: 0.2171 Loss_G: 3.4085\n",
      "[269/301][7/8] Loss_D: 0.2253 Loss_G: 5.8057\n",
      "[270/301][0/8] Loss_D: 0.4131 Loss_G: 2.4272\n",
      "[270/301][1/8] Loss_D: 0.2062 Loss_G: 3.1515\n",
      "[270/301][2/8] Loss_D: 0.2155 Loss_G: 3.8751\n",
      "[270/301][3/8] Loss_D: 0.2524 Loss_G: 2.7213\n",
      "[270/301][4/8] Loss_D: 0.1901 Loss_G: 3.2040\n",
      "[270/301][5/8] Loss_D: 0.1563 Loss_G: 3.5299\n",
      "[270/301][6/8] Loss_D: 0.1381 Loss_G: 3.5060\n",
      "[270/301][7/8] Loss_D: 0.2050 Loss_G: 2.6197\n",
      "[271/301][0/8] Loss_D: 0.2891 Loss_G: 4.1954\n",
      "[271/301][1/8] Loss_D: 0.2201 Loss_G: 3.2528\n",
      "[271/301][2/8] Loss_D: 0.1966 Loss_G: 2.6104\n",
      "[271/301][3/8] Loss_D: 0.1980 Loss_G: 3.8649\n",
      "[271/301][4/8] Loss_D: 0.1719 Loss_G: 3.3570\n",
      "[271/301][5/8] Loss_D: 0.1772 Loss_G: 3.2432\n",
      "[271/301][6/8] Loss_D: 0.1232 Loss_G: 3.5301\n",
      "[271/301][7/8] Loss_D: 0.2680 Loss_G: 1.8482\n",
      "[272/301][0/8] Loss_D: 0.4132 Loss_G: 5.0249\n",
      "[272/301][1/8] Loss_D: 0.2854 Loss_G: 3.2041\n",
      "[272/301][2/8] Loss_D: 0.1352 Loss_G: 3.1274\n",
      "[272/301][3/8] Loss_D: 0.1446 Loss_G: 3.3138\n",
      "[272/301][4/8] Loss_D: 0.2006 Loss_G: 3.1712\n",
      "[272/301][5/8] Loss_D: 0.2216 Loss_G: 2.9917\n",
      "[272/301][6/8] Loss_D: 0.2307 Loss_G: 3.5554\n",
      "[272/301][7/8] Loss_D: 0.3221 Loss_G: 1.9657\n",
      "[273/301][0/8] Loss_D: 0.3838 Loss_G: 4.4261\n",
      "[273/301][1/8] Loss_D: 0.1559 Loss_G: 4.4169\n",
      "[273/301][2/8] Loss_D: 0.4090 Loss_G: 1.4336\n",
      "[273/301][3/8] Loss_D: 0.6647 Loss_G: 5.9347\n",
      "[273/301][4/8] Loss_D: 0.4668 Loss_G: 2.7949\n",
      "[273/301][5/8] Loss_D: 0.1802 Loss_G: 2.9834\n",
      "[273/301][6/8] Loss_D: 0.2485 Loss_G: 2.9908\n",
      "[273/301][7/8] Loss_D: 0.2792 Loss_G: 2.0876\n",
      "[274/301][0/8] Loss_D: 0.4180 Loss_G: 4.9170\n",
      "[274/301][1/8] Loss_D: 0.2886 Loss_G: 3.2374\n",
      "[274/301][2/8] Loss_D: 0.1239 Loss_G: 3.0202\n",
      "[274/301][3/8] Loss_D: 0.2111 Loss_G: 3.7807\n",
      "[274/301][4/8] Loss_D: 0.1478 Loss_G: 3.6775\n",
      "[274/301][5/8] Loss_D: 0.1435 Loss_G: 3.1142\n",
      "[274/301][6/8] Loss_D: 0.1868 Loss_G: 2.5504\n",
      "[274/301][7/8] Loss_D: 0.2720 Loss_G: 5.4057\n",
      "[275/301][0/8] Loss_D: 0.2372 Loss_G: 3.3057\n",
      "[275/301][1/8] Loss_D: 0.1841 Loss_G: 2.9178\n",
      "[275/301][2/8] Loss_D: 0.1315 Loss_G: 3.8026\n",
      "[275/301][3/8] Loss_D: 0.1235 Loss_G: 3.5295\n",
      "[275/301][4/8] Loss_D: 0.1682 Loss_G: 2.8369\n",
      "[275/301][5/8] Loss_D: 0.1739 Loss_G: 3.4898\n",
      "[275/301][6/8] Loss_D: 0.2216 Loss_G: 2.5738\n",
      "[275/301][7/8] Loss_D: 0.1991 Loss_G: 4.2216\n",
      "[276/301][0/8] Loss_D: 0.1379 Loss_G: 4.0569\n",
      "[276/301][1/8] Loss_D: 0.1671 Loss_G: 3.1712\n",
      "[276/301][2/8] Loss_D: 0.1935 Loss_G: 2.8717\n",
      "[276/301][3/8] Loss_D: 0.1296 Loss_G: 3.6623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276/301][4/8] Loss_D: 0.1709 Loss_G: 2.9474\n",
      "[276/301][5/8] Loss_D: 0.1648 Loss_G: 3.1684\n",
      "[276/301][6/8] Loss_D: 0.1175 Loss_G: 3.8202\n",
      "[276/301][7/8] Loss_D: 0.2288 Loss_G: 2.4979\n",
      "[277/301][0/8] Loss_D: 0.2357 Loss_G: 3.6629\n",
      "[277/301][1/8] Loss_D: 0.2024 Loss_G: 3.4831\n",
      "[277/301][2/8] Loss_D: 0.1125 Loss_G: 3.7603\n",
      "[277/301][3/8] Loss_D: 0.1661 Loss_G: 2.9994\n",
      "[277/301][4/8] Loss_D: 0.1454 Loss_G: 3.4921\n",
      "[277/301][5/8] Loss_D: 0.0999 Loss_G: 4.1255\n",
      "[277/301][6/8] Loss_D: 0.2668 Loss_G: 1.7936\n",
      "[277/301][7/8] Loss_D: 0.0760 Loss_G: 3.2754\n",
      "[278/301][0/8] Loss_D: 0.4206 Loss_G: 6.2694\n",
      "[278/301][1/8] Loss_D: 0.6987 Loss_G: 1.5115\n",
      "[278/301][2/8] Loss_D: 0.4186 Loss_G: 6.2048\n",
      "[278/301][3/8] Loss_D: 0.7371 Loss_G: 0.3532\n",
      "[278/301][4/8] Loss_D: 1.8030 Loss_G: 11.0035\n",
      "[278/301][5/8] Loss_D: 4.8786 Loss_G: 1.9580\n",
      "[278/301][6/8] Loss_D: 0.9461 Loss_G: 9.0340\n",
      "[278/301][7/8] Loss_D: 4.6882 Loss_G: 0.0905\n",
      "[279/301][0/8] Loss_D: 4.2169 Loss_G: 6.9665\n",
      "[279/301][1/8] Loss_D: 1.6970 Loss_G: 2.3122\n",
      "[279/301][2/8] Loss_D: 0.9741 Loss_G: 2.8069\n",
      "[279/301][3/8] Loss_D: 0.9084 Loss_G: 4.8184\n",
      "[279/301][4/8] Loss_D: 1.4345 Loss_G: 0.2858\n",
      "[279/301][5/8] Loss_D: 2.5636 Loss_G: 7.8954\n",
      "[279/301][6/8] Loss_D: 0.7171 Loss_G: 4.5723\n",
      "[279/301][7/8] Loss_D: 0.4743 Loss_G: 1.6657\n",
      "[280/301][0/8] Loss_D: 0.7407 Loss_G: 4.4575\n",
      "[280/301][1/8] Loss_D: 0.2840 Loss_G: 4.2363\n",
      "[280/301][2/8] Loss_D: 0.6811 Loss_G: 1.2455\n",
      "[280/301][3/8] Loss_D: 0.9405 Loss_G: 5.1115\n",
      "[280/301][4/8] Loss_D: 0.5835 Loss_G: 2.6978\n",
      "[280/301][5/8] Loss_D: 0.2311 Loss_G: 3.2646\n",
      "[280/301][6/8] Loss_D: 0.4607 Loss_G: 2.7824\n",
      "[280/301][7/8] Loss_D: 0.2824 Loss_G: 3.4544\n",
      "[281/301][0/8] Loss_D: 0.1862 Loss_G: 3.8012\n",
      "[281/301][1/8] Loss_D: 0.1996 Loss_G: 3.5385\n",
      "[281/301][2/8] Loss_D: 0.3230 Loss_G: 2.2244\n",
      "[281/301][3/8] Loss_D: 0.3614 Loss_G: 3.6037\n",
      "[281/301][4/8] Loss_D: 0.3711 Loss_G: 2.8846\n",
      "[281/301][5/8] Loss_D: 0.3212 Loss_G: 2.3956\n",
      "[281/301][6/8] Loss_D: 0.2708 Loss_G: 2.9784\n",
      "[281/301][7/8] Loss_D: 0.1997 Loss_G: 3.5913\n",
      "[282/301][0/8] Loss_D: 0.2933 Loss_G: 3.5933\n",
      "[282/301][1/8] Loss_D: 0.2316 Loss_G: 2.9093\n",
      "[282/301][2/8] Loss_D: 0.2659 Loss_G: 3.4675\n",
      "[282/301][3/8] Loss_D: 0.2181 Loss_G: 3.3280\n",
      "[282/301][4/8] Loss_D: 0.1523 Loss_G: 3.2559\n",
      "[282/301][5/8] Loss_D: 0.3155 Loss_G: 1.8320\n",
      "[282/301][6/8] Loss_D: 0.5660 Loss_G: 4.6718\n",
      "[282/301][7/8] Loss_D: 0.3856 Loss_G: 3.2344\n",
      "[283/301][0/8] Loss_D: 0.2502 Loss_G: 2.7106\n",
      "[283/301][1/8] Loss_D: 0.2676 Loss_G: 4.0342\n",
      "[283/301][2/8] Loss_D: 0.2914 Loss_G: 2.8169\n",
      "[283/301][3/8] Loss_D: 0.2300 Loss_G: 2.7378\n",
      "[283/301][4/8] Loss_D: 0.2032 Loss_G: 3.9851\n",
      "[283/301][5/8] Loss_D: 0.2944 Loss_G: 2.7186\n",
      "[283/301][6/8] Loss_D: 0.2650 Loss_G: 2.7833\n",
      "[283/301][7/8] Loss_D: 0.1589 Loss_G: 4.7298\n",
      "[284/301][0/8] Loss_D: 0.3075 Loss_G: 2.5101\n",
      "[284/301][1/8] Loss_D: 0.1975 Loss_G: 3.0715\n",
      "[284/301][2/8] Loss_D: 0.2215 Loss_G: 3.5605\n",
      "[284/301][3/8] Loss_D: 0.1242 Loss_G: 3.9755\n",
      "[284/301][4/8] Loss_D: 0.4005 Loss_G: 1.3193\n",
      "[284/301][5/8] Loss_D: 0.4487 Loss_G: 4.3011\n",
      "[284/301][6/8] Loss_D: 0.2525 Loss_G: 3.7772\n",
      "[284/301][7/8] Loss_D: 0.2669 Loss_G: 2.8624\n",
      "[285/301][0/8] Loss_D: 0.2930 Loss_G: 3.7373\n",
      "[285/301][1/8] Loss_D: 0.1459 Loss_G: 4.0578\n",
      "[285/301][2/8] Loss_D: 0.0954 Loss_G: 4.0972\n",
      "[285/301][3/8] Loss_D: 0.2635 Loss_G: 2.2882\n",
      "[285/301][4/8] Loss_D: 0.2785 Loss_G: 3.5534\n",
      "[285/301][5/8] Loss_D: 0.2323 Loss_G: 3.5521\n",
      "[285/301][6/8] Loss_D: 0.2948 Loss_G: 2.0898\n",
      "[285/301][7/8] Loss_D: 0.4115 Loss_G: 4.1448\n",
      "[286/301][0/8] Loss_D: 0.2101 Loss_G: 4.3108\n",
      "[286/301][1/8] Loss_D: 0.0791 Loss_G: 4.6898\n",
      "[286/301][2/8] Loss_D: 0.3321 Loss_G: 1.8471\n",
      "[286/301][3/8] Loss_D: 0.3219 Loss_G: 3.5781\n",
      "[286/301][4/8] Loss_D: 0.2677 Loss_G: 2.8631\n",
      "[286/301][5/8] Loss_D: 0.2287 Loss_G: 3.5015\n",
      "[286/301][6/8] Loss_D: 0.2150 Loss_G: 3.0563\n",
      "[286/301][7/8] Loss_D: 0.3107 Loss_G: 3.8325\n",
      "[287/301][0/8] Loss_D: 0.1218 Loss_G: 3.5687\n",
      "[287/301][1/8] Loss_D: 0.1363 Loss_G: 3.2679\n",
      "[287/301][2/8] Loss_D: 0.2009 Loss_G: 3.5576\n",
      "[287/301][3/8] Loss_D: 0.1142 Loss_G: 3.8908\n",
      "[287/301][4/8] Loss_D: 0.2143 Loss_G: 2.3374\n",
      "[287/301][5/8] Loss_D: 0.1969 Loss_G: 3.7008\n",
      "[287/301][6/8] Loss_D: 0.1409 Loss_G: 3.8349\n",
      "[287/301][7/8] Loss_D: 0.4398 Loss_G: 0.8685\n",
      "[288/301][0/8] Loss_D: 0.9314 Loss_G: 6.8015\n",
      "[288/301][1/8] Loss_D: 0.3144 Loss_G: 5.4224\n",
      "[288/301][2/8] Loss_D: 0.3907 Loss_G: 1.8813\n",
      "[288/301][3/8] Loss_D: 0.4311 Loss_G: 4.3154\n",
      "[288/301][4/8] Loss_D: 0.1062 Loss_G: 5.1761\n",
      "[288/301][5/8] Loss_D: 0.6617 Loss_G: 1.0906\n",
      "[288/301][6/8] Loss_D: 0.8882 Loss_G: 8.0695\n",
      "[288/301][7/8] Loss_D: 3.1751 Loss_G: 0.4003\n",
      "[289/301][0/8] Loss_D: 2.3063 Loss_G: 10.7853\n",
      "[289/301][1/8] Loss_D: 6.0454 Loss_G: 0.1762\n",
      "[289/301][2/8] Loss_D: 2.9481 Loss_G: 6.6990\n",
      "[289/301][3/8] Loss_D: 0.9979 Loss_G: 2.4794\n",
      "[289/301][4/8] Loss_D: 0.8713 Loss_G: 1.3785\n",
      "[289/301][5/8] Loss_D: 1.4172 Loss_G: 7.5702\n",
      "[289/301][6/8] Loss_D: 3.0696 Loss_G: 0.9661\n",
      "[289/301][7/8] Loss_D: 0.8288 Loss_G: 5.2820\n",
      "[290/301][0/8] Loss_D: 0.5977 Loss_G: 5.0434\n",
      "[290/301][1/8] Loss_D: 0.6546 Loss_G: 2.3888\n",
      "[290/301][2/8] Loss_D: 0.4542 Loss_G: 3.3498\n",
      "[290/301][3/8] Loss_D: 0.3883 Loss_G: 4.2474\n",
      "[290/301][4/8] Loss_D: 0.3798 Loss_G: 3.0792\n",
      "[290/301][5/8] Loss_D: 0.6661 Loss_G: 1.9484\n",
      "[290/301][6/8] Loss_D: 0.6509 Loss_G: 5.0114\n",
      "[290/301][7/8] Loss_D: 0.3173 Loss_G: 4.0700\n",
      "[291/301][0/8] Loss_D: 0.2978 Loss_G: 3.1516\n",
      "[291/301][1/8] Loss_D: 0.1957 Loss_G: 4.3004\n",
      "[291/301][2/8] Loss_D: 0.3935 Loss_G: 2.4398\n",
      "[291/301][3/8] Loss_D: 0.3291 Loss_G: 3.2236\n",
      "[291/301][4/8] Loss_D: 0.2839 Loss_G: 3.6143\n",
      "[291/301][5/8] Loss_D: 0.2232 Loss_G: 3.6444\n",
      "[291/301][6/8] Loss_D: 0.2719 Loss_G: 2.7429\n",
      "[291/301][7/8] Loss_D: 0.2032 Loss_G: 3.3796\n",
      "[292/301][0/8] Loss_D: 0.3254 Loss_G: 3.1599\n",
      "[292/301][1/8] Loss_D: 0.2657 Loss_G: 3.0026\n",
      "[292/301][2/8] Loss_D: 0.1876 Loss_G: 3.3734\n",
      "[292/301][3/8] Loss_D: 0.1435 Loss_G: 3.4669\n",
      "[292/301][4/8] Loss_D: 0.6058 Loss_G: 3.4319\n",
      "[292/301][5/8] Loss_D: 0.3342 Loss_G: 3.6121\n",
      "[292/301][6/8] Loss_D: 0.2499 Loss_G: 3.6793\n",
      "[292/301][7/8] Loss_D: 0.3349 Loss_G: 2.5274\n",
      "[293/301][0/8] Loss_D: 0.4923 Loss_G: 4.8481\n",
      "[293/301][1/8] Loss_D: 0.3907 Loss_G: 3.1918\n",
      "[293/301][2/8] Loss_D: 0.1636 Loss_G: 3.1478\n",
      "[293/301][3/8] Loss_D: 0.2300 Loss_G: 3.6155\n",
      "[293/301][4/8] Loss_D: 0.1921 Loss_G: 3.4538\n",
      "[293/301][5/8] Loss_D: 0.1768 Loss_G: 3.3994\n",
      "[293/301][6/8] Loss_D: 0.1925 Loss_G: 3.1754\n",
      "[293/301][7/8] Loss_D: 0.1702 Loss_G: 3.1309\n",
      "[294/301][0/8] Loss_D: 0.2286 Loss_G: 3.2094\n",
      "[294/301][1/8] Loss_D: 0.2086 Loss_G: 3.9528\n",
      "[294/301][2/8] Loss_D: 0.3056 Loss_G: 2.3519\n",
      "[294/301][3/8] Loss_D: 0.2909 Loss_G: 3.2946\n",
      "[294/301][4/8] Loss_D: 0.2149 Loss_G: 3.1986\n",
      "[294/301][5/8] Loss_D: 0.1756 Loss_G: 3.1357\n",
      "[294/301][6/8] Loss_D: 0.2325 Loss_G: 2.8729\n",
      "[294/301][7/8] Loss_D: 0.2395 Loss_G: 3.2934\n",
      "[295/301][0/8] Loss_D: 0.1666 Loss_G: 3.5473\n",
      "[295/301][1/8] Loss_D: 0.1835 Loss_G: 3.3929\n",
      "[295/301][2/8] Loss_D: 0.1977 Loss_G: 2.7517\n",
      "[295/301][3/8] Loss_D: 0.2359 Loss_G: 3.3654\n",
      "[295/301][4/8] Loss_D: 0.2401 Loss_G: 2.8352\n",
      "[295/301][5/8] Loss_D: 0.1996 Loss_G: 3.1174\n",
      "[295/301][6/8] Loss_D: 0.1483 Loss_G: 3.4848\n",
      "[295/301][7/8] Loss_D: 0.3547 Loss_G: 2.3011\n",
      "[296/301][0/8] Loss_D: 0.1788 Loss_G: 3.4016\n",
      "[296/301][1/8] Loss_D: 0.1646 Loss_G: 3.6255\n",
      "[296/301][2/8] Loss_D: 0.2386 Loss_G: 3.0721\n",
      "[296/301][3/8] Loss_D: 0.2453 Loss_G: 2.9281\n",
      "[296/301][4/8] Loss_D: 0.2050 Loss_G: 3.2274\n",
      "[296/301][5/8] Loss_D: 0.1914 Loss_G: 3.1576\n",
      "[296/301][6/8] Loss_D: 0.1901 Loss_G: 3.2427\n",
      "[296/301][7/8] Loss_D: 0.2173 Loss_G: 2.6965\n",
      "[297/301][0/8] Loss_D: 0.2702 Loss_G: 3.9576\n",
      "[297/301][1/8] Loss_D: 0.2077 Loss_G: 3.2361\n",
      "[297/301][2/8] Loss_D: 0.1511 Loss_G: 3.1886\n",
      "[297/301][3/8] Loss_D: 0.1456 Loss_G: 3.4469\n",
      "[297/301][4/8] Loss_D: 0.1689 Loss_G: 3.2779\n",
      "[297/301][5/8] Loss_D: 0.1936 Loss_G: 2.6242\n",
      "[297/301][6/8] Loss_D: 0.2332 Loss_G: 3.8077\n",
      "[297/301][7/8] Loss_D: 0.2390 Loss_G: 2.8222\n",
      "[298/301][0/8] Loss_D: 0.1459 Loss_G: 3.3489\n",
      "[298/301][1/8] Loss_D: 0.2338 Loss_G: 3.7215\n",
      "[298/301][2/8] Loss_D: 0.1827 Loss_G: 2.9921\n",
      "[298/301][3/8] Loss_D: 0.2144 Loss_G: 4.3812\n",
      "[298/301][4/8] Loss_D: 0.3444 Loss_G: 1.8756\n",
      "[298/301][5/8] Loss_D: 0.2698 Loss_G: 3.6013\n",
      "[298/301][6/8] Loss_D: 0.1815 Loss_G: 3.6766\n",
      "[298/301][7/8] Loss_D: 0.1257 Loss_G: 3.4465\n",
      "[299/301][0/8] Loss_D: 0.1394 Loss_G: 3.2623\n",
      "[299/301][1/8] Loss_D: 0.1718 Loss_G: 3.7486\n",
      "[299/301][2/8] Loss_D: 0.1905 Loss_G: 2.8862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/301][3/8] Loss_D: 0.1763 Loss_G: 3.5320\n",
      "[299/301][4/8] Loss_D: 0.1592 Loss_G: 3.6449\n",
      "[299/301][5/8] Loss_D: 0.1182 Loss_G: 3.6322\n",
      "[299/301][6/8] Loss_D: 0.1419 Loss_G: 3.0977\n",
      "[299/301][7/8] Loss_D: 0.1330 Loss_G: 3.4557\n",
      "[300/301][0/8] Loss_D: 0.1463 Loss_G: 3.2332\n",
      "[300/301][1/8] Loss_D: 0.1185 Loss_G: 3.6125\n",
      "[300/301][2/8] Loss_D: 0.1979 Loss_G: 3.1213\n",
      "[300/301][3/8] Loss_D: 0.0979 Loss_G: 3.8180\n",
      "[300/301][4/8] Loss_D: 0.1319 Loss_G: 3.3328\n",
      "[300/301][5/8] Loss_D: 0.1472 Loss_G: 3.0734\n",
      "[300/301][6/8] Loss_D: 0.1409 Loss_G: 3.0880\n",
      "[300/301][7/8] Loss_D: 0.2173 Loss_G: 2.2410\n"
     ]
    }
   ],
   "source": [
    "# Training the DCGANs\n",
    "\n",
    "criterion = nn.BCELoss() # We create a criterion object that will measure the error between the prediction and the target.\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator.\n",
    "\n",
    "for epoch in range(num_epochs): # We iterate over epochs.\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0): # We iterate over the images of the dataset.\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad() # We initialize to 0 the gradients of the discriminator with respect to the weights.\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data # We get a real image of the dataset which will be used to train the discriminator.\n",
    "        input = Variable(real) # We wrap it in a variable.\n",
    "        target = Variable(torch.ones(input.size()[0])) # We get the target.\n",
    "        output = netD(input) # We forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errD_real = criterion(output, target) # We compute the loss between the predictions (output) and the target (equal to 1).\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], length_of_noise_vector, 1, 1)) # We make a random input vector (noise) of the generator.\n",
    "        fake = netG(noise) # We forward propagate this random input vector into the neural network of the generator to get some fake generated images.\n",
    "        target = Variable(torch.zeros(input.size()[0])) # We get the target.\n",
    "        output = netD(fake.detach()) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errD_fake = criterion(output, target) # We compute the loss between the prediction (output) and the target (equal to 0).\n",
    "\n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake # We compute the total error of the discriminator.\n",
    "        errD.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the discriminator.\n",
    "        optimizerD.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad() # We initialize to 0 the gradients of the generator with respect to the weights.\n",
    "        target = Variable(torch.ones(input.size()[0])) # We get the target.\n",
    "        output = netD(fake) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errG = criterion(output, target) # We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).\n",
    "        errG.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.\n",
    "        optimizerG.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, num_epochs, i, len(dataloader), errD.data.item(), errG.data.item())) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).\n",
    "        if i%1000==0 and epoch%10==0: # Every 1000 steps and 10 epochs:\n",
    "            #vutils.save_image(real, '%s/real_samples.png' % \"output pics for GANs\", normalize = True) # We save the real images of the minibatch.\n",
    "            fake = netG(noise) # We get our fake generated images.\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"output pics for GANs\", epoch), normalize = True) # We also save the fake generated images of the minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
